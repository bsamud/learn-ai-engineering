{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SageMaker ML Workshop: Credit Card Fraud Detection with XGBoost\n\n## Workshop Overview\n\nWelcome to this hands-on machine learning workshop! You'll learn to build a **real-world fintech fraud detection system** using **XGBoost** - the algorithm trusted by major financial institutions worldwide.\n\n### What You'll Build:\n\nA machine learning model that can:\n- ‚úÖ Detect fraudulent credit card transactions in real-time\n- ‚úÖ Minimize false positives (legitimate transactions marked as fraud)\n- ‚úÖ Maximize fraud catch rate (true positives)\n- ‚úÖ Handle highly imbalanced data (fraud is rare!)\n\n### What You'll Learn:\n\n1. **Environment Setup** - Install XGBoost and fintech libraries\n2. **Data Acquisition** - Download and load credit card transaction data\n3. **Data Exploration** - Deep dive into fraud patterns\n4. **Data Preprocessing** - Handle imbalanced classes\n5. **Model Training** - Build XGBoost classifier with extensive tuning\n6. **Model Evaluation** - Assess performance with fintech-specific metrics\n7. **Feature Importance** - Understand fraud indicators\n8. **Real-time Scoring** - Deploy for transaction screening\n9. **Production Deployment** - Build production-ready inference code\n\n### Real-World Application:\n\nThis same approach is used by:\n- **Credit card companies**: Real-time fraud detection\n- **Financial institutions**: Transaction monitoring and AML (Anti-Money Laundering)\n- **Payment processors**: Risk scoring for transactions\n- **Fintech startups**: User behavior analysis\n\n### Dataset:\n\nWe'll use the **Credit Card Fraud Detection Dataset** from Kaggle, which contains:\n- **284,807 transactions** from European cardholders (September 2013)\n- **492 frauds** (0.172% - highly imbalanced!)\n- **Anonymized features** (PCA-transformed for privacy)\n- **Real-world challenges**: Class imbalance, feature engineering\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1: Understanding XGBoost for Fraud Detection\n",
    "\n",
    "### What is XGBoost?\n",
    "\n",
    "**XGBoost** stands for **eXtreme Gradient Boosting**. It's the #1 algorithm for fraud detection because:\n",
    "\n",
    "#### Why Financial Institutions Choose XGBoost:\n",
    "\n",
    "1. **High Accuracy**: Catches 95-99% of fraud cases\n",
    "2. **Low False Positives**: Doesn't block legitimate transactions\n",
    "3. **Fast Inference**: Makes decisions in milliseconds\n",
    "4. **Handles Imbalance**: Works even when fraud is 0.1% of transactions\n",
    "5. **Interpretable**: Explains why a transaction was flagged\n",
    "6. **Production-Ready**: Scales to millions of transactions\n",
    "\n",
    "#### How Does XGBoost Work? (Financial Example)\n",
    "\n",
    "Imagine you're a fraud analyst:\n",
    "\n",
    "**Traditional Approach** (Single Rule):\n",
    "- \"Flag transactions over $1,000 from new locations\"\n",
    "- **Problem**: Misses sophisticated fraud, blocks legitimate travel purchases\n",
    "\n",
    "**XGBoost Approach** (Ensemble of 100+ \"Expert Analysts\"):\n",
    "\n",
    "1. **Analyst 1**: \"This looks fraudulent because amount is unusual for this merchant\"\n",
    "2. **Analyst 2**: \"Actually, let me check - this customer travels frequently, so new location is normal\"\n",
    "3. **Analyst 3**: \"But wait - the transaction velocity is suspicious (10 transactions in 5 minutes)\"\n",
    "4. **Analyst 4**: \"And the device fingerprint doesn't match customer's usual devices\"\n",
    "5. **Continue for 100+ analysts...**\n",
    "\n",
    "**Final Decision**: Combines wisdom of all analysts, weighted by their accuracy\n",
    "\n",
    "#### Technical: How Boosting Works\n",
    "\n",
    "```\n",
    "Step 1: Build Tree 1 ‚Üí Catches obvious fraud (60% accuracy)\n",
    "        ‚Üì\n",
    "Step 2: Find what Tree 1 missed ‚Üí Build Tree 2 to catch those (70% accuracy)\n",
    "        ‚Üì\n",
    "Step 3: Find what Trees 1+2 missed ‚Üí Build Tree 3 (80% accuracy)\n",
    "        ‚Üì\n",
    "Continue for 100-1000 trees...\n",
    "        ‚Üì\n",
    "Final Model: Combines all trees ‚Üí 95%+ accuracy\n",
    "```\n",
    "\n",
    "Each tree **learns from the mistakes** of previous trees!\n",
    "\n",
    "#### XGBoost vs Other Algorithms (Fraud Detection Context):\n",
    "\n",
    "| Algorithm | Fraud Detection Accuracy | False Positive Rate | Inference Speed | Imbalance Handling |\n",
    "|-----------|-------------------------|---------------------|-----------------|--------------------|\n",
    "| **XGBoost** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (95-99%) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very Low | ‚≠ê‚≠ê‚≠ê‚≠ê Fast | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent |\n",
    "| Random Forest | ‚≠ê‚≠ê‚≠ê‚≠ê (90-95%) | ‚≠ê‚≠ê‚≠ê‚≠ê Low | ‚≠ê‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê‚≠ê‚≠ê Good |\n",
    "| Logistic Regression | ‚≠ê‚≠ê‚≠ê (75-85%) | ‚≠ê‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very Fast | ‚≠ê‚≠ê Poor |\n",
    "| Neural Networks | ‚≠ê‚≠ê‚≠ê‚≠ê (90-95%) | ‚≠ê‚≠ê‚≠ê Moderate | ‚≠ê‚≠ê Slow | ‚≠ê‚≠ê‚≠ê Fair |\n",
    "| Rule-Based | ‚≠ê‚≠ê (60-70%) | ‚≠ê High | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Very Fast | ‚≠ê Very Poor |\n",
    "\n",
    "**Winner for Fraud Detection**: XGBoost combines best accuracy with production-ready performance!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Data Download Instructions\n",
    "\n",
    "### Option 1: Download from Kaggle (Recommended)\n",
    "\n",
    "**Dataset**: Credit Card Fraud Detection\n",
    "**URL**: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "**Size**: 150 MB (compressed)\n",
    "\n",
    "#### Steps to Download:\n",
    "\n",
    "**Method A: Using Kaggle Website (Easiest)**\n",
    "\n",
    "1. **Create Kaggle Account** (if you don't have one)\n",
    "   - Go to https://www.kaggle.com/\n",
    "   - Click \"Register\" and sign up (it's free!)\n",
    "\n",
    "2. **Navigate to Dataset**\n",
    "   - Visit: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "   - Click the blue \"Download\" button\n",
    "   - File will download as `creditcardfraud.zip`\n",
    "\n",
    "3. **Upload to SageMaker**\n",
    "   - Extract the zip file (contains `creditcard.csv`)\n",
    "   - In SageMaker Studio/Notebook:\n",
    "     - Click the upload button (‚¨ÜÔ∏è icon)\n",
    "     - Select `creditcard.csv`\n",
    "     - Wait for upload to complete\n",
    "\n",
    "**Method B: Using Kaggle API (For Advanced Users)**\n",
    "\n",
    "```bash\n",
    "# 1. Install Kaggle API\n",
    "pip install kaggle\n",
    "\n",
    "# 2. Get your Kaggle API credentials\n",
    "# - Go to https://www.kaggle.com/account\n",
    "# - Scroll to \"API\" section\n",
    "# - Click \"Create New API Token\"\n",
    "# - This downloads kaggle.json\n",
    "\n",
    "# 3. Set up credentials\n",
    "mkdir -p ~/.kaggle\n",
    "mv kaggle.json ~/.kaggle/\n",
    "chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "# 4. Download dataset\n",
    "kaggle datasets download -d mlg-ulb/creditcardfraud\n",
    "\n",
    "# 5. Unzip\n",
    "unzip creditcardfraud.zip\n",
    "```\n",
    "\n",
    "### Option 2: Use Sample Dataset (For Quick Start)\n",
    "\n",
    "If you want to get started immediately, we can create a smaller synthetic dataset for practice:\n",
    "\n",
    "```python\n",
    "# We'll provide code below to generate sample data\n",
    "# This is useful for testing but use real data for production!\n",
    "```\n",
    "\n",
    "### Option 3: AWS S3 (For Production)\n",
    "\n",
    "```bash\n",
    "# Download from S3 if your organization has the data there\n",
    "aws s3 cp s3://your-bucket/creditcard.csv ./creditcard.csv\n",
    "```\n",
    "\n",
    "### Dataset Information:\n",
    "\n",
    "Once downloaded, `creditcard.csv` contains:\n",
    "- **Rows**: 284,807 transactions\n",
    "- **Columns**: 31 features\n",
    "  - `Time`: Seconds elapsed between transaction and first transaction\n",
    "  - `V1-V28`: Anonymized features (PCA components)\n",
    "  - `Amount`: Transaction amount\n",
    "  - `Class`: Target variable (0 = legitimate, 1 = fraud)\n",
    "\n",
    "**Privacy Note**: Features V1-V28 are PCA-transformed to protect customer privacy while maintaining fraud patterns.\n",
    "\n",
    "### Expected File Location:\n",
    "\n",
    "After upload, your file should be at:\n",
    "```\n",
    "/home/sagemaker-user/creditcard.csv\n",
    "```\n",
    "\n",
    "Or wherever you saved it in your SageMaker environment.\n",
    "\n",
    "---\n",
    "\n",
    "**‚úÖ Before proceeding**: Make sure you have `creditcard.csv` ready!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Environment Setup\n",
    "\n",
    "### Step 3.1: Install Required Libraries\n",
    "\n",
    "First, let's set up our Python environment with all necessary packages.\n",
    "\n",
    "**What each library does:**\n",
    "- **xgboost**: The main XGBoost library for fraud detection models\n",
    "- **pandas**: For working with transaction data (like Excel for Python)\n",
    "- **numpy**: For numerical operations and array handling\n",
    "- **scikit-learn**: For data preprocessing and evaluation metrics\n",
    "- **matplotlib & seaborn**: For creating visualizations\n",
    "- **imbalanced-learn**: Special tools for handling fraud (rare event) detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required packages\n",
    "# The '--quiet' flag suppresses installation messages\n",
    "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn imbalanced-learn --quiet\n",
    "\n",
    "print(\"‚úÖ All libraries installed successfully!\")\n",
    "print(\"\\nüì¶ Installed packages:\")\n",
    "print(\"   ‚Ä¢ XGBoost: Gradient boosting framework\")\n",
    "print(\"   ‚Ä¢ Scikit-learn: Machine learning utilities\")\n",
    "print(\"   ‚Ä¢ Pandas: Data manipulation\")\n",
    "print(\"   ‚Ä¢ NumPy: Numerical computing\")\n",
    "print(\"   ‚Ä¢ Matplotlib & Seaborn: Data visualization\")\n",
    "print(\"   ‚Ä¢ Imbalanced-learn: Tools for imbalanced datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: Import Libraries\n",
    "\n",
    "Now we'll import all the tools we need. Think of this as laying out all your tools before starting a project.\n",
    "\n",
    "**Import Organization:**\n",
    "1. **Core libraries**: pandas, numpy (data handling)\n",
    "2. **Machine learning**: XGBoost, scikit-learn (model building)\n",
    "3. **Visualization**: matplotlib, seaborn (charts and graphs)\n",
    "4. **Utilities**: warnings, time (helper functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORE DATA MANIPULATION LIBRARIES\n",
    "# =============================================================================\n",
    "import pandas as pd                    # For working with transaction data tables\n",
    "import numpy as np                     # For numerical operations\n",
    "import warnings                        # To suppress unnecessary warnings\n",
    "warnings.filterwarnings('ignore')      # Keep output clean\n",
    "\n",
    "# =============================================================================\n",
    "# MACHINE LEARNING LIBRARIES\n",
    "# =============================================================================\n",
    "import xgboost as xgb                  # The star of our show!\n",
    "\n",
    "# Scikit-learn: Data preprocessing\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,                  # Split data into train/test sets\n",
    "    cross_val_score,                   # Cross-validation\n",
    "    StratifiedKFold                    # For imbalanced data splitting\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler  # Feature scaling\n",
    "\n",
    "# Scikit-learn: Model evaluation metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,                    # Overall correctness\n",
    "    precision_score,                   # Precision: TP / (TP + FP)\n",
    "    recall_score,                      # Recall: TP / (TP + FN)\n",
    "    f1_score,                          # F1: Harmonic mean of precision & recall\n",
    "    roc_auc_score,                     # Area under ROC curve\n",
    "    average_precision_score,           # Area under PR curve (better for imbalanced data)\n",
    "    confusion_matrix,                  # Breakdown of predictions\n",
    "    classification_report,             # Comprehensive report\n",
    "    roc_curve,                         # For ROC curve plotting\n",
    "    precision_recall_curve,            # For PR curve plotting\n",
    "    matthews_corrcoef                  # MCC: Good for imbalanced data\n",
    ")\n",
    "\n",
    "# Imbalanced-learn: Special tools for fraud detection\n",
    "from imblearn.over_sampling import SMOTE  # Synthetic minority oversampling\n",
    "from imblearn.under_sampling import RandomUnderSampler  # Undersample majority\n",
    "\n",
    "# =============================================================================\n",
    "# VISUALIZATION LIBRARIES\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt        # Basic plotting\n",
    "import seaborn as sns                  # Beautiful statistical plots\n",
    "\n",
    "# Configure visualization settings for professional-looking plots\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "sns.set_context(\"notebook\", font_scale=1.1)\n",
    "%matplotlib inline\n",
    "\n",
    "# =============================================================================\n",
    "# PANDAS DISPLAY SETTINGS\n",
    "# =============================================================================\n",
    "pd.set_option('display.max_columns', None)     # Show all columns\n",
    "pd.set_option('display.max_rows', 100)         # Show up to 100 rows\n",
    "pd.set_option('display.precision', 4)          # 4 decimal places\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)  # Consistent formatting\n",
    "\n",
    "# =============================================================================\n",
    "# REPRODUCIBILITY\n",
    "# =============================================================================\n",
    "# Set random seeds for reproducible results\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# =============================================================================\n",
    "# IMPORT VERIFICATION\n",
    "# =============================================================================\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"\\nüìä Library Versions:\")\n",
    "print(f\"   ‚Ä¢ XGBoost: {xgb.__version__}\")\n",
    "print(f\"   ‚Ä¢ Pandas: {pd.__version__}\")\n",
    "print(f\"   ‚Ä¢ NumPy: {np.__version__}\")\n",
    "print(f\"\\nüé≤ Random seed set to: {RANDOM_SEED}\")\n",
    "print(\"   ‚Üí This ensures reproducible results across runs\")\n",
    "\n",
    "print(\"\\nüöÄ Ready to build fraud detection models!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.3: Create Helper Functions\n",
    "\n",
    "Let's create some utility functions we'll use throughout the notebook. These will help us:\n",
    "- Print formatted outputs\n",
    "- Calculate custom metrics\n",
    "- Create consistent visualizations\n",
    "\n",
    "**Why create helper functions?**\n",
    "- **DRY Principle**: Don't Repeat Yourself\n",
    "- **Readability**: Cleaner code\n",
    "- **Reusability**: Use across multiple projects\n",
    "- **Maintainability**: Fix bugs in one place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_section_header(title, emoji=\"üìä\"):\n",
    "    \"\"\"\n",
    "    Print a formatted section header for better readability.\n",
    "    \n",
    "    Args:\n",
    "        title (str): Section title\n",
    "        emoji (str): Emoji to display (default: üìä)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"{emoji} {title}\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "\n",
    "def calculate_financial_metrics(y_true, y_pred, transaction_amounts=None):\n",
    "    \"\"\"\n",
    "    Calculate business-relevant metrics for fraud detection.\n",
    "    \n",
    "    In fraud detection, it's not just about accuracy - we care about:\n",
    "    - How much fraud $ we catch\n",
    "    - How many legitimate customers we inconvenience\n",
    "    - The total financial impact\n",
    "    \n",
    "    Args:\n",
    "        y_true: Actual labels (0=legitimate, 1=fraud)\n",
    "        y_pred: Predicted labels\n",
    "        transaction_amounts: Dollar amounts (optional)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary of financial metrics\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    \n",
    "    metrics = {\n",
    "        'true_negatives': int(tn),      # Correctly identified legitimate\n",
    "        'false_positives': int(fp),     # Legitimate flagged as fraud (BAD for UX)\n",
    "        'false_negatives': int(fn),     # Fraud missed (BAD for losses)\n",
    "        'true_positives': int(tp),      # Correctly identified fraud\n",
    "        'fraud_catch_rate': tp / (tp + fn) if (tp + fn) > 0 else 0,  # Recall\n",
    "        'precision': tp / (tp + fp) if (tp + fp) > 0 else 0,\n",
    "        'customer_friction_rate': fp / (fp + tn) if (fp + tn) > 0 else 0  # FPR\n",
    "    }\n",
    "    \n",
    "    # Calculate financial impact if amounts provided\n",
    "    if transaction_amounts is not None:\n",
    "        amounts = np.array(transaction_amounts)\n",
    "        \n",
    "        # Money saved by catching fraud\n",
    "        fraud_caught_amount = amounts[(y_true == 1) & (y_pred == 1)].sum()\n",
    "        \n",
    "        # Money lost to missed fraud\n",
    "        fraud_missed_amount = amounts[(y_true == 1) & (y_pred == 0)].sum()\n",
    "        \n",
    "        # Volume of legitimate transactions incorrectly blocked\n",
    "        false_positive_amount = amounts[(y_true == 0) & (y_pred == 1)].sum()\n",
    "        \n",
    "        metrics['fraud_caught_$'] = fraud_caught_amount\n",
    "        metrics['fraud_missed_$'] = fraud_missed_amount\n",
    "        metrics['false_positive_$'] = false_positive_amount\n",
    "        metrics['fraud_prevention_rate'] = (fraud_caught_amount / \n",
    "                                           (fraud_caught_amount + fraud_missed_amount) \n",
    "                                           if (fraud_caught_amount + fraud_missed_amount) > 0 else 0)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_metric_comparison(metrics_dict, title=\"Model Performance Comparison\"):\n",
    "    \"\"\"\n",
    "    Create a bar plot comparing different metrics.\n",
    "    \n",
    "    Args:\n",
    "        metrics_dict (dict): Dictionary of metric names and values\n",
    "        title (str): Plot title\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    metrics = list(metrics_dict.keys())\n",
    "    values = list(metrics_dict.values())\n",
    "    \n",
    "    colors = plt.cm.viridis(np.linspace(0.3, 0.9, len(metrics)))\n",
    "    bars = ax.barh(metrics, values, color=colors, alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Score', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlim([0, 1.0])\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, value in zip(bars, values):\n",
    "        width = bar.get_width()\n",
    "        ax.text(width, bar.get_y() + bar.get_height()/2, \n",
    "                f' {value:.4f}', \n",
    "                ha='left', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"‚úÖ Helper functions created!\")\n",
    "print(\"\\nüìã Available functions:\")\n",
    "print(\"   ‚Ä¢ print_section_header(): Format section titles\")\n",
    "print(\"   ‚Ä¢ calculate_financial_metrics(): Business-focused metrics\")\n",
    "print(\"   ‚Ä¢ plot_metric_comparison(): Visualize performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Data Loading\n",
    "\n",
    "### Step 4.1: Load the Credit Card Fraud Dataset\n",
    "\n",
    "Now we'll load our credit card transaction data. This dataset is from Kaggle and contains real (anonymized) credit card transactions.\n",
    "\n",
    "**What to expect:**\n",
    "- **Size**: ~150 MB, 284,807 transactions\n",
    "- **Time period**: 2 days of transactions (September 2013)\n",
    "- **Features**: 30 features + 1 target variable\n",
    "- **Challenge**: Highly imbalanced (only 0.172% fraud)\n",
    "\n",
    "**Important**: If you haven't downloaded the data yet, go back to Section 2 for instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Loading Credit Card Fraud Dataset\", \"üí≥\")\n",
    "\n",
    "# Define the path to your data file\n",
    "# Adjust this path based on where you saved the file\n",
    "DATA_PATH = 'creditcard.csv'  # Change this if your file is elsewhere\n",
    "\n",
    "print(f\"\\nüìÇ Loading data from: {DATA_PATH}\")\n",
    "print(\"‚è≥ This may take 30-60 seconds for large dataset...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load the data\n",
    "    # Parse_dates and infer_datetime_format help with Time column\n",
    "    df = pd.read_csv(DATA_PATH)\n",
    "    \n",
    "    print(\"‚úÖ Dataset loaded successfully!\")\n",
    "    print(\"\\nüìä Dataset Overview:\")\n",
    "    print(f\"   ‚Ä¢ Total transactions: {len(df):,}\")\n",
    "    print(f\"   ‚Ä¢ Number of features: {len(df.columns) - 1}  (excluding target 'Class')\")\n",
    "    print(f\"   ‚Ä¢ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    print(f\"   ‚Ä¢ Time period: {df['Time'].max() / 3600:.1f} hours\")\n",
    "    \n",
    "    # Quick fraud statistics\n",
    "    n_fraud = (df['Class'] == 1).sum()\n",
    "    n_legit = (df['Class'] == 0).sum()\n",
    "    fraud_pct = (n_fraud / len(df)) * 100\n",
    "    \n",
    "    print(f\"\\nüö® Fraud Statistics:\")\n",
    "    print(f\"   ‚Ä¢ Legitimate transactions: {n_legit:,} ({100-fraud_pct:.3f}%)\")\n",
    "    print(f\"   ‚Ä¢ Fraudulent transactions: {n_fraud:,} ({fraud_pct:.3f}%)\")\n",
    "    print(f\"   ‚Ä¢ Imbalance ratio: {n_legit/n_fraud:.1f}:1\")\n",
    "    print(f\"   ‚Üí This is a HIGHLY IMBALANCED dataset!\")\n",
    "    \nexcept FileNotFoundError:\n",
    "    print(\"‚ùå Error: Data file not found!\")\n",
    "    print(\"\\nüìù Please follow these steps:\")\n",
    "    print(\"   1. Download from: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\")\n",
    "    print(\"   2. Extract creditcard.csv from the zip file\")\n",
    "    print(\"   3. Upload to this notebook's directory\")\n",
    "    print(\"   4. Update DATA_PATH variable if needed\")\n",
    "    print(\"\\n   Or use the Kaggle API instructions from Section 2\")\n",
    "    raise\n",
    "\nexcept Exception as e:\n",
    "    print(f\"‚ùå Unexpected error loading data: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Initial Data Inspection\n",
    "\n",
    "Let's take our first look at the data. This is like opening a package to see what's inside!\n",
    "\n",
    "**What we're looking for:**\n",
    "- Column names and types\n",
    "- Sample transactions\n",
    "- Data types (numeric, categorical, etc.)\n",
    "- Any obvious issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Data Inspection\", \"üîç\")\n",
    "\n",
    "print(\"\\nüìã First 5 Transactions:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n\\nüìã Last 5 Transactions:\")\n",
    "print(df.tail())\n",
    "\n",
    "print(\"\\n\\nüìä Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n\\nüìà Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n\\nüí° Column Explanation:\")\n",
    "print(\"=\"*80)\n",
    "print(\"   ‚Ä¢ Time: Seconds since first transaction (for sequence analysis)\")\n",
    "print(\"   ‚Ä¢ V1-V28: Principal components from PCA transformation\")\n",
    "print(\"             ‚Üí Original features anonymized for privacy\")\n",
    "print(\"             ‚Üí Still capture important fraud patterns\")\n",
    "print(\"   ‚Ä¢ Amount: Transaction amount in Euros (‚Ç¨)\")\n",
    "print(\"   ‚Ä¢ Class: Target variable (0 = Legitimate, 1 = Fraud)\")\n",
    "print(\"\\n   ‚ö†Ô∏è  Note: We can't know what V1-V28 originally represented\")\n",
    "print(\"      (could be merchant category, location, time of day, etc.)\")\n",
    "print(\"      but they still contain the fraud patterns we need!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3: Check for Data Quality Issues\n",
    "\n",
    "Before building any model, we MUST check data quality. In fintech, bad data = bad decisions = lost money!\n",
    "\n",
    "**Common data quality issues:**\n",
    "1. **Missing values**: Incomplete records\n",
    "2. **Duplicates**: Same transaction recorded twice\n",
    "3. **Outliers**: Unusual values that might be errors\n",
    "4. **Data types**: Wrong format (e.g., numbers stored as text)\n",
    "5. **Impossible values**: Negative amounts, future dates, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_section_header(\"Data Quality Assessment\", \"üî¨\")\n",
    "\n",
    "# 1. Check for missing values\n",
    "print(\"\\n1Ô∏è‚É£  Missing Values Check:\")\n",
    "print(\"-\" * 80)\n",
    "missing_values = df.isnull().sum()\n",
    "total_missing = missing_values.sum()\n",
    "\n",
    "if total_missing == 0:\n",
    "    print(\"‚úÖ No missing values found!\")\n",
    "    print(\"   ‚Üí Dataset is complete - excellent for model training\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Found {total_missing} missing values:\")\n",
    "    print(missing_values[missing_values > 0])\n",
    "    print(\"\\n   ‚Üí We'll need to handle these before training\")\n",
    "\n",
    "# 2. Check for duplicate transactions\n",
    "print(\"\\n\\n2Ô∏è‚É£  Duplicate Transactions Check:\")\n",
    "print(\"-\" * 80)\n",
    "n_duplicates = df.duplicated().sum()\n",
    "\n",
    "if n_duplicates == 0:\n",
    "    print(\"‚úÖ No duplicate transactions found!\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Found {n_duplicates} duplicate transactions\")\n",
    "    print(\"   ‚Üí These might be legitimate (e.g., subscription payments)\")\n",
    "    print(\"   ‚Üí Or data collection errors\")\n",
    "    print(\"   ‚Üí We'll investigate further\")\n",
    "\n",
    "# 3. Check data types\n",
    "print(\"\\n\\n3Ô∏è‚É£  Data Types Check:\")\n",
    "print(\"-\" * 80)\n",
    "print(df.dtypes.value_counts())\n",
    "print(\"\\n   Expected: All numeric (float64 or int64)\")\n",
    "\n",
    "if (df.dtypes == 'object').any():\n",
    "    print(\"\\n‚ö†Ô∏è  Warning: Found non-numeric columns:\")\n",
    "    print(df.select_dtypes(include='object').columns.tolist())\n",
    "else:\n",
    "    print(\"   ‚úÖ All columns are numeric - ready for ML!\")\n",
    "\n",
    "# 4. Check for impossible values\n",
    "print(\"\\n\\n4Ô∏è‚É£  Business Logic Validation:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check for negative amounts (shouldn't exist in transactions)\n",
    "negative_amounts = (df['Amount'] < 0).sum()\n",
    "if negative_amounts > 0:\n",
    "    print(f\"‚ö†Ô∏è  Found {negative_amounts} transactions with negative amounts!\")\n",
    "else:\n",
    "    print(\"‚úÖ All transaction amounts are non-negative\")\n",
    "\n",
    "# Check for zero amounts\n",
    "zero_amounts = (df['Amount'] == 0).sum()\n",
    "print(f\"   ‚Ä¢ Transactions with $0 amount: {zero_amounts:,}\")\n",
    "if zero_amounts > 100:\n",
    "    print(\"     ‚Üí This is unusual - might be authorization checks\")\n",
    "\n",
    "# Check Class values\n",
    "unique_classes = df['Class'].unique()\n",
    "print(f\"\\n   ‚Ä¢ Unique Class values: {sorted(unique_classes)}\")\n",
    "if set(unique_classes) == {0, 1}:\n",
    "    print(\"     ‚úÖ Correct: Only 0 (legitimate) and 1 (fraud)\")\n",
    "else:\n",
    "    print(\"     ‚ö†Ô∏è  Unexpected class values!\")\n",
    "\n",
    "# 5. Summary\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "print(\"üìä Data Quality Summary:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "quality_score = 0\n",
    "if total_missing == 0: quality_score += 25\n",
    "if n_duplicates == 0: quality_score += 25\n",
    "if not (df.dtypes == 'object').any(): quality_score += 25\n",
    "if negative_amounts == 0: quality_score += 25\n",
    "\n",
    "print(f\"\\n   Overall Data Quality Score: {quality_score}/100\")\n",
    "\n",
    "if quality_score == 100:\n",
    "    print(\"   üåü Excellent! Data is production-ready\")\n",
    "elif quality_score >= 75:\n",
    "    print(\"   ‚úÖ Good! Minor issues that are manageable\")\n",
    "elif quality_score >= 50:\n",
    "    print(\"   ‚ö†Ô∏è  Fair - some data cleaning required\")\n",
    "else:\n",
    "    print(\"   ‚ùå Poor - significant data quality issues to address\")\n",
    "\n",
    "print(\"\\n   ‚úÖ Ready to proceed with exploratory analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TO BE CONTINUED...\n",
    "\n",
    "This notebook is being generated. The remaining sections will include:\n",
    "\n",
    "- **Section 5**: Exploratory Data Analysis (fraud patterns, distributions)\n",
    "- **Section 6**: Feature Engineering (time-based features, aggregations)\n",
    "- **Section 7**: Data Preprocessing (scaling, train/test split)\n",
    "- **Section 8**: Handling Class Imbalance (SMOTE, class weights)\n",
    "- **Section 9**: Model Training (XGBoost with extensive tuning)\n",
    "- **Section 10**: Model Evaluation (ROC-AUC, Precision-Recall, Cost-Benefit)\n",
    "- **Section 11**: Feature Importance (understanding fraud indicators)\n",
    "- **Section 12**: Hyperparameter Tuning (grid search, optimization)\n",
    "- **Section 13**: Production Deployment (inference code, monitoring)\n",
    "- **Section 14**: Real-time Scoring (API endpoint simulation)\n",
    "- **Appendix**: Exercises and challenges\n",
    "\n",
    "The file is being created - please run the next cell to continue..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}