{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Credit Card Fraud Detection with XGBoost\n\n## Workshop Overview (1 Hour)\n\nWelcome! In this hands-on workshop, you'll build a **fraud detection system** using XGBoost - the algorithm trusted by major financial institutions and fintech companies worldwide.\n\n### What You'll Build:\n- A model that detects fraudulent credit card transactions\n- Learn to handle imbalanced data (fraud is rare!)\n- Understand evaluation metrics for fraud detection\n- Save your model for production use\n\n### Dataset:\nCredit Card Fraud Detection dataset with:\n- **284,807 transactions** over 2 days\n- **492 frauds** (0.172% - highly imbalanced!)\n- **Real-world challenge**: Detect rare events accurately\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Understanding XGBoost for Fraud Detection\n\n### What is XGBoost?\n\n**XGBoost** = e**X**treme **G**radient **Boosting**\n\n#### Why Financial Institutions Choose XGBoost:\n\n1. **High Accuracy**: Catches 95-99% of fraud cases\n2. **Low False Positives**: Doesn't block legitimate customers\n3. **Fast**: Makes decisions in milliseconds (real-time fraud detection)\n4. **Handles Imbalance**: Works even when fraud is 0.1% of transactions\n5. **Interpretable**: Explains why a transaction was flagged\n6. **Production-Ready**: Used by PayPal, Airbnb, and major financial institutions\n\n#### How Does XGBoost Work? (Simple Analogy)\n\nImagine you have 100 fraud analysts:\n\n**Traditional Approach** (One Rule):\n- \"Flag all transactions over $1,000 from new locations\"\n- **Problem**: Misses sophisticated fraud, blocks legitimate travelers\n\n**XGBoost Approach** (100 Analysts Working Together):\n\n```\nAnalyst 1: \"Unusual amount for this merchant\" ‚Üí 60% sure it's fraud\n    ‚Üì\nAnalyst 2: \"Customer travels frequently, new location is normal\" ‚Üí Actually 40% sure\n    ‚Üì\nAnalyst 3: \"But wait - 10 transactions in 5 minutes!\" ‚Üí Back to 70% sure\n    ‚Üì\nAnalyst 4: \"Device fingerprint doesn't match\" ‚Üí 85% sure it's fraud\n    ‚Üì\n... 96 more analysts ...\n    ‚Üì\nFinal Decision: 96% confidence it's FRAUD ‚Üí Block transaction\n```\n\n**Key Insight**: Each analyst (tree) learns from the mistakes of previous analysts!\n\n#### Technical: How Boosting Works\n\n```\nStep 1: Tree 1 catches obvious fraud ‚Üí 60% accuracy\nStep 2: Tree 2 focuses on what Tree 1 missed ‚Üí Combined 75% accuracy  \nStep 3: Tree 3 focuses on what Trees 1+2 missed ‚Üí Combined 85% accuracy\n...\nStep 100: Tree 100 fine-tunes everything ‚Üí Final 95%+ accuracy\n```\n\n#### XGBoost vs Other Algorithms:\n\n| Algorithm | Fraud Detection | Speed | Imbalance Handling | Production Ready |\n|-----------|----------------|-------|-------------------|------------------|\n| **XGBoost** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (95-99%) | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n| Random Forest | ‚≠ê‚≠ê‚≠ê‚≠ê (90-95%) | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |\n| Logistic Regression | ‚≠ê‚≠ê‚≠ê (75-85%) | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê |\n| Neural Networks | ‚≠ê‚≠ê‚≠ê‚≠ê (90-95%) | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê |\n\n**Winner**: XGBoost combines best accuracy with production performance!\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Download Instructions\n",
    "\n",
    "### Option 1: Download from Kaggle (Recommended)\n",
    "\n",
    "**Quick Steps:**\n",
    "1. Go to: https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud\n",
    "2. Create account (free) if needed\n",
    "3. Click \"Download\" button\n",
    "4. Extract `creditcard.csv` from zip file\n",
    "5. Upload to this SageMaker environment\n",
    "\n",
    "### Option 2: Kaggle API (Fastest)\n",
    "\n",
    "```bash\n",
    "pip install kaggle\n",
    "kaggle datasets download -d mlg-ulb/creditcardfraud\n",
    "unzip creditcardfraud.zip\n",
    "```\n",
    "\n",
    "### Option 3: Generate Sample Data (For Testing)\n",
    "\n",
    "If you can't access Kaggle right now, uncomment and run this code to generate sample data:\n",
    "\n",
    "```python\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from sklearn.datasets import make_classification\n",
    "# \n",
    "# X, y = make_classification(n_samples=10000, n_features=30, n_classes=2, \n",
    "#                            weights=[0.98, 0.02], random_state=42)\n",
    "# df = pd.DataFrame(X, columns=[f'V{i}' for i in range(1,29)] + ['Amount', 'Time'])\n",
    "# df['Class'] = y\n",
    "# df.to_csv('creditcard.csv', index=False)\n",
    "# print(\"‚úÖ Sample data created!\")\n",
    "```\n",
    "\n",
    "**‚ö†Ô∏è Important**: Make sure `creditcard.csv` is in the same directory as this notebook!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install xgboost scikit-learn pandas numpy matplotlib seaborn --quiet\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Settings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "print(f\"‚úÖ Libraries imported!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load and Explore the Data\n",
    "\n",
    "### About the Dataset:\n",
    "\n",
    "**Columns:**\n",
    "- `Time`: Seconds since first transaction\n",
    "- `V1-V28`: Anonymized features (PCA-transformed for privacy)\n",
    "- `Amount`: Transaction amount in Euros (‚Ç¨)\n",
    "- `Class`: Target variable (0 = Legitimate, 1 = Fraud)\n",
    "\n",
    "**Privacy Note**: V1-V28 are transformed to protect customer identity while keeping fraud patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('creditcard.csv')\n",
    "\n",
    "print(\"üìä Dataset loaded successfully!\")\n",
    "print(f\"\\nShape: {df.shape[0]:,} transactions, {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst 5 transactions:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "fraud_count = (df['Class'] == 1).sum()\n",
    "legit_count = (df['Class'] == 0).sum()\n",
    "fraud_pct = (fraud_count / len(df)) * 100\n",
    "\n",
    "print(\"üö® Class Distribution:\")\n",
    "print(f\"   Legitimate: {legit_count:,} ({100-fraud_pct:.3f}%)\")\n",
    "print(f\"   Fraud: {fraud_count:,} ({fraud_pct:.3f}%)\")\n",
    "print(f\"   Imbalance Ratio: {legit_count/fraud_count:.0f}:1\")\n",
    "print(f\"\\n‚ö†Ô∏è  This is HIGHLY IMBALANCED - a key challenge in fraud detection!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize class distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Count plot\n",
    "df['Class'].value_counts().plot(kind='bar', ax=axes[0], color=['green', 'red'], alpha=0.7)\n",
    "axes[0].set_title('Transaction Class Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Class (0=Legitimate, 1=Fraud)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticklabels(['Legitimate', 'Fraud'], rotation=0)\n",
    "\n",
    "# Amount distribution by class\n",
    "df[df['Class']==0]['Amount'].hist(bins=50, ax=axes[1], alpha=0.5, label='Legitimate', color='green')\n",
    "df[df['Class']==1]['Amount'].hist(bins=50, ax=axes[1], alpha=0.5, label='Fraud', color='red')\n",
    "axes[1].set_title('Transaction Amount Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Amount (‚Ç¨)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].legend()\n",
    "axes[1].set_xlim([0, 500])  # Focus on typical amounts\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"üí° Observations:\")\n",
    "print(\"   ‚Ä¢ Fraud is extremely rare (0.17%)\")\n",
    "print(\"   ‚Ä¢ Fraud and legitimate transactions have different amount patterns\")\n",
    "print(\"   ‚Ä¢ This imbalance is typical in real-world fraud detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Prepare Data for Training\n",
    "\n",
    "### What we're doing:\n",
    "1. **Separate features (X) from target (y)**: X = what we know, y = what we predict\n",
    "2. **Split into train and test sets**: Train to learn, test to evaluate\n",
    "3. **Use stratified split**: Ensures both sets have similar fraud rates\n",
    "\n",
    "### Why 80/20 split?\n",
    "- **80% for training**: Model needs data to learn patterns\n",
    "- **20% for testing**: Evaluate on unseen data (simulates real-world use)\n",
    "- **Stratified**: Maintains the 0.17% fraud rate in both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target\n",
    "X = df.drop('Class', axis=1)  # All columns except 'Class'\n",
    "y = df['Class']                # Only the 'Class' column\n",
    "\n",
    "print(\"üìä Data Separation:\")\n",
    "print(f\"   Features (X): {X.shape}\")\n",
    "print(f\"   Target (y): {y.shape}\")\n",
    "print(f\"\\n   Features: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,           # 20% for testing\n",
    "    random_state=RANDOM_SEED, # Reproducible results\n",
    "    stratify=y               # Keep class distribution\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Train-Test Split Complete!\")\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"   X_train: {X_train.shape}\")\n",
    "print(f\"   y_train: {y_train.shape}\")\n",
    "print(f\"   Fraud rate: {(y_train.sum()/len(y_train)*100):.3f}%\")\n",
    "\n",
    "print(f\"\\nTest Set:\")\n",
    "print(f\"   X_test: {X_test.shape}\")\n",
    "print(f\"   y_test: {y_test.shape}\")\n",
    "print(f\"   Fraud rate: {(y_test.sum()/len(y_test)*100):.3f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ Fraud rates are similar - stratification worked!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Build and Train XGBoost Model\n",
    "\n",
    "### Key Hyperparameters Explained:\n",
    "\n",
    "1. **n_estimators=100**: Build 100 decision trees\n",
    "   - More trees = better learning but slower\n",
    "   - 100 is a good starting point\n",
    "\n",
    "2. **max_depth=6**: Each tree can be 6 levels deep\n",
    "   - Deeper = more complex patterns but risk overfitting\n",
    "   - 6 is a balanced choice\n",
    "\n",
    "3. **learning_rate=0.1**: How much each tree contributes\n",
    "   - Lower = slower learning but potentially better\n",
    "   - 0.1 is standard\n",
    "\n",
    "4. **scale_pos_weight=578**: Handle class imbalance\n",
    "   - Formula: (# legitimate) / (# fraud)\n",
    "   - Tells model to pay more attention to fraud\n",
    "   - **Critical for imbalanced data!**\n",
    "\n",
    "5. **eval_metric='auc'**: Use AUC to measure performance\n",
    "   - Better than accuracy for imbalanced data\n",
    "   - Measures ability to distinguish fraud from legitimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate scale_pos_weight for class imbalance\n",
    "scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "\n",
    "print(f\"‚öñÔ∏è  Handling Class Imbalance:\")\n",
    "print(f\"   scale_pos_weight = {scale_pos_weight:.0f}\")\n",
    "print(f\"   This tells XGBoost: 'Fraud is {scale_pos_weight:.0f}x more important!'\")\n",
    "print(f\"   Without this, model would just predict everything as legitimate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create XGBoost classifier\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,                    # Number of trees\n",
    "    max_depth=6,                         # Tree depth\n",
    "    learning_rate=0.1,                   # Learning rate\n",
    "    scale_pos_weight=scale_pos_weight,   # Handle imbalance\n",
    "    eval_metric='auc',                   # Evaluation metric\n",
    "    random_state=RANDOM_SEED,            # Reproducibility\n",
    "    use_label_encoder=False\n",
    ")\n",
    "\n",
    "print(\"üîß XGBoost Model Initialized!\")\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"   ‚Ä¢ Trees: {xgb_model.n_estimators}\")\n",
    "print(f\"   ‚Ä¢ Depth: {xgb_model.max_depth}\")\n",
    "print(f\"   ‚Ä¢ Learning rate: {xgb_model.learning_rate}\")\n",
    "print(f\"   ‚Ä¢ Scale pos weight: {scale_pos_weight:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"üöÄ Training XGBoost model...\")\n",
    "print(\"   This may take 30-60 seconds...\\n\")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)],\n",
    "    verbose=False  # Set to True to see training progress\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Model Training Complete!\")\n",
    "print(f\"   Trained on {len(X_train):,} transactions\")\n",
    "print(f\"   Model is ready to detect fraud!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluate Model Performance\n",
    "\n",
    "### Understanding Fraud Detection Metrics:\n",
    "\n",
    "**Why not just use Accuracy?**\n",
    "- If we predict \"no fraud\" for everything, we get 99.83% accuracy!\n",
    "- But we miss ALL fraud ‚Üí Useless model\n",
    "- **We need better metrics for imbalanced data**\n",
    "\n",
    "### Key Metrics for Fraud Detection:\n",
    "\n",
    "1. **Recall (Fraud Catch Rate)**\n",
    "   - What % of actual fraud did we catch?\n",
    "   - Formula: True Positives / (True Positives + False Negatives)\n",
    "   - **Higher is better** - we want to catch fraud!\n",
    "\n",
    "2. **Precision**\n",
    "   - Of transactions we flagged, what % were actually fraud?\n",
    "   - Formula: True Positives / (True Positives + False Positives)\n",
    "   - **Higher is better** - avoid blocking legitimate customers\n",
    "\n",
    "3. **F1-Score**\n",
    "   - Balance between Precision and Recall\n",
    "   - Harmonic mean of both\n",
    "\n",
    "4. **AUC-ROC**\n",
    "   - Overall ability to distinguish fraud from legitimate\n",
    "   - 0.5 = random guessing, 1.0 = perfect\n",
    "   - **Best single metric for imbalanced data**\n",
    "\n",
    "### Business Impact:\n",
    "- **False Negative (FN)**: Missed fraud ‚Üí Lost money\n",
    "- **False Positive (FP)**: Blocked legitimate customer ‚Üí Lost customer, bad UX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "y_pred_proba = xgb_model.predict_proba(X_test)[:, 1]  # Probability of fraud\n",
    "\n",
    "print(\"üéØ Predictions Generated!\")\n",
    "print(f\"   Predicted {(y_pred==1).sum()} fraudulent transactions\")\n",
    "print(f\"   Predicted {(y_pred==0).sum()} legitimate transactions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"üìä Model Performance Metrics:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision: {precision:.4f} ({precision*100:.2f}%)\")\n",
    "print(f\"Recall:    {recall:.4f} ({recall*100:.2f}%) ‚Üê Fraud catch rate\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"AUC-ROC:   {auc:.4f} ‚Üê Overall performance\")\n",
    "\n",
    "print(\"\\nüí° What This Means:\")\n",
    "print(f\"   ‚Ä¢ We catch {recall*100:.1f}% of all fraud\")\n",
    "print(f\"   ‚Ä¢ When we flag fraud, we're right {precision*100:.1f}% of the time\")\n",
    "print(f\"   ‚Ä¢ AUC of {auc:.2f} = {'Excellent' if auc >= 0.9 else 'Good' if auc >= 0.8 else 'Fair'} discrimination\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\nüìä Confusion Matrix Breakdown:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"True Negatives (TN):  {tn:5,} ‚úÖ Correctly identified as legitimate\")\n",
    "print(f\"False Positives (FP): {fp:5,} ‚ùå Legitimate flagged as fraud\")\n",
    "print(f\"False Negatives (FN): {fn:5,} ‚ùå Fraud that we missed (BAD!)\")\n",
    "print(f\"True Positives (TP):  {tp:5,} ‚úÖ Correctly caught fraud\")\n",
    "\n",
    "print(\"\\nüí∞ Business Impact:\")\n",
    "print(f\"   ‚Ä¢ Fraud caught: {tp} out of {tp+fn} ({tp/(tp+fn)*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Fraud missed: {fn} (potential losses)\")\n",
    "print(f\"   ‚Ä¢ Customers inconvenienced: {fp} (false alarms)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Legitimate', 'Fraud'],\n",
    "            yticklabels=['Legitimate', 'Fraud'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Fraud Detection', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Green (TN, TP): Correct predictions\")\n",
    "print(\"‚ùå Red (FP, FN): Errors we need to minimize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Feature Importance\n",
    "\n",
    "### Why Feature Importance Matters:\n",
    "\n",
    "1. **Interpretability**: Understand what drives fraud predictions\n",
    "2. **Trust**: Verify model makes sense (not just a black box)\n",
    "3. **Optimization**: Focus on most important features\n",
    "4. **Compliance**: Explain decisions to regulators\n",
    "\n",
    "### What to Look For:\n",
    "- Which features have highest importance?\n",
    "- Do they make business sense?\n",
    "- Are there surprising patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"üìä Top 10 Most Important Features:\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüí° Insight:\")\n",
    "top_feature = feature_importance.iloc[0]['Feature']\n",
    "print(f\"   '{top_feature}' is the most important fraud indicator\")\n",
    "print(f\"   Top 3 features account for {feature_importance.head(3)['Importance'].sum()*100:.1f}% of importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance (top 10)\n",
    "plt.figure(figsize=(10, 6))\n",
    "top_features = feature_importance.head(10)\n",
    "plt.barh(top_features['Feature'], top_features['Importance'], color='steelblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.title('Top 10 Features for Fraud Detection', fontsize=14, fontweight='bold')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ These are the key fraud indicators the model learned!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Make Predictions on New Transactions\n",
    "\n",
    "### Real-World Application:\n",
    "\n",
    "In production, this model would:\n",
    "1. Receive transaction data in real-time\n",
    "2. Score each transaction (0-1 probability)\n",
    "3. Flag high-risk transactions for review\n",
    "4. Block or challenge suspicious transactions\n",
    "\n",
    "### Risk Levels:\n",
    "- **High Risk** (>70%): Block transaction, send alert\n",
    "- **Medium Risk** (40-70%): Challenge with 2FA/OTP\n",
    "- **Low Risk** (<40%): Approve automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take 5 sample transactions (mix of fraud and legitimate)\n",
    "sample_indices = [10, 100, 1000, 5000, 10000]\n",
    "sample_transactions = X_test.iloc[sample_indices]\n",
    "sample_actual = y_test.iloc[sample_indices]\n",
    "\n",
    "# Make predictions\n",
    "sample_predictions = xgb_model.predict(sample_transactions)\n",
    "sample_probabilities = xgb_model.predict_proba(sample_transactions)[:, 1]\n",
    "\n",
    "# Create results DataFrame\n",
    "results = pd.DataFrame({\n",
    "    'Transaction_ID': sample_indices,\n",
    "    'Actual': ['Fraud' if x==1 else 'Legit' for x in sample_actual],\n",
    "    'Predicted': ['Fraud' if x==1 else 'Legit' for x in sample_predictions],\n",
    "    'Fraud_Probability': [f\"{x:.2%}\" for x in sample_probabilities],\n",
    "    'Risk_Level': ['High' if x>0.7 else 'Medium' if x>0.4 else 'Low' for x in sample_probabilities],\n",
    "    'Correct': ['‚úÖ' if a==p else '‚ùå' for a, p in zip(sample_actual, sample_predictions)]\n",
    "})\n",
    "\n",
    "print(\"üéØ Sample Transaction Predictions:\")\n",
    "print(\"=\"*80)\n",
    "print(results.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° How to Read This:\")\n",
    "print(\"   ‚Ä¢ Fraud_Probability: Model's confidence (0-100%)\")\n",
    "print(\"   ‚Ä¢ Risk_Level: Action to take\")\n",
    "print(\"   ‚Ä¢ Correct: Did model get it right?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save the Model\n",
    "\n",
    "### Why Save Models?\n",
    "\n",
    "1. **Deployment**: Use in production systems\n",
    "2. **Sharing**: Share with team members\n",
    "3. **Versioning**: Track different model versions\n",
    "4. **Efficiency**: Don't retrain every time\n",
    "\n",
    "### File Formats:\n",
    "- **JSON (.json)**: XGBoost native format, best for production\n",
    "- **Pickle (.pkl)**: Python format, includes full object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Create models directory\n",
    "model_dir = Path('/home/sagemaker-user/models')\n",
    "model_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"üíæ Saving Model...\")\n",
    "\n",
    "# Save as JSON (recommended for production)\n",
    "json_path = model_dir / 'fraud_detection_model.json'\n",
    "xgb_model.save_model(str(json_path))\n",
    "print(f\"‚úÖ Saved as JSON: {json_path}\")\n",
    "\n",
    "# Save as Pickle (includes Python object)\n",
    "pickle_path = model_dir / 'fraud_detection_model.pkl'\n",
    "with open(pickle_path, 'wb') as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "print(f\"‚úÖ Saved as Pickle: {pickle_path}\")\n",
    "\n",
    "# Save model metadata\n",
    "metadata = {\n",
    "    'model_type': 'XGBoost Classifier',\n",
    "    'training_date': pd.Timestamp.now().isoformat(),\n",
    "    'n_training_samples': len(X_train),\n",
    "    'n_features': len(X_train.columns),\n",
    "    'performance': {\n",
    "        'accuracy': float(accuracy),\n",
    "        'precision': float(precision),\n",
    "        'recall': float(recall),\n",
    "        'f1_score': float(f1),\n",
    "        'auc_roc': float(auc)\n",
    "    },\n",
    "    'hyperparameters': {\n",
    "        'n_estimators': xgb_model.n_estimators,\n",
    "        'max_depth': xgb_model.max_depth,\n",
    "        'learning_rate': xgb_model.learning_rate,\n",
    "        'scale_pos_weight': float(scale_pos_weight)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = model_dir / 'model_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"‚úÖ Saved metadata: {metadata_path}\")\n",
    "\n",
    "print(\"\\n‚úÖ Model saved successfully!\")\n",
    "print(\"   Ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate loading the model\n",
    "print(\"üìÇ Loading Saved Model...\")\n",
    "\n",
    "# Load from JSON\n",
    "loaded_model = xgb.XGBClassifier()\n",
    "loaded_model.load_model(str(json_path))\n",
    "\n",
    "# Verify it works\n",
    "test_prediction = loaded_model.predict(X_test[:5])\n",
    "original_prediction = xgb_model.predict(X_test[:5])\n",
    "\n",
    "if np.array_equal(test_prediction, original_prediction):\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(\"   Predictions match original model\")\n",
    "    print(\"   Ready to use for real-time fraud detection!\")\n",
    "else:\n",
    "    print(\"‚ùå Warning: Loaded model predictions don't match!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Workshop Summary\n\n### üéâ Congratulations!\n\nYou've successfully built a fraud detection system using XGBoost!\n\n### What You Learned:\n\n‚úÖ **Understanding XGBoost**\n- How gradient boosting works (sequential tree learning)\n- Why it's the industry standard for fraud detection\n- Key advantages over other algorithms\n\n‚úÖ **Handling Imbalanced Data**\n- Fraud is rare (0.17% of transactions)\n- Used `scale_pos_weight` to balance classes\n- Chose appropriate metrics (Recall, Precision, AUC)\n\n‚úÖ **Building Production Models**\n- Trained on 284K real transactions\n- Achieved {auc*100:.1f}% AUC score\n- Catching {recall*100:.1f}% of fraud cases\n- Saved model for deployment\n\n‚úÖ **Model Interpretation**\n- Analyzed feature importance\n- Understood confusion matrix\n- Business impact of FP vs FN\n\n### Real-World Applications:\n\nThis same approach is used by:\n- **PayPal**: Real-time transaction monitoring\n- **Stripe**: Payment fraud prevention\n- **Financial institutions**: Credit card fraud detection\n- **Airbnb**: Booking fraud prevention\n\n### Next Steps:\n\n1. **Hyperparameter Tuning**: Experiment with different settings\n2. **Feature Engineering**: Create new features (time-based, aggregations)\n3. **Threshold Optimization**: Adjust decision threshold for business needs\n4. **A/B Testing**: Compare with existing fraud systems\n5. **Deployment**: Build REST API or SageMaker endpoint\n\n### Key Takeaways:\n\nüí° **Accuracy is NOT enough** for imbalanced data\nüí° **Class weighting** is critical for rare event detection\nüí° **Business context** matters - FP and FN have different costs\nüí° **Feature importance** builds trust and interpretability\nüí° **XGBoost** is production-ready out of the box\n\n### Resources:\n\n- **XGBoost Docs**: https://xgboost.readthedocs.io/\n- **Kaggle Competition**: https://www.kaggle.com/c/ieee-fraud-detection\n- **Research Paper**: https://arxiv.org/abs/1603.02754\n\n---\n\n**Thank you for participating! üöÄ**\n\nQuestions? Discuss with your instructor or fellow participants!"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}