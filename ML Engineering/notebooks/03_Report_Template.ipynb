{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: MLOps Capstone Project Report\n",
    "\n",
    "## Team Information\n",
    "- **Team Name:** [Your Team Name]\n",
    "- **Team Members:** [List all team members]\n",
    "- **Date:** [Report Date]\n",
    "- **Project:** Credit Default Prediction MLOps Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Executive Summary\n",
    "\n",
    "*Provide a brief overview (3-5 sentences) of your project, key findings, and recommendations.*\n",
    "\n",
    "[Your executive summary here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Model Training (H2O Driverless AI)\n",
    "\n",
    "### 1.1 Training Configuration\n",
    "- **Dataset:** training_dataset.csv\n",
    "- **Number of samples:** [e.g., 10,000]\n",
    "- **Number of features:** [e.g., 24]\n",
    "- **Target variable:** DEFAULT_PAYMENT_NEXT_MONTH\n",
    "- **Training time:** [e.g., 45 minutes]\n",
    "\n",
    "### 1.2 Model Details\n",
    "- **Model type:** [e.g., Gradient Boosting, XGBoost, LightGBM ensemble]\n",
    "- **Number of models in ensemble:** [if applicable]\n",
    "- **Feature engineering transformations applied:** [List key transformations]\n",
    "\n",
    "### 1.3 Training Metrics\n",
    "| Metric | Value |\n",
    "|--------|-------|\n",
    "| Training AUC | [value] |\n",
    "| Validation AUC | [value] |\n",
    "| Training Accuracy | [value] |\n",
    "| Validation Accuracy | [value] |\n",
    "\n",
    "### 1.4 Feature Importance\n",
    "List the top 5 most important features:\n",
    "1. [Feature name] - [Importance score]\n",
    "2. [Feature name] - [Importance score]\n",
    "3. [Feature name] - [Importance score]\n",
    "4. [Feature name] - [Importance score]\n",
    "5. [Feature name] - [Importance score]\n",
    "\n",
    "### 1.5 Screenshots\n",
    "*Include relevant screenshots from H2O Driverless AI console*\n",
    "\n",
    "[Insert training dashboard screenshot]\n",
    "[Insert feature importance chart]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Model Deployment (H2O MLOps)\n",
    "\n",
    "### 2.1 Deployment Configuration\n",
    "- **Deployment ID:** [Your deployment ID]\n",
    "- **Endpoint URL:** [Your endpoint URL]\n",
    "- **Deployment type:** [e.g., REST API]\n",
    "- **Infrastructure:** [e.g., Kubernetes, Docker]\n",
    "\n",
    "### 2.2 Deployment Steps\n",
    "1. Created project in H2O MLOps UI\n",
    "2. Uploaded MOJO.zip file\n",
    "3. Registered as new model\n",
    "4. Created deployment with following options:\n",
    "   - [List deployment options selected]\n",
    "5. Obtained scoring endpoint\n",
    "\n",
    "### 2.3 Screenshots\n",
    "*Include relevant screenshots from H2O MLOps console*\n",
    "\n",
    "[Insert deployment configuration screenshot]\n",
    "[Insert endpoint details screenshot]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Inference API Performance\n",
    "\n",
    "### 3.1 Load Inference Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Load inference metrics\n",
    "with open('../reports/inference_metrics.json', 'r') as f:\n",
    "    inference_metrics = json.load(f)\n",
    "\n",
    "print(\"Inference Metrics Loaded\")\n",
    "print(f\"Timestamp: {inference_metrics.get('timestamp', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 API Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"API PERFORMANCE METRICS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples scored: {inference_metrics.get('total_samples', 'N/A'):,}\")\n",
    "print(f\"Batch size: {inference_metrics.get('batch_size', 'N/A')}\")\n",
    "print(f\"Total batches: {inference_metrics.get('total_batches', 'N/A')}\")\n",
    "print(f\"Failed batches: {inference_metrics.get('failed_batches', 'N/A')}\")\n",
    "print(f\"Success rate: {inference_metrics.get('success_rate', 'N/A'):.2f}%\")\n",
    "print(f\"\\nTotal time: {inference_metrics.get('total_time_seconds', 0):.2f} seconds\")\n",
    "print(f\"Throughput: {inference_metrics.get('throughput_samples_per_second', 0):.2f} samples/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Latency Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latency_stats = inference_metrics.get('latency_stats', {})\n",
    "\n",
    "print(\"LATENCY STATISTICS (milliseconds)\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Average latency:  {latency_stats.get('avg_ms', 0):.2f} ms\")\n",
    "print(f\"Minimum latency:  {latency_stats.get('min_ms', 0):.2f} ms\")\n",
    "print(f\"Maximum latency:  {latency_stats.get('max_ms', 0):.2f} ms\")\n",
    "print(f\"P50 (Median):     {latency_stats.get('p50_ms', 0):.2f} ms\")\n",
    "print(f\"P95:              {latency_stats.get('p95_ms', 0):.2f} ms\")\n",
    "print(f\"P99:              {latency_stats.get('p99_ms', 0):.2f} ms\")\n",
    "print(f\"\\nTotal API time:   {latency_stats.get('total_ms', 0):.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Latency Interpretation\n",
    "\n",
    "*Analyze your latency results:*\n",
    "\n",
    "- **Is the average latency acceptable for production use?** [Your analysis]\n",
    "- **Are there any concerning outliers (P95/P99)?** [Your analysis]\n",
    "- **How does throughput scale with batch size?** [Your analysis]\n",
    "- **Recommendations for optimization:** [Your recommendations]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Monitoring Results\n",
    "\n",
    "### 4.1 Load Monitoring Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load monitoring report\n",
    "with open('../reports/monitoring_report.json', 'r') as f:\n",
    "    monitoring_report = json.load(f)\n",
    "\n",
    "print(\"Monitoring Report Loaded\")\n",
    "print(f\"Timestamp: {monitoring_report.get('timestamp', 'N/A')}\")\n",
    "print(f\"Samples evaluated: {monitoring_report.get('samples_evaluated', 'N/A'):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = monitoring_report.get('classification_metrics', {})\n",
    "\n",
    "print(\"CLASSIFICATION PERFORMANCE\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Accuracy:             {metrics.get('accuracy', 0):.4f} ({metrics.get('accuracy', 0)*100:.2f}%)\")\n",
    "print(f\"Precision:            {metrics.get('precision', 0):.4f}\")\n",
    "print(f\"Recall (Sensitivity): {metrics.get('recall', 0):.4f}\")\n",
    "print(f\"Specificity:          {metrics.get('specificity', 0):.4f}\")\n",
    "print(f\"F1 Score:             {metrics.get('f1_score', 0):.4f}\")\n",
    "print(f\"ROC AUC:              {metrics.get('roc_auc', 0):.4f}\")\n",
    "print(f\"\\nFalse Positive Rate:  {metrics.get('false_positive_rate', 0):.4f}\")\n",
    "print(f\"False Negative Rate:  {metrics.get('false_negative_rate', 0):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = monitoring_report.get('confusion_matrix', {})\n",
    "\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 50)\n",
    "print(\"                    Predicted\")\n",
    "print(\"                    No Default    Default\")\n",
    "print(f\"Actual  No Default    {cm.get('true_negatives', 0):>6}       {cm.get('false_positives', 0):>6}\")\n",
    "print(f\"        Default       {cm.get('false_negatives', 0):>6}       {cm.get('true_positives', 0):>6}\")\n",
    "\n",
    "# Display saved confusion matrix image\n",
    "from IPython.display import Image, display\n",
    "try:\n",
    "    display(Image(filename='../reports/confusion_matrix.png'))\n",
    "except:\n",
    "    print(\"\\n[Confusion matrix visualization not found]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Business Impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = monitoring_report.get('business_metrics', {})\n",
    "\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total samples:               {bm.get('total_samples', 0):,}\")\n",
    "print(f\"Actual defaults:             {bm.get('actual_defaults', 0):,}\")\n",
    "print(f\"Predicted defaults:          {bm.get('predicted_defaults', 0):,}\")\n",
    "print(f\"\\nCorrectly identified:        {bm.get('correctly_identified_defaults', 0):,}\")\n",
    "print(f\"Missed defaults:             {bm.get('missed_defaults', 0):,}\")\n",
    "print(f\"False alarms:                {bm.get('false_alarms', 0):,}\")\n",
    "print(f\"\\nTotal cost:                  ${bm.get('total_cost', 0):,}\")\n",
    "print(f\"Average cost per sample:     ${bm.get('avg_cost_per_sample', 0):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Model Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift = monitoring_report.get('drift_detection', {})\n",
    "alerts = drift.get('alerts', [])\n",
    "\n",
    "print(\"MODEL DRIFT DETECTION\")\n",
    "print(\"=\" * 50)\n",
    "if alerts:\n",
    "    print(f\"ALERTS RAISED: {len(alerts)}\")\n",
    "    for alert in alerts:\n",
    "        print(f\"  [!] {alert}\")\n",
    "else:\n",
    "    print(\"No model drift detected.\")\n",
    "    print(\"Performance within acceptable thresholds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Visualizations\n",
    "\n",
    "*Include the generated plots from model monitoring*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "import os\n",
    "\n",
    "# Display ROC and PR curves\n",
    "if os.path.exists('../reports/roc_pr_curves.png'):\n",
    "    print(\"ROC and Precision-Recall Curves:\")\n",
    "    display(Image(filename='../reports/roc_pr_curves.png'))\n",
    "else:\n",
    "    print(\"[ROC/PR curves not found]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display probability distribution\n",
    "if os.path.exists('../reports/probability_distribution.png'):\n",
    "    print(\"Probability Distribution:\")\n",
    "    display(Image(filename='../reports/probability_distribution.png'))\n",
    "else:\n",
    "    print(\"[Probability distribution plot not found]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Analysis and Interpretation\n",
    "\n",
    "### 5.1 Model Performance Analysis\n",
    "\n",
    "*Provide your analysis of the model performance metrics:*\n",
    "\n",
    "**Strengths:**\n",
    "- [List model strengths based on metrics]\n",
    "\n",
    "**Weaknesses:**\n",
    "- [List model weaknesses based on metrics]\n",
    "\n",
    "**Key Insights:**\n",
    "- [Your insights from the confusion matrix]\n",
    "- [Your insights from ROC/PR curves]\n",
    "- [Your insights from probability calibration]\n",
    "\n",
    "### 5.2 Business Impact Interpretation\n",
    "\n",
    "*Analyze the business implications:*\n",
    "\n",
    "- **Cost of errors:** [Your analysis of FN vs FP costs]\n",
    "- **Risk mitigation:** [How well does the model mitigate default risk?]\n",
    "- **Operational efficiency:** [Impact on business operations]\n",
    "\n",
    "### 5.3 Threshold Optimization\n",
    "\n",
    "*Would you recommend a different classification threshold?*\n",
    "\n",
    "- Current threshold: 0.5\n",
    "- Recommended threshold: [Your recommendation]\n",
    "- Justification: [Why this threshold is better for the business case]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Challenges and Lessons Learned\n",
    "\n",
    "### 6.1 Technical Challenges\n",
    "*Describe any technical challenges encountered:*\n",
    "\n",
    "1. [Challenge 1] - [How you resolved it]\n",
    "2. [Challenge 2] - [How you resolved it]\n",
    "3. [Challenge 3] - [How you resolved it]\n",
    "\n",
    "### 6.2 Lessons Learned\n",
    "*Key takeaways from this project:*\n",
    "\n",
    "1. [Lesson 1]\n",
    "2. [Lesson 2]\n",
    "3. [Lesson 3]\n",
    "\n",
    "### 6.3 Best Practices Identified\n",
    "*MLOps best practices you identified:*\n",
    "\n",
    "1. [Best practice 1]\n",
    "2. [Best practice 2]\n",
    "3. [Best practice 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Recommendations\n",
    "\n",
    "### 7.1 Model Improvements\n",
    "*Recommendations to improve model performance:*\n",
    "\n",
    "1. [Recommendation 1]\n",
    "2. [Recommendation 2]\n",
    "3. [Recommendation 3]\n",
    "\n",
    "### 7.2 Infrastructure Improvements\n",
    "*Recommendations for deployment and API:*\n",
    "\n",
    "1. [Recommendation 1]\n",
    "2. [Recommendation 2]\n",
    "\n",
    "### 7.3 Monitoring Enhancements\n",
    "*Recommendations for ongoing monitoring:*\n",
    "\n",
    "1. [Recommendation 1]\n",
    "2. [Recommendation 2]\n",
    "\n",
    "### 7.4 Future Work\n",
    "*Potential future enhancements:*\n",
    "\n",
    "1. [Future enhancement 1]\n",
    "2. [Future enhancement 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusion\n",
    "\n",
    "*Summarize your project findings and final thoughts (2-3 paragraphs):*\n",
    "\n",
    "[Your conclusion here]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Appendix\n",
    "\n",
    "### A. Complete Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print complete metrics for reference\n",
    "print(\"COMPLETE INFERENCE METRICS:\")\n",
    "print(json.dumps(inference_metrics, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nCOMPLETE MONITORING REPORT:\")\n",
    "print(json.dumps(monitoring_report, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. File Manifest\n",
    "\n",
    "List all files generated/used in this project:\n",
    "\n",
    "**Data Files:**\n",
    "- `data/training_dataset.csv` - Training data (10,000 samples)\n",
    "- `data/inference_dataset.csv` - Inference data (2,000 samples)\n",
    "- `data/ground_truth.csv` - Actual labels for inference data\n",
    "- `data/scores.csv` - Model predictions\n",
    "\n",
    "**Report Files:**\n",
    "- `reports/inference_metrics.json` - API performance metrics\n",
    "- `reports/monitoring_report.json` - Model monitoring metrics\n",
    "- `reports/confusion_matrix.png` - Confusion matrix visualization\n",
    "- `reports/roc_pr_curves.png` - ROC and PR curves\n",
    "- `reports/probability_distribution.png` - Probability distribution plots\n",
    "\n",
    "**Model Artifacts:**\n",
    "- `mojo.zip` - H2O Driverless AI model export\n",
    "\n",
    "### C. Team Contributions\n",
    "\n",
    "| Team Member | Contribution |\n",
    "|-------------|-------------|\n",
    "| [Name 1] | [Their contribution] |\n",
    "| [Name 2] | [Their contribution] |\n",
    "| [Name 3] | [Their contribution] |\n",
    "| [Name 4] | [Their contribution] |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**End of Report**\n",
    "\n",
    "Report generated on: [Date]\n",
    "\n",
    "Submitted by: [Team Name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
