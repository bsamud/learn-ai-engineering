{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SageMaker ML Workshop: Building and Evaluating ML Models\n\n## Workshop Overview\n\nWelcome to this hands-on machine learning workshop! In this notebook, you'll learn how to:\n\n1. **Set up your ML environment** - Configure H2O and dependencies\n2. **Load and prepare data** - Import datasets for training\n3. **Build ML models** - Train a Gradient Boosting Machine (GBM)\n4. **Evaluate performance** - Assess model quality and metrics\n5. **Generate predictions** - Use your trained model for inference\n6. **Extract insights** - Analyze feature importance and interactions\n7. **Save artifacts** - Export models for deployment\n\n### About This Example\n\nWe'll use the **credit card fraud detection dataset** to predict fraudulent transactions. This is a binary classification problem that demonstrates real-world ML workflows with imbalanced data.\n\n---"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Section 1: Environment Setup\n\n### What is H2O?\n\nH2O is an open-source machine learning platform that provides:\n- Fast, scalable algorithms (GBM, Random Forest, Deep Learning, etc.)\n- Automatic feature engineering\n- Easy-to-use Python API\n- Built-in model validation and metrics\n\n### Step 1.1: Install Required Dependencies\n\nH2O requires Java 8+ to run. We'll install the dependencies using conda/pip."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install Java (required for H2O) and H2O package\n!conda install -y -c conda-forge openjdk=11 -q\n!pip install h2o -q\n\nprint(\"Dependencies installed successfully!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Step 1.2: Import Libraries and Initialize H2O\n\nNow we'll import the necessary Python libraries and start the H2O cluster."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import h2o\nfrom h2o.estimators import H2OGradientBoostingEstimator\n\n# Initialize H2O cluster\n# This starts a local H2O instance that runs in the background\nh2o.init()\n\nprint(\"H2O initialized successfully!\")\nprint(f\"H2O cluster is running at: {h2o.cluster().base_url}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Section 2: Data Loading and Preparation\n\n### About the Credit Card Fraud Dataset\n\nThis dataset contains anonymized credit card transactions:\n- **Target variable (Class)**: Whether the transaction is fraudulent (binary: 0 or 1)\n- **Features**: V1-V28 (PCA-transformed features), Amount, Time\n\n### Step 2.1: Import Dataset"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import the credit card fraud dataset into H2O\n# H2O loads data into its distributed in-memory format for fast processing\n# Note: For this workshop, we'll use a sample of the data for faster training\ndf = h2o.import_file(\"creditcard.csv\")\n\n# Sample the data for faster training (optional - remove for full dataset)\ndf = df.sample(n=10000, seed=42)\n\n# Display the first few rows\nprint(\"Dataset loaded successfully!\")\nprint(f\"\\nDataset shape: {df.shape}\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Define Features and Target\n",
    "\n",
    "In machine learning, we need to specify:\n",
    "- **Predictors (X)**: Input features used to make predictions\n",
    "- **Response (y)**: Target variable we want to predict\n",
    "\n",
    "We also need to convert categorical variables to factors (similar to R factors or pandas categorical types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Convert the target variable to categorical (factor)\ndf[\"Class\"] = df[\"Class\"].asfactor()\n\n# Define predictor columns (all columns except Time and Class)\npredictors = [col for col in df.columns if col not in [\"Time\", \"Class\"]]\n\n# Define response column\nresponse = \"Class\"\n\nprint(f\"Predictors: {predictors}\")\nprint(f\"Response: {response}\")\nprint(f\"\\nTarget variable distribution:\")\ndf[response].table()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 3: Model Training\n",
    "\n",
    "### Understanding Gradient Boosting Machines (GBM)\n",
    "\n",
    "GBM is an ensemble learning method that:\n",
    "1. Builds multiple decision trees sequentially\n",
    "2. Each tree learns from the errors of previous trees\n",
    "3. Combines predictions from all trees for final output\n",
    "\n",
    "### Key Hyperparameters:\n",
    "\n",
    "- **nfolds**: Number of cross-validation folds (5 means 80% train, 20% validation per fold)\n",
    "- **seed**: Random seed for reproducibility\n",
    "- **keep_cross_validation_predictions**: Saves predictions for analysis\n",
    "\n",
    "### Step 3.1: Configure and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize the Gradient Boosting Model\nfraud_gbm = H2OGradientBoostingEstimator(\n    nfolds=5,                                    # 5-fold cross-validation\n    seed=1111,                                   # For reproducibility\n    keep_cross_validation_predictions=True      # Keep CV predictions for analysis\n)\n\n# Train the model\nprint(\"üöÄ Starting model training...\\n\")\nfraud_gbm.train(\n    x=predictors,           # Feature columns\n    y=response,             # Target column\n    training_frame=df       # Training dataset\n)\n\nprint(\"‚úÖ Model training complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.2: View Model Details\n",
    "\n",
    "Let's examine the trained model's architecture and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display model summary\nprint(\"üìä Model Details\")\nprint(\"=\" * 80)\nprint(f\"Model Type: {fraud_gbm}\")\nprint(f\"Model Key: {fraud_gbm.model_id}\")\nprint(\"\\n\")\n\n# Show model summary with tree statistics\nprint(\"Model Summary:\")\nprint(fraud_gbm.summary())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 4: Model Evaluation\n",
    "\n",
    "### Understanding Model Metrics\n",
    "\n",
    "For binary classification, we evaluate models using:\n",
    "- **MSE (Mean Squared Error)**: Average squared difference between predictions and actual values\n",
    "- **RMSE (Root Mean Squared Error)**: Square root of MSE, in the same units as the target\n",
    "- **LogLoss**: Measures the performance of classification models (lower is better)\n",
    "- **AUC (Area Under Curve)**: Measures classifier's ability to distinguish between classes (higher is better)\n",
    "- **Confusion Matrix**: Shows true positives, false positives, etc.\n",
    "\n",
    "### Step 4.1: Get Model Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get comprehensive model performance\nperf = fraud_gbm.model_performance()\n\nprint(\"üìà Model Performance Metrics\")\nprint(\"=\" * 80)\nprint(perf)\nprint(\"\\n\")\n\n# Extract key metrics\nprint(\"Key Metrics Summary:\")\nprint(f\"  MSE:  {perf.mse():.6f}\")\nprint(f\"  RMSE: {perf.rmse():.6f}\")\nprint(f\"  LogLoss: {perf.logloss():.6f}\")\nprint(f\"  AUC: {perf.auc():.6f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2: Analyze Cross-Validation Results\n",
    "\n",
    "Cross-validation helps us understand how well our model generalizes to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get cross-validation metrics\nprint(\"üîÑ Cross-Validation Performance\")\nprint(\"=\" * 80)\nprint(f\"CV MSE:  {fraud_gbm.mse(xval=True):.6f}\")\nprint(f\"CV RMSE: {fraud_gbm.rmse(xval=True):.6f}\")\nprint(f\"CV AUC:  {fraud_gbm.auc(xval=True):.6f}\")\n\n# Display confusion matrix\nprint(\"\\nConfusion Matrix:\")\nprint(fraud_gbm.confusion_matrix())"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 5: Making Predictions\n",
    "\n",
    "### Step 5.1: Generate Predictions on Training Data\n",
    "\n",
    "Now let's use our trained model to make predictions. In a real scenario, you would do this on a separate test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Generate predictions on the dataset\npred = fraud_gbm.predict(df)\n\nprint(\"üéØ Predictions Generated\")\nprint(\"=\" * 80)\nprint(\"\\nFirst 10 predictions:\")\nprint(pred.head(10))\n\n# The prediction frame contains:\n# - predict: The predicted class (0 or 1)\n# - p0: Probability of class 0\n# - p1: Probability of class 1"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 6: Model Interpretation\n",
    "\n",
    "### Understanding Feature Importance and Interactions\n",
    "\n",
    "Feature importance tells us which variables have the most impact on predictions.\n",
    "Feature interactions reveal how features work together.\n",
    "\n",
    "### Step 6.1: Extract Feature Interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract feature interactions\nfeature_interactions = fraud_gbm.feature_interaction()\n\nprint(\"üîç Feature Interactions\")\nprint(\"=\" * 80)\nprint(feature_interactions)\nprint(\"\\nHigher values indicate stronger feature interactions.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.2: Calculate Friedman and Popescu's H Statistics\n",
    "\n",
    "H-statistics measure the strength of interaction between features:\n",
    "- **H = 0**: No interaction\n",
    "- **H > 0**: Features interact (higher values = stronger interaction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get Friedman and Popescu's H statistics for specific features\nh = fraud_gbm.h(df, ['V1', 'V2'])\n\nprint(\"üìä Friedman and Popescu's H Statistics\")\nprint(\"=\" * 80)\nprint(f\"H-statistic for V1 and V2 interaction: {h}\")\nprint(\"\\nInterpretation:\")\nprint(\"  H ‚âà 0: Features are independent\")\nprint(\"  H > 0: Features interact in predictions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.3: Variable Importance Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get variable importance\nvar_importance = fraud_gbm.varimp(use_pandas=True)\n\nprint(\"üìä Variable Importance\")\nprint(\"=\" * 80)\nprint(var_importance)\n\n# Plot variable importance\nfraud_gbm.varimp_plot()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 7: Model Persistence\n",
    "\n",
    "### Saving Your Model\n",
    "\n",
    "After training a model, you need to save it for later use in production or for sharing with others.\n",
    "\n",
    "H2O models are saved in MOJO (Model Object, Optimized) format:\n",
    "- **Fast**: Optimized for low-latency predictions\n",
    "- **Portable**: Can be used in Java, Python, R, and other languages\n",
    "- **Small**: Compact file size\n",
    "\n",
    "### Step 7.1: Download Model Artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Download the model as MOJO format\nmodel_path = fraud_gbm.download_mojo('/home/sagemaker-user/fraud_gbm.mojo')\n\nprint(\"üíæ Model Saved Successfully!\")\nprint(\"=\" * 80)\nprint(f\"Model saved to: {model_path}\")\nprint(\"\\nYou can now:\")\nprint(\"  1. Deploy this model to production\")\nprint(\"  2. Share it with your team\")\nprint(\"  3. Load it in other environments\")\nprint(\"  4. Use it with H2O's MOJO scoring pipeline\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7.2: Loading a Saved Model (Optional)\n",
    "\n",
    "Here's how you would load the model back for future use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load a saved MOJO model\n",
    "# loaded_model = h2o.import_mojo('/home/sagemaker-user/pros_gbm.mojo')\n",
    "# predictions = loaded_model.predict(new_data)\n",
    "\n",
    "print(\"‚ÑπÔ∏è  Use the commented code above to load your saved model in future sessions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 8: Advanced Tips and Next Steps\n",
    "\n",
    "### Model Inspection Commands\n",
    "\n",
    "H2O provides several useful methods for model inspection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpful inspection commands\n",
    "print(\"üîß Additional Model Inspection Tools\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n1. Detailed model explanation:\")\n",
    "print(\"   Use: model.explain()\")\n",
    "print(\"\\n2. Toggle display tips:\")\n",
    "print(\"   Use: h2o.display.toggle_user_tips()\")\n",
    "print(\"\\n3. Access H2O Flow UI:\")\n",
    "print(f\"   Open: {h2o.cluster().base_url}\")\n",
    "print(\"\\n4. Get model as plain text:\")\n",
    "print(\"   Use: model.show()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "To improve model performance, try tuning these hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Example: Advanced model with tuned hyperparameters\n# Uncomment and experiment with these settings\n\n# advanced_gbm = H2OGradientBoostingEstimator(\n#     ntrees=100,              # Number of trees (default: 50)\n#     max_depth=6,             # Maximum tree depth (default: 5)\n#     learn_rate=0.1,          # Learning rate (default: 0.1)\n#     sample_rate=0.8,         # Row sampling rate (default: 1.0)\n#     col_sample_rate=0.8,     # Column sampling rate (default: 1.0)\n#     min_rows=10,             # Minimum observations per leaf (default: 10)\n#     nfolds=5,\n#     seed=1111\n# )\n# \n# advanced_gbm.train(x=predictors, y=response, training_frame=df)\n\nprint(\"üí° Experiment with hyperparameters to improve model performance!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Section 9: Workshop Summary\n",
    "\n",
    "### What You've Learned\n",
    "\n",
    "Congratulations! In this workshop, you've learned how to:\n",
    "\n",
    "‚úÖ **Set up an ML environment** with H2O and dependencies  \n",
    "‚úÖ **Load and prepare datasets** for training  \n",
    "‚úÖ **Train a Gradient Boosting Machine** classifier  \n",
    "‚úÖ **Evaluate model performance** using multiple metrics  \n",
    "‚úÖ **Generate predictions** on new data  \n",
    "‚úÖ **Interpret models** using feature importance and interactions  \n",
    "‚úÖ **Save and deploy models** as MOJO artifacts  \n",
    "\n",
    "### Best Practices for Production ML\n",
    "\n",
    "1. **Always split your data**: Use separate train/test sets\n",
    "2. **Cross-validate**: Use k-fold CV to assess generalization\n",
    "3. **Track experiments**: Log hyperparameters and metrics\n",
    "4. **Monitor models**: Track performance drift over time\n",
    "5. **Version models**: Keep track of model versions and lineage\n",
    "6. **Document everything**: Explain your modeling decisions\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "To continue your ML journey:\n",
    "\n",
    "- **Try different algorithms**: Random Forest, XGBoost, Deep Learning\n",
    "- **Feature engineering**: Create new features from existing ones\n",
    "- **AutoML**: Let H2O automatically find the best model\n",
    "- **Deploy to production**: Use SageMaker endpoints or batch transform\n",
    "- **Integrate with MLOps**: Add monitoring, logging, and CI/CD\n",
    "\n",
    "### Resources\n",
    "\n",
    "- H2O Documentation: https://docs.h2o.ai/\n",
    "- SageMaker Developer Guide: https://docs.aws.amazon.com/sagemaker/\n",
    "- H2O AutoML: https://docs.h2o.ai/h2o/latest-stable/h2o-docs/automl.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Clean Up\n",
    "\n",
    "Don't forget to shut down the H2O cluster when you're done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shutdown H2O cluster\n",
    "# h2o.cluster().shutdown()\n",
    "print(\"‚ö†Ô∏è  Uncomment the line above to shutdown H2O when finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Appendix: Exercise Challenges\n\n### Challenge 1: Data Splitting\nModify the code to split the data into train (70%), validation (15%), and test (15%) sets.\n\n```python\n# Hint: Use h2o.H2OFrame.split_frame()\ntrain, valid, test = df.split_frame(ratios=[0.7, 0.15], seed=1111)\n```\n\n### Challenge 2: Hyperparameter Grid Search\nImplement a grid search to find the best hyperparameters.\n\n```python\n# Hint: Use h2o.grid.H2OGridSearch\nfrom h2o.grid.grid_search import H2OGridSearch\n```\n\n### Challenge 3: Compare Multiple Algorithms\nTrain and compare GBM, Random Forest, and Deep Learning models.\n\n```python\n# Hint: Import from h2o.estimators\nfrom h2o.estimators import H2ORandomForestEstimator, H2ODeepLearningEstimator\n```\n\n### Challenge 4: Create a Confusion Matrix Heatmap\nVisualize the confusion matrix using matplotlib or seaborn.\n\n### Challenge 5: ROC Curve\nPlot the ROC curve and calculate the optimal threshold.\n\n```python\n# Hint: Use model.roc() and model.find_threshold_by_max_metric()\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}