{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Model Monitoring\n",
    "\n",
    "## Objective\n",
    "Evaluate model performance by comparing predictions (`scores.csv`) with actual outcomes (`ground_truth.csv`).\n",
    "\n",
    "## Prerequisites\n",
    "- Completed Step 4: API Inference Scoring\n",
    "- `scores.csv` file with model predictions\n",
    "- `ground_truth.csv` file with actual labels\n",
    "\n",
    "## Deliverables\n",
    "- Model performance metrics (accuracy, precision, recall, F1, AUC)\n",
    "- Confusion matrix analysis\n",
    "- Business impact assessment\n",
    "- Model drift detection\n",
    "- `monitoring_report.json` with comprehensive metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas numpy scikit-learn matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, confusion_matrix, classification_report,\n",
    "    roc_curve, precision_recall_curve\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "SCORES_PATH = \"../data/scores.csv\"\n",
    "GROUND_TRUTH_PATH = \"../data/ground_truth.csv\"\n",
    "REPORT_OUTPUT_PATH = \"../reports/monitoring_report.json\"\n",
    "\n",
    "# Performance thresholds (for drift detection)\n",
    "EXPECTED_ACCURACY = 0.70\n",
    "EXPECTED_AUC = 0.65\n",
    "MAX_PREDICTION_BIAS = 0.10\n",
    "\n",
    "print(\"Configuration loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "scores_df = pd.read_csv(SCORES_PATH)\n",
    "print(f\"Loaded scores:\")\n",
    "print(f\"  - Samples: {len(scores_df):,}\")\n",
    "print(f\"  - Columns: {list(scores_df.columns)}\")\n",
    "\n",
    "# Load ground truth\n",
    "ground_truth_df = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "print(f\"\\nLoaded ground truth:\")\n",
    "print(f\"  - Samples: {len(ground_truth_df):,}\")\n",
    "print(f\"  - Columns: {list(ground_truth_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge predictions with ground truth\n",
    "merged_df = pd.merge(scores_df, ground_truth_df, on='ID', how='inner')\n",
    "\n",
    "print(f\"\\nMerged dataset:\")\n",
    "print(f\"  - Matched samples: {len(merged_df):,}\")\n",
    "\n",
    "# Check for any null predictions\n",
    "null_preds = merged_df['prediction'].isnull().sum()\n",
    "if null_preds > 0:\n",
    "    print(f\"  - WARNING: {null_preds} null predictions found\")\n",
    "    # Remove null predictions for analysis\n",
    "    merged_df = merged_df.dropna(subset=['prediction'])\n",
    "    print(f\"  - After removing nulls: {len(merged_df):,} samples\")\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Classification Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions and actuals\n",
    "y_true = merged_df['actual_default'].values\n",
    "y_pred = merged_df['prediction'].values.astype(int)\n",
    "y_prob = merged_df['probability_default'].values\n",
    "\n",
    "print(\"Data prepared for evaluation\")\n",
    "print(f\"  - Total samples: {len(y_true):,}\")\n",
    "print(f\"  - Actual defaults: {y_true.sum():,} ({y_true.mean():.2%})\")\n",
    "print(f\"  - Predicted defaults: {y_pred.sum():,} ({y_pred.mean():.2%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate classification metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "specificity = recall_score(y_true, y_pred, pos_label=0, zero_division=0)\n",
    "\n",
    "# ROC AUC\n",
    "if len(np.unique(y_true)) > 1:\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "else:\n",
    "    roc_auc = None\n",
    "    print(\"Warning: Only one class present in ground truth\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CLASSIFICATION METRICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Accuracy:             {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "print(f\"Precision:            {precision:.4f}\")\n",
    "print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "print(f\"Specificity:          {specificity:.4f}\")\n",
    "print(f\"F1 Score:             {f1:.4f}\")\n",
    "if roc_auc:\n",
    "    print(f\"ROC AUC:              {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Confusion Matrix Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CONFUSION MATRIX\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\n                    Predicted\")\n",
    "print(\"                    No Default    Default\")\n",
    "print(f\"Actual  No Default    {tn:>6}       {fp:>6}\")\n",
    "print(f\"        Default       {fn:>6}       {tp:>6}\")\n",
    "\n",
    "# Additional metrics from confusion matrix\n",
    "fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "\n",
    "print(f\"\\nTrue Negatives:  {tn:,}\")\n",
    "print(f\"False Positives: {fp:,} (False Alarm Rate: {fpr:.4f})\")\n",
    "print(f\"False Negatives: {fn:,} (Miss Rate: {fnr:.4f})\")\n",
    "print(f\"True Positives:  {tp:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Default', 'Default'],\n",
    "            yticklabels=['No Default', 'Default'],\n",
    "            ax=ax)\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_title('Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/confusion_matrix.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Confusion matrix saved to: reports/confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. ROC Curve and Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC and Precision-Recall curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# ROC Curve\n",
    "if roc_auc:\n",
    "    fpr_curve, tpr_curve, _ = roc_curve(y_true, y_prob)\n",
    "    axes[0].plot(fpr_curve, tpr_curve, label=f'ROC (AUC = {roc_auc:.4f})')\n",
    "    axes[0].plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "    axes[0].set_xlabel('False Positive Rate')\n",
    "    axes[0].set_ylabel('True Positive Rate')\n",
    "    axes[0].set_title('ROC Curve')\n",
    "    axes[0].legend(loc='lower right')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Precision-Recall Curve\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_true, y_prob)\n",
    "axes[1].plot(recall_curve, precision_curve, label='Precision-Recall')\n",
    "axes[1].axhline(y=y_true.mean(), color='k', linestyle='--', label=f'Baseline ({y_true.mean():.4f})')\n",
    "axes[1].set_xlabel('Recall')\n",
    "axes[1].set_ylabel('Precision')\n",
    "axes[1].set_title('Precision-Recall Curve')\n",
    "axes[1].legend(loc='upper right')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/roc_pr_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Curves saved to: reports/roc_pr_curves.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Prediction Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze probability distribution\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"PREDICTION DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean probability:   {y_prob.mean():.4f}\")\n",
    "print(f\"Std probability:    {y_prob.std():.4f}\")\n",
    "print(f\"Median probability: {np.median(y_prob):.4f}\")\n",
    "print(f\"Min probability:    {y_prob.min():.4f}\")\n",
    "print(f\"Max probability:    {y_prob.max():.4f}\")\n",
    "\n",
    "# Percentiles\n",
    "percentiles = [10, 25, 50, 75, 90, 95, 99]\n",
    "print(\"\\nPercentiles:\")\n",
    "for p in percentiles:\n",
    "    print(f\"  P{p:2d}: {np.percentile(y_prob, p):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize probability distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram by actual class\n",
    "for label, name in [(0, 'No Default'), (1, 'Default')]:\n",
    "    mask = y_true == label\n",
    "    axes[0].hist(y_prob[mask], bins=30, alpha=0.6, label=name, density=True)\n",
    "axes[0].set_xlabel('Predicted Probability of Default')\n",
    "axes[0].set_ylabel('Density')\n",
    "axes[0].set_title('Probability Distribution by Actual Class')\n",
    "axes[0].legend()\n",
    "axes[0].axvline(x=0.5, color='red', linestyle='--', label='Threshold (0.5)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Calibration plot\n",
    "prob_bins = pd.cut(y_prob, bins=10)\n",
    "calibration_df = pd.DataFrame({\n",
    "    'prob': y_prob,\n",
    "    'actual': y_true,\n",
    "    'bin': prob_bins\n",
    "})\n",
    "calibration = calibration_df.groupby('bin').agg({\n",
    "    'prob': 'mean',\n",
    "    'actual': 'mean'\n",
    "}).dropna()\n",
    "\n",
    "axes[1].plot([0, 1], [0, 1], 'k--', label='Perfect Calibration')\n",
    "axes[1].scatter(calibration['prob'], calibration['actual'], s=100, label='Model')\n",
    "axes[1].set_xlabel('Mean Predicted Probability')\n",
    "axes[1].set_ylabel('Actual Default Rate')\n",
    "axes[1].set_title('Model Calibration Plot')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../reports/probability_distribution.png', dpi=150)\n",
    "plt.show()\n",
    "print(\"Distribution plots saved to: reports/probability_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Business Impact Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Business cost analysis\n",
    "# Assumptions:\n",
    "# - Cost of missing a default (False Negative): Higher risk\n",
    "# - Cost of false alarm (False Positive): Lower risk\n",
    "\n",
    "COST_FALSE_NEGATIVE = 500  # Cost of missing a default\n",
    "COST_FALSE_POSITIVE = 100  # Cost of unnecessary action on non-default\n",
    "\n",
    "total_cost = (fn * COST_FALSE_NEGATIVE) + (fp * COST_FALSE_POSITIVE)\n",
    "avg_cost_per_sample = total_cost / len(y_true)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"BUSINESS IMPACT ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples:                  {len(y_true):,}\")\n",
    "print(f\"Actual defaults:                {y_true.sum():,} ({y_true.mean()*100:.2f}%)\")\n",
    "print(f\"Predicted defaults:             {y_pred.sum():,} ({y_pred.mean()*100:.2f}%)\")\n",
    "print(f\"\\nCorrectly identified defaults:  {tp:,}\")\n",
    "print(f\"Missed defaults (FN):           {fn:,}\")\n",
    "print(f\"False alarms (FP):              {fp:,}\")\n",
    "print(f\"\\nCost Analysis:\")\n",
    "print(f\"  Cost per missed default:      ${COST_FALSE_NEGATIVE:,}\")\n",
    "print(f\"  Cost per false alarm:         ${COST_FALSE_POSITIVE:,}\")\n",
    "print(f\"  Total cost:                   ${total_cost:,}\")\n",
    "print(f\"  Average cost per sample:      ${avg_cost_per_sample:.2f}\")\n",
    "print(f\"\\nDefault Detection Rate:         {tp/y_true.sum()*100:.2f}% of actual defaults caught\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Model Drift Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for model drift\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL DRIFT DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "alerts = []\n",
    "\n",
    "# Check accuracy\n",
    "if accuracy < EXPECTED_ACCURACY:\n",
    "    alert = f\"WARNING: Accuracy ({accuracy:.4f}) below threshold ({EXPECTED_ACCURACY})\"\n",
    "    alerts.append(alert)\n",
    "    print(f\"[!] {alert}\")\n",
    "else:\n",
    "    print(f\"[OK] Accuracy ({accuracy:.4f}) meets threshold ({EXPECTED_ACCURACY})\")\n",
    "\n",
    "# Check AUC\n",
    "if roc_auc and roc_auc < EXPECTED_AUC:\n",
    "    alert = f\"WARNING: ROC AUC ({roc_auc:.4f}) below threshold ({EXPECTED_AUC})\"\n",
    "    alerts.append(alert)\n",
    "    print(f\"[!] {alert}\")\n",
    "elif roc_auc:\n",
    "    print(f\"[OK] ROC AUC ({roc_auc:.4f}) meets threshold ({EXPECTED_AUC})\")\n",
    "\n",
    "# Check for prediction bias\n",
    "prediction_bias = abs(y_pred.mean() - y_true.mean())\n",
    "if prediction_bias > MAX_PREDICTION_BIAS:\n",
    "    alert = f\"WARNING: Prediction bias ({prediction_bias:.4f}) exceeds threshold ({MAX_PREDICTION_BIAS})\"\n",
    "    alerts.append(alert)\n",
    "    print(f\"[!] {alert}\")\n",
    "else:\n",
    "    print(f\"[OK] Prediction bias ({prediction_bias:.4f}) within threshold ({MAX_PREDICTION_BIAS})\")\n",
    "\n",
    "# Check error rates\n",
    "if fnr > 0.3:\n",
    "    alert = f\"WARNING: High False Negative Rate ({fnr:.4f})\"\n",
    "    alerts.append(alert)\n",
    "    print(f\"[!] {alert}\")\n",
    "else:\n",
    "    print(f\"[OK] False Negative Rate ({fnr:.4f}) acceptable\")\n",
    "\n",
    "if fpr > 0.3:\n",
    "    alert = f\"WARNING: High False Positive Rate ({fpr:.4f})\"\n",
    "    alerts.append(alert)\n",
    "    print(f\"[!] {alert}\")\n",
    "else:\n",
    "    print(f\"[OK] False Positive Rate ({fpr:.4f}) acceptable\")\n",
    "\n",
    "if not alerts:\n",
    "    print(\"\\nNo model drift detected. Performance within acceptable thresholds.\")\n",
    "else:\n",
    "    print(f\"\\n{len(alerts)} alert(s) raised. Model may require retraining.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Generate Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile all metrics into report\n",
    "monitoring_report = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'samples_evaluated': len(y_true),\n",
    "    'classification_metrics': {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'specificity': specificity,\n",
    "        'roc_auc': roc_auc,\n",
    "        'false_positive_rate': fpr,\n",
    "        'false_negative_rate': fnr\n",
    "    },\n",
    "    'confusion_matrix': {\n",
    "        'true_negatives': int(tn),\n",
    "        'false_positives': int(fp),\n",
    "        'false_negatives': int(fn),\n",
    "        'true_positives': int(tp)\n",
    "    },\n",
    "    'distribution': {\n",
    "        'mean_probability': float(y_prob.mean()),\n",
    "        'std_probability': float(y_prob.std()),\n",
    "        'median_probability': float(np.median(y_prob)),\n",
    "        'min_probability': float(y_prob.min()),\n",
    "        'max_probability': float(y_prob.max()),\n",
    "        'actual_default_rate': float(y_true.mean()),\n",
    "        'predicted_default_rate': float(y_pred.mean())\n",
    "    },\n",
    "    'business_metrics': {\n",
    "        'total_samples': len(y_true),\n",
    "        'actual_defaults': int(y_true.sum()),\n",
    "        'predicted_defaults': int(y_pred.sum()),\n",
    "        'correctly_identified_defaults': int(tp),\n",
    "        'missed_defaults': int(fn),\n",
    "        'false_alarms': int(fp),\n",
    "        'cost_per_missed_default': COST_FALSE_NEGATIVE,\n",
    "        'cost_per_false_alarm': COST_FALSE_POSITIVE,\n",
    "        'total_cost': total_cost,\n",
    "        'avg_cost_per_sample': avg_cost_per_sample\n",
    "    },\n",
    "    'drift_detection': {\n",
    "        'alerts': alerts,\n",
    "        'expected_accuracy': EXPECTED_ACCURACY,\n",
    "        'expected_auc': EXPECTED_AUC,\n",
    "        'max_prediction_bias': MAX_PREDICTION_BIAS\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save report\n",
    "os.makedirs(os.path.dirname(REPORT_OUTPUT_PATH), exist_ok=True)\n",
    "with open(REPORT_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(monitoring_report, f, indent=2)\n",
    "\n",
    "print(f\"\\nMonitoring report saved to: {REPORT_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Summary\n",
    "\n",
    "Print a final summary of all key findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL MONITORING SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Report Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Samples Evaluated: {len(y_true):,}\\n\")\n",
    "\n",
    "print(\"Key Performance Indicators:\")\n",
    "print(f\"  Accuracy:        {accuracy:.4f}\")\n",
    "print(f\"  Precision:       {precision:.4f}\")\n",
    "print(f\"  Recall:          {recall:.4f}\")\n",
    "print(f\"  F1 Score:        {f1:.4f}\")\n",
    "if roc_auc:\n",
    "    print(f\"  ROC AUC:         {roc_auc:.4f}\")\n",
    "\n",
    "print(f\"\\nError Analysis:\")\n",
    "print(f\"  False Positives: {fp:,}\")\n",
    "print(f\"  False Negatives: {fn:,}\")\n",
    "print(f\"  Total Cost:      ${total_cost:,}\")\n",
    "\n",
    "print(f\"\\nDrift Detection:\")\n",
    "if alerts:\n",
    "    for alert in alerts:\n",
    "        print(f\"  - {alert}\")\n",
    "else:\n",
    "    print(\"  No drift detected\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Files Generated:\")\n",
    "print(f\"  - {REPORT_OUTPUT_PATH}\")\n",
    "print(\"  - reports/confusion_matrix.png\")\n",
    "print(\"  - reports/roc_pr_curves.png\")\n",
    "print(\"  - reports/probability_distribution.png\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "You have successfully completed model monitoring. Proceed to **Step 6: Report Generation** to compile your findings into a comprehensive project report.\n",
    "\n",
    "### Key Insights to Include in Your Report:\n",
    "1. Model accuracy and classification metrics\n",
    "2. Confusion matrix interpretation\n",
    "3. Business impact and cost analysis\n",
    "4. Any model drift alerts\n",
    "5. Recommendations for model improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
