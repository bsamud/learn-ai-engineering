{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Classification Model - FintelHub Capstone\n\n**Objective**: Build and evaluate a classification model to predict binary outcomes in financial data.\n\n**Choose Your Problem:**\n- **Option A**: Fraud Detection (PaySim dataset)\n- **Option B**: Customer Churn Prediction (Customer Churn dataset)\n- **Option C**: Credit Default Prediction (Credit Risk dataset)\n\n---\n\n## Table of Contents\n1. [Setup & Imports](#setup)\n2. [Data Loading & Exploration](#data-loading)\n3. [Data Engineering](#data-engineering)\n4. [Model Training](#model-training)\n5. [Model Evaluation](#model-evaluation)\n6. [Model Saving](#model-saving)\n7. [Conclusions](#conclusions)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports <a id='setup'></a>\n",
    "\n",
    "### Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Helper modules (in src folder)\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_engineering import *\n",
    "from model_utils import *\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 2. Data Loading & Exploration <a id='data-loading'></a>\n\n### 2.1 Load Dataset\n\n**Instructions**: Update the file path based on your chosen dataset:\n- Fraud Detection: `'../data/raw/fraud_data.csv'`\n- Churn Prediction: `'../data/raw/customer_churn.csv'`\n- Credit Default: `'../data/raw/credit_risk.csv'`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TODO: Update with your chosen dataset path\nDATA_PATH = '../data/raw/customer_churn.csv'  # Change this!\n\n# Load data (limit rows for large datasets like fraud data)\n# For fraud data, use nrows=100000 to load first 100k rows\ndf = load_data(DATA_PATH, nrows=None)\n\n# Display first few rows\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Basic Data Exploration (GUIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(f\"Dataset Shape: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Column information\n",
    "print(\"Column Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Identify Target Variable\n",
    "\n",
    "**Target Variable Names:**\n",
    "- Fraud Detection: `isFraud`\n",
    "- Churn Prediction: `churn`\n",
    "- Credit Default: `loan_status`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Set your target column name\n",
    "TARGET_COLUMN = 'churn'  # Change this based on your dataset!\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"Target Variable: {TARGET_COLUMN}\")\n",
    "print(f\"\\nValue Counts:\")\n",
    "print(df[TARGET_COLUMN].value_counts())\n",
    "print(f\"\\nPercentage Distribution:\")\n",
    "print(df[TARGET_COLUMN].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize target distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "df[TARGET_COLUMN].value_counts().plot(kind='bar', color=['#2ecc71', '#e74c3c'])\n",
    "plt.title(f'Distribution of {TARGET_COLUMN}')\n",
    "plt.xlabel(TARGET_COLUMN)\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = df[TARGET_COLUMN].value_counts().min() / df[TARGET_COLUMN].value_counts().max()\n",
    "print(f\"\\nClass Imbalance Ratio: {imbalance_ratio:.3f}\")\n",
    "if imbalance_ratio < 0.5:\n",
    "    print(\"âš ï¸ Dataset is imbalanced. We'll need to handle this!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check for Missing Values (GUIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_summary = check_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Identify Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use the helper function to identify numerical and categorical features\n",
    "feature_types = get_feature_types(df)\n",
    "\n",
    "numerical_features = feature_types['numerical']\n",
    "categorical_features = feature_types['categorical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Checkpoint 1\n",
    "Before proceeding, ensure:\n",
    "- âœ… Data is loaded successfully\n",
    "- âœ… You understand the target variable distribution\n",
    "- âœ… You've identified missing values (if any)\n",
    "- âœ… You know which features are numerical vs categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Engineering <a id='data-engineering'></a>\n",
    "\n",
    "### 3.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Handle missing values if any were found\n",
    "# Choose strategy: 'mean', 'median', 'mode', or 'drop'\n",
    "\n",
    "if len(missing_summary) > 0:\n",
    "    # Example: Fill numerical columns with median\n",
    "    df = handle_missing_values(df, strategy='median')\n",
    "else:\n",
    "    print(\"No missing values to handle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Drop Irrelevant Columns\n",
    "\n",
    "**Common columns to drop:**\n",
    "- ID columns (customer_id, transaction_id, etc.)\n",
    "- Timestamp columns (unless doing time-series analysis)\n",
    "- Columns with too many unique values (like names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Drop irrelevant columns\n",
    "# Example columns that might need dropping:\n",
    "# - 'customer_id', 'name', 'nameOrig', 'nameDest', 'step', etc.\n",
    "\n",
    "columns_to_drop = []  # Add column names here\n",
    "\n",
    "if columns_to_drop:\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "print(f\"\\nRemaining columns: {df.shape[1]}\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Encode categorical variables\n",
    "# Update categorical_features list if you dropped any columns\n",
    "\n",
    "# Remove target column from categorical features if present\n",
    "if TARGET_COLUMN in categorical_features:\n",
    "    categorical_features.remove(TARGET_COLUMN)\n",
    "\n",
    "if categorical_features:\n",
    "    print(f\"Encoding categorical features: {categorical_features}\")\n",
    "    # Use 'onehot' for nominal categories (no inherent order)\n",
    "    # Use 'label' for ordinal categories (has order)\n",
    "    df = encode_categorical(df, categorical_features, method='onehot')\n",
    "else:\n",
    "    print(\"No categorical features to encode\")\n",
    "\n",
    "print(f\"\\nShape after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Separate features (X) and target (y)\n",
    "X = df.drop(columns=[TARGET_COLUMN])\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Train-Test Split (GUIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% training, 20% testing\n",
    "# Use stratify=y to maintain class distribution in imbalanced datasets\n",
    "X_train, X_test, y_train, y_test = create_train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify class distribution is maintained\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "print(y_train.value_counts(normalize=True))\n",
    "print(f\"\\nTest set class distribution:\")\n",
    "print(y_test.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Scale numerical features\n",
    "# Choose 'standard' (mean=0, std=1) or 'minmax' (range 0-1)\n",
    "\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(\n",
    "    X_train, X_test, \n",
    "    method='standard'\n",
    ")\n",
    "\n",
    "# Convert back to DataFrame to preserve column names\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"\\nFeatures scaled successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Handle Class Imbalance (Optional)\n",
    "\n",
    "Only needed if your dataset is imbalanced (ratio < 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Apply SMOTE if dataset is imbalanced\n",
    "# Uncomment the code below if needed\n",
    "\n",
    "# smote = SMOTE(random_state=42)\n",
    "# X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)\n",
    "\n",
    "# print(f\"Original training set: {X_train_scaled.shape[0]} samples\")\n",
    "# print(f\"Resampled training set: {X_train_resampled.shape[0]} samples\")\n",
    "# print(f\"\\nClass distribution after SMOTE:\")\n",
    "# print(pd.Series(y_train_resampled).value_counts())\n",
    "\n",
    "# # Use resampled data for training\n",
    "# X_train_final = X_train_resampled\n",
    "# y_train_final = y_train_resampled\n",
    "\n",
    "# If not using SMOTE, use original scaled data\n",
    "X_train_final = X_train_scaled\n",
    "y_train_final = y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ“Š Checkpoint 2\n",
    "Before proceeding to modeling, verify:\n",
    "- âœ… Missing values are handled\n",
    "- âœ… Categorical variables are encoded\n",
    "- âœ… Data is split into train/test sets\n",
    "- âœ… Features are scaled\n",
    "- âœ… Class imbalance is addressed (if needed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Training <a id='model-training'></a>\n",
    "\n",
    "### 4.1 Train Multiple Models (GUIDED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train three classification models\n",
    "trained_models = train_classification_models(X_train_final, y_train_final)\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")\n",
    "print(f\"Models: {list(trained_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compare Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare all models and create comparison dataframe\n",
    "comparison_df = compare_classification_models(trained_models, X_test_scaled, y_test)\n",
    "\n",
    "# Display comparison\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Select best model based on F1 score (or other metric)\n",
    "# You can change the metric if needed (accuracy, precision, recall, roc_auc)\n",
    "\n",
    "best_model_name = comparison_df.loc[comparison_df['f1'].idxmax(), 'Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"âœ… Best Model: {best_model_name}\")\n",
    "print(f\"\\nBest Model Metrics:\")\n",
    "print(comparison_df[comparison_df['Model'] == best_model_name].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Evaluation <a id='model-evaluation'></a>\n",
    "\n",
    "### 5.1 Detailed Evaluation of Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "y_pred_proba = best_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Print detailed classification report\n",
    "print(f\"Classification Report - {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot confusion matrix\n",
    "plot_confusion_matrix(best_model, X_test_scaled, y_test, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot ROC curve\n",
    "plot_roc_curve(best_model, X_test_scaled, y_test, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get and plot feature importance (for tree-based models)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance_df = get_feature_importance(\n",
    "        best_model, \n",
    "        X_train_final.columns, \n",
    "        top_n=10\n",
    "    )\n",
    "else:\n",
    "    print(f\"{best_model_name} does not have feature importance attribute\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Saving <a id='model-saving'></a>\n",
    "\n",
    "### 6.1 Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save your best model\n",
    "model_filename = f\"../models/classification_{best_model_name.lower().replace(' ', '_')}.pkl\"\n",
    "save_model(best_model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Save Preprocessing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Save scaler for use in production\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/scaler.pkl')\n",
    "print(\"âœ… Scaler saved to ../models/scaler.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save comparison results\n",
    "comparison_df.to_csv('../models/classification_model_comparison.csv', index=False)\n",
    "print(\"âœ… Model comparison saved to ../models/classification_model_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Conclusions <a id='conclusions'></a>\n",
    "\n",
    "### 7.1 Summary of Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO: Write a brief summary of your findings**\n",
    "\n",
    "1. **Dataset**: [Describe which dataset you used]\n",
    "\n",
    "2. **Data Challenges**: [Note any issues like missing values, imbalance, etc.]\n",
    "\n",
    "3. **Best Model**: [State which model performed best and why]\n",
    "\n",
    "4. **Key Metrics**: [Highlight the most important metrics]\n",
    "\n",
    "5. **Important Features**: [List top 3-5 features that influenced predictions]\n",
    "\n",
    "6. **Model Limitations**: [Note any limitations or concerns]\n",
    "\n",
    "7. **Next Steps**: [Suggest improvements or further analysis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify model can be loaded\n",
    "loaded_model = load_model(model_filename)\n",
    "\n",
    "# Test prediction\n",
    "test_prediction = loaded_model.predict(X_test_scaled[:5])\n",
    "print(f\"\\nTest predictions: {test_prediction}\")\n",
    "print(f\"Actual values: {y_test[:5].values}\")\n",
    "print(\"\\nâœ… Model loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've successfully completed the Classification Model notebook!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Review your model performance\n",
    "2. Try improving the model (feature engineering, hyperparameter tuning)\n",
    "3. Move on to `02_regression_model.ipynb`\n",
    "4. Prepare for model deployment to H2O"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}