{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Model - SOLUTION\n",
    "\n",
    "**Instructor Solution Notebook**\n",
    "\n",
    "This notebook contains the complete solution for the regression problem using the **Credit Risk Dataset** to predict loan amounts.\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup & Imports](#setup)\n",
    "2. [Data Loading & Exploration](#data-loading)\n",
    "3. [Data Engineering](#data-engineering)\n",
    "4. [Model Training](#model-training)\n",
    "5. [Model Evaluation](#model-evaluation)\n",
    "6. [Model Saving](#model-saving)\n",
    "7. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Imports <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Helper modules\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "from data_engineering import *\n",
    "from model_utils import *\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Loading & Exploration <a id='data-loading'></a>\n",
    "\n",
    "### 2.1 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Using Credit Risk Dataset to predict loan amounts\n",
    "DATA_PATH = '../data/raw/credit_risk.csv'\n",
    "\n",
    "# Load data\n",
    "df = load_data(DATA_PATH)\n",
    "\n",
    "# Display first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Basic Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset shape\n",
    "print(f\"Dataset Shape: {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
    "\n",
    "# Column information\n",
    "print(\"Column Information:\")\n",
    "print(df.info())\n",
    "\n",
    "# Statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Identify Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Target is loan_amnt (loan amount)\n",
    "TARGET_COLUMN = 'loan_amnt'\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"Target Variable: {TARGET_COLUMN}\")\n",
    "print(f\"\\nBasic Statistics:\")\n",
    "print(df[TARGET_COLUMN].describe())\n",
    "\n",
    "# Visualize target distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df[TARGET_COLUMN], bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "axes[0].set_xlabel(TARGET_COLUMN)\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title(f'Distribution of {TARGET_COLUMN}')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Boxplot\n",
    "axes[1].boxplot(df[TARGET_COLUMN])\n",
    "axes[1].set_ylabel(TARGET_COLUMN)\n",
    "axes[1].set_title(f'Boxplot of {TARGET_COLUMN}')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for outliers\n",
    "Q1 = df[TARGET_COLUMN].quantile(0.25)\n",
    "Q3 = df[TARGET_COLUMN].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers = ((df[TARGET_COLUMN] < (Q1 - 1.5 * IQR)) | (df[TARGET_COLUMN] > (Q3 + 1.5 * IQR))).sum()\n",
    "print(f\"\\nNumber of outliers: {outliers} ({outliers/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values\n",
    "missing_summary = check_missing_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Identify Feature Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Identify feature types\n",
    "feature_types = get_feature_types(df)\n",
    "\n",
    "numerical_features = feature_types['numerical']\n",
    "categorical_features = feature_types['categorical']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Analyze correlations with target\n",
    "correlations = df[numerical_features].corr()[TARGET_COLUMN].sort_values(ascending=False)\n",
    "print(\"Correlation with target variable:\")\n",
    "print(correlations)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlations.drop(TARGET_COLUMN).plot(kind='barh', color='steelblue')\n",
    "plt.xlabel('Correlation with Target')\n",
    "plt.title(f'Feature Correlations with {TARGET_COLUMN}')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.axvline(x=0, color='red', linestyle='--', linewidth=0.8)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Engineering <a id='data-engineering'></a>\n",
    "\n",
    "### 3.1 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Handle missing values if any\n",
    "if len(missing_summary) > 0:\n",
    "    df = handle_missing_values(df, strategy='median')\n",
    "else:\n",
    "    print(\"No missing values to handle!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Drop Irrelevant Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Drop loan_status as it's not useful for predicting loan amount\n",
    "# (We're predicting the amount, not whether they'll default)\n",
    "columns_to_drop = ['loan_status'] if 'loan_status' in df.columns else []\n",
    "\n",
    "if columns_to_drop:\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    print(f\"Dropped columns: {columns_to_drop}\")\n",
    "\n",
    "print(f\"\\nRemaining columns: {df.shape[1]}\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Optionally remove extreme outliers in target variable\n",
    "# Be cautious - these might be legitimate high-value loans\n",
    "print(f\"Dataset shape before outlier removal: {df.shape}\")\n",
    "\n",
    "# Remove only extreme outliers (beyond 3 IQR)\n",
    "Q1 = df[TARGET_COLUMN].quantile(0.25)\n",
    "Q3 = df[TARGET_COLUMN].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 3 * IQR\n",
    "upper_bound = Q3 + 3 * IQR\n",
    "\n",
    "df = df[(df[TARGET_COLUMN] >= lower_bound) & (df[TARGET_COLUMN] <= upper_bound)]\n",
    "\n",
    "print(f\"Dataset shape after outlier removal: {df.shape}\")\n",
    "print(f\"Rows removed: {df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Encode categorical variables\n",
    "categorical_features = [col for col in df.select_dtypes(include=['object']).columns \n",
    "                       if col != TARGET_COLUMN]\n",
    "\n",
    "if categorical_features:\n",
    "    print(f\"Encoding categorical features: {categorical_features}\")\n",
    "    df = encode_categorical(df, categorical_features, method='onehot')\n",
    "else:\n",
    "    print(\"No categorical features to encode\")\n",
    "\n",
    "print(f\"\\nShape after encoding: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Separate X and y\n",
    "X = df.drop(columns=[TARGET_COLUMN])\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns ({len(X.columns)}):\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Create train-test split\n",
    "X_train, X_test, y_train, y_test = create_train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Check target distribution\n",
    "print(f\"\\nTraining target - Mean: {y_train.mean():.2f}, Std: {y_train.std():.2f}\")\n",
    "print(f\"Test target - Mean: {y_test.mean():.2f}, Std: {y_test.std():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Scale features\n",
    "X_train_scaled, X_test_scaled, scaler = scale_features(\n",
    "    X_train, X_test, \n",
    "    method='standard'\n",
    ")\n",
    "\n",
    "# Convert back to DataFrame\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"\\nFeatures scaled successfully!\")\n",
    "print(f\"Training set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Model Training <a id='model-training'></a>\n",
    "\n",
    "### 4.1 Train Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Train four regression models\n",
    "trained_models = train_regression_models(X_train_scaled, y_train)\n",
    "\n",
    "print(\"\\nAll models trained successfully!\")\n",
    "print(f\"Models: {list(trained_models.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Compare Models on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Compare all models\n",
    "comparison_df = compare_regression_models(trained_models, X_test_scaled, y_test)\n",
    "\n",
    "# Display comparison\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Select Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Select best model based on RÂ² score\n",
    "best_model_name = comparison_df.loc[comparison_df['r2'].idxmax(), 'Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "\n",
    "print(f\"âœ… Best Model: {best_model_name}\")\n",
    "print(f\"\\nBest Model Metrics:\")\n",
    "print(comparison_df[comparison_df['Model'] == best_model_name].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Model Evaluation <a id='model-evaluation'></a>\n",
    "\n",
    "### 5.1 Detailed Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Get predictions and calculate metrics\n",
    "y_pred = best_model.predict(X_test_scaled)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Detailed Evaluation - {best_model_name}\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
    "print(f\"Mean Absolute Error (MAE):      {mae:.2f}\")\n",
    "print(f\"RÂ² Score:                       {r2:.4f}\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(f\"- On average, predictions are off by ${mae:.2f}\")\n",
    "print(f\"- Model explains {r2*100:.2f}% of the variance in loan amounts\")\n",
    "print(f\"- Mean absolute percentage error: {(mae/y_test.mean())*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Actual vs Predicted Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Plot actual vs predicted\n",
    "plot_predictions(y_test, y_pred, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Plot residuals\n",
    "plot_residuals(y_test, y_pred, best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Error Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Analyze prediction errors in detail\n",
    "errors = y_test - y_pred\n",
    "percent_errors = (errors / y_test) * 100\n",
    "\n",
    "print(\"Error Analysis:\")\n",
    "print(f\"Mean Error: ${errors.mean():.2f}\")\n",
    "print(f\"Std Error: ${errors.std():.2f}\")\n",
    "print(f\"Mean Absolute Percentage Error: {np.abs(percent_errors).mean():.2f}%\")\n",
    "print(f\"Median Absolute Error: ${np.abs(errors).median():.2f}\")\n",
    "\n",
    "# Find worst predictions\n",
    "worst_predictions = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred,\n",
    "    'Error': np.abs(errors),\n",
    "    'Percent_Error': np.abs(percent_errors)\n",
    "}).sort_values('Error', ascending=False).head(10)\n",
    "\n",
    "print(\"\\nTop 10 Worst Predictions:\")\n",
    "print(worst_predictions.to_string())\n",
    "\n",
    "# Best predictions\n",
    "best_predictions = pd.DataFrame({\n",
    "    'Actual': y_test,\n",
    "    'Predicted': y_pred,\n",
    "    'Error': np.abs(errors)\n",
    "}).sort_values('Error').head(10)\n",
    "\n",
    "print(\"\\nTop 10 Best Predictions:\")\n",
    "print(best_predictions.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5 Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Get and plot feature importance\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance_df = get_feature_importance(\n",
    "        best_model, \n",
    "        X_train_scaled.columns, \n",
    "        top_n=10\n",
    "    )\n",
    "else:\n",
    "    print(f\"{best_model_name} does not have feature importance attribute\")\n",
    "    print(\"\\nFor Linear/Ridge models, examining coefficients:\")\n",
    "    if hasattr(best_model, 'coef_'):\n",
    "        coef_df = pd.DataFrame({\n",
    "            'Feature': X_train_scaled.columns,\n",
    "            'Coefficient': best_model.coef_\n",
    "        }).sort_values('Coefficient', key=abs, ascending=False).head(10)\n",
    "        print(coef_df.to_string(index=False))\n",
    "        \n",
    "        # Plot top coefficients\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.barh(coef_df['Feature'], coef_df['Coefficient'])\n",
    "        plt.xlabel('Coefficient Value')\n",
    "        plt.title('Top 10 Feature Coefficients')\n",
    "        plt.gca().invert_yaxis()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Model Saving <a id='model-saving'></a>\n",
    "\n",
    "### 6.1 Save the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Save best model\n",
    "model_filename = f\"../models/regression_{best_model_name.lower().replace(' ', '_')}.pkl\"\n",
    "save_model(best_model, model_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Save Preprocessing Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Save scaler\n",
    "import joblib\n",
    "joblib.dump(scaler, '../models/scaler_regression.pkl')\n",
    "print(\"âœ… Scaler saved to ../models/scaler_regression.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Save Model Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Save comparison and predictions\n",
    "comparison_df.to_csv('../models/regression_model_comparison.csv', index=False)\n",
    "print(\"âœ… Model comparison saved\")\n",
    "\n",
    "save_predictions(y_test, y_pred, '../models/regression_predictions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Conclusions <a id='conclusions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SOLUTION SUMMARY\n",
    "\n",
    "### 1. Dataset\n",
    "- Used **Credit Risk Dataset** with ~32,000 loan applications\n",
    "- Target: Predict **loan amount** (continuous value)\n",
    "\n",
    "### 2. Target Variable\n",
    "- Predicting loan amounts requested by borrowers\n",
    "- Range typically from $1,000 to $40,000\n",
    "- Distribution shows most loans cluster around certain amounts (5k, 10k, 15k)\n",
    "\n",
    "### 3. Data Challenges\n",
    "- **Outliers**: Some extreme loan amounts removed (>3 IQR)\n",
    "- **Categorical Features**: Encoded loan intent, home ownership, etc.\n",
    "- **Feature Scaling**: StandardScaler applied for models sensitive to scale\n",
    "- **Correlations**: Strong predictors include person_income and loan_int_rate\n",
    "\n",
    "### 4. Best Model\n",
    "- **Winner**: Random Forest or XGBoost (typically performs best)\n",
    "- **Why**: \n",
    "  - Captures non-linear relationships between income and loan amount\n",
    "  - Handles feature interactions well\n",
    "  - Less sensitive to outliers than linear models\n",
    "\n",
    "### 5. Key Metrics (Expected Performance)\n",
    "- **RMSE**: $2,500 - $3,500\n",
    "- **MAE**: $1,800 - $2,500\n",
    "- **RÂ² Score**: 0.65 - 0.75\n",
    "- **MAPE**: 15-20%\n",
    "\n",
    "**Interpretation:**\n",
    "- Model explains 65-75% of variance in loan amounts\n",
    "- Average prediction error of $2,000-$2,500\n",
    "- Predictions within 15-20% of actual values on average\n",
    "\n",
    "### 6. Prediction Accuracy Insights\n",
    "- **Performs Well**: Standard loan amounts ($5k, $10k, $15k)\n",
    "- **Challenges**: Very high or very low loan amounts\n",
    "- **Error Pattern**: Slight tendency to underpredict very large loans\n",
    "\n",
    "### 7. Important Features (Typical Rankings)\n",
    "1. **Person Income** - Strong positive correlation with loan amount\n",
    "2. **Loan Percent Income** - Key ratio for loan sizing\n",
    "3. **Person Employment Length** - Stability indicator\n",
    "4. **Person Age** - Life stage affects borrowing\n",
    "5. **Loan Grade** - Risk-based loan categorization\n",
    "6. **Credit History Length** - Creditworthiness indicator\n",
    "7. **Home Ownership** - Asset ownership affects loan size\n",
    "\n",
    "### 8. Model Limitations\n",
    "- **Cannot capture**: Economic cycles, market conditions\n",
    "- **Temporal factors**: No time-series considerations\n",
    "- **External factors**: Interest rate environment, housing market\n",
    "- **Residual patterns**: Some heteroscedasticity may exist\n",
    "- **Edge cases**: Underperforms on extreme loan amounts\n",
    "\n",
    "### 9. Business Insights\n",
    "- **Income is king**: Primary driver of loan amount eligibility\n",
    "- **Debt-to-income ratio**: Critical for loan sizing decisions\n",
    "- **Employment stability**: Longer employment = larger loans\n",
    "- **Risk-based pricing**: Loan grade strongly influences amount\n",
    "- **Life stage matters**: Age and home ownership affect loan size\n",
    "\n",
    "### 10. Next Steps for Improvement\n",
    "1. **Feature Engineering**:\n",
    "   - Create debt-to-income ratio features\n",
    "   - Income Ã— employment length interactions\n",
    "   - Loan amount categories (small, medium, large)\n",
    "\n",
    "2. **Advanced Modeling**:\n",
    "   - Ensemble methods (stacking RF + XGBoost)\n",
    "   - Hyperparameter tuning with GridSearchCV\n",
    "   - Try CatBoost or LightGBM\n",
    "\n",
    "3. **Residual Analysis**:\n",
    "   - Investigate high-error predictions\n",
    "   - Address heteroscedasticity if present\n",
    "   - Consider log transformation of target\n",
    "\n",
    "4. **Production Considerations**:\n",
    "   - Add prediction intervals (uncertainty quantification)\n",
    "   - Implement model monitoring for drift\n",
    "   - Create fallback rules for edge cases\n",
    "\n",
    "5. **Deployment**:\n",
    "   - Deploy to H2O platform\n",
    "   - Create REST API endpoint\n",
    "   - Set up automated retraining pipeline\n",
    "   - Monitor prediction accuracy over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION: Verify model can be loaded and used\n",
    "loaded_model = load_model(model_filename)\n",
    "\n",
    "# Test prediction on sample data\n",
    "test_prediction = loaded_model.predict(X_test_scaled[:5])\n",
    "\n",
    "print(f\"\\nTest predictions: {test_prediction}\")\n",
    "print(f\"Actual values: {y_test[:5].values}\")\n",
    "print(f\"\\nPrediction errors:\")\n",
    "for i, (pred, actual) in enumerate(zip(test_prediction, y_test[:5].values)):\n",
    "    error = actual - pred\n",
    "    pct_error = (error / actual) * 100\n",
    "    print(f\"  Sample {i+1}: ${error:+.2f} ({pct_error:+.1f}%)\")\n",
    "\n",
    "print(\"\\nâœ… Model loaded and tested successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ðŸŽ‰ Solution Complete!\n",
    "\n",
    "This solution demonstrates:\n",
    "- âœ… Complete regression workflow from data to deployment\n",
    "- âœ… Proper handling of continuous target variables\n",
    "- âœ… Outlier analysis and treatment\n",
    "- âœ… Comprehensive model evaluation (RMSE, MAE, RÂ²)\n",
    "- âœ… Residual analysis for model diagnostics\n",
    "- âœ… Feature importance interpretation\n",
    "- âœ… Business insights and actionable recommendations\n",
    "\n",
    "**Expected Student Outcomes:**\n",
    "- Similar regression performance with chosen dataset\n",
    "- Understanding of regression metrics vs classification\n",
    "- Ability to interpret and explain predictions\n",
    "- Ready for H2O deployment phase\n",
    "\n",
    "**Key Differences from Classification:**\n",
    "- No class imbalance concerns\n",
    "- Different metrics (RMSE/MAE/RÂ² vs Precision/Recall/F1)\n",
    "- Residual analysis crucial for diagnostics\n",
    "- Prediction intervals more important than binary decisions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
