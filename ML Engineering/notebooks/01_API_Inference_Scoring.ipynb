{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Step 4: API Inference Scoring\n\n## Objective\nScore the `inference_dataset.csv` using your deployed H2O MLOps model endpoint and save predictions to `scores.csv`.\n\n## Prerequisites\n- Model trained in H2O Driverless AI (Step 2)\n- Model deployed to H2O MLOps with active endpoint (Step 3)\n- Your deployment endpoint URL (e.g., `https://api.example.com/v1/models/<model-id>/score`)\n- `inference_dataset.csv` file in your data directory\n\n## Deliverables\n- `scores.csv` with predictions for all samples\n- `inference_metrics.json` with API performance metrics (latency, throughput)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install pandas numpy requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "print(f\"Notebook started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Configuration\n",
    "\n",
    "**IMPORTANT**: Update the `ENDPOINT_URL` with your actual H2O MLOps deployment endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# CONFIGURATION - UPDATE THESE VALUES\n",
    "# ============================================\n",
    "\n",
    "# Your H2O MLOps model endpoint URL\n",
    "ENDPOINT_URL = \"<your endpoint>\"\n",
    "\n",
    "# API Key (if required by your deployment)\n",
    "API_KEY = None  # Set to your API key string if needed, e.g., \"your-api-key-here\"\n",
    "\n",
    "# File paths\n",
    "INFERENCE_DATA_PATH = \"../data/inference_dataset.csv\"\n",
    "SCORES_OUTPUT_PATH = \"../data/scores.csv\"\n",
    "METRICS_OUTPUT_PATH = \"../reports/inference_metrics.json\"\n",
    "\n",
    "# Batch size for API requests\n",
    "BATCH_SIZE = 100  # Number of samples per API call\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"Endpoint: {ENDPOINT_URL}\")\n",
    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Load Inference Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the inference dataset\n",
    "inference_df = pd.read_csv(INFERENCE_DATA_PATH)\n",
    "\n",
    "print(f\"Loaded inference dataset:\")\n",
    "print(f\"  - Samples: {len(inference_df):,}\")\n",
    "print(f\"  - Features: {len(inference_df.columns)}\")\n",
    "print(f\"  - Columns: {list(inference_df.columns)}\")\n",
    "\n",
    "# Display first few rows\n",
    "inference_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Helper Functions for API Scoring\n",
    "\n",
    "These functions handle sending requests to your H2O MLOps endpoint and parsing responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_request_headers(api_key=None):\n",
    "    \"\"\"\n",
    "    Create HTTP headers for API request.\n",
    "    \n",
    "    Args:\n",
    "        api_key: Optional API key for authentication\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary of headers\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    "    if api_key:\n",
    "        headers['Authorization'] = f'Bearer {api_key}'\n",
    "    return headers\n",
    "\n",
    "\n",
    "def create_scoring_payload(df_batch):\n",
    "    \"\"\"\n",
    "    Create JSON payload for H2O MLOps scoring API.\n",
    "    \n",
    "    Args:\n",
    "        df_batch: DataFrame with rows to score\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary payload for API request\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"fields\": list(df_batch.columns),\n",
    "        \"rows\": df_batch.values.tolist()\n",
    "    }\n",
    "    return payload\n",
    "\n",
    "\n",
    "def send_scoring_request(endpoint_url, payload, headers, timeout=60):\n",
    "    \"\"\"\n",
    "    Send scoring request to H2O MLOps endpoint.\n",
    "    \n",
    "    Args:\n",
    "        endpoint_url: Full URL to scoring endpoint\n",
    "        payload: JSON payload with data to score\n",
    "        headers: HTTP headers\n",
    "        timeout: Request timeout in seconds\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (response_json, latency_ms, success)\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        response = requests.post(\n",
    "            endpoint_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=timeout\n",
    "        )\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json(), latency_ms, True\n",
    "        else:\n",
    "            print(f\"Error: HTTP {response.status_code} - {response.text}\")\n",
    "            return None, latency_ms, False\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        print(f\"Error: Request timeout after {timeout} seconds\")\n",
    "        return None, latency_ms, False\n",
    "        \n",
    "    except Exception as e:\n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return None, latency_ms, False\n",
    "\n",
    "\n",
    "def parse_scoring_response(response_json, batch_df):\n",
    "    \"\"\"\n",
    "    Parse H2O MLOps scoring response.\n",
    "    \n",
    "    Expected response format:\n",
    "    {\n",
    "        \"fields\": [\"DEFAULT_PAYMENT_NEXT_MONTH.0\", \"DEFAULT_PAYMENT_NEXT_MONTH.1\"],\n",
    "        \"score\": [[0.8, 0.2], [0.3, 0.7], ...]\n",
    "    }\n",
    "    \n",
    "    Args:\n",
    "        response_json: JSON response from API\n",
    "        batch_df: Original batch DataFrame for ID mapping\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with predictions\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    try:\n",
    "        fields = response_json.get('fields', [])\n",
    "        scores = response_json.get('score', response_json.get('scores', []))\n",
    "        \n",
    "        # Determine which index is probability of default (class 1)\n",
    "        prob_default_idx = 1  # Usually second column is P(class=1)\n",
    "        prob_no_default_idx = 0\n",
    "        \n",
    "        # Check field names to be sure\n",
    "        for i, field in enumerate(fields):\n",
    "            if '.1' in str(field) or 'DEFAULT' in str(field).upper():\n",
    "                prob_default_idx = i\n",
    "                prob_no_default_idx = 1 - i if i <= 1 else 0\n",
    "                break\n",
    "        \n",
    "        # Parse each score\n",
    "        for i, score_row in enumerate(scores):\n",
    "            if i < len(batch_df):\n",
    "                row_id = batch_df.iloc[i]['ID']\n",
    "                \n",
    "                if len(score_row) >= 2:\n",
    "                    prob_no_default = float(score_row[prob_no_default_idx])\n",
    "                    prob_default = float(score_row[prob_default_idx])\n",
    "                else:\n",
    "                    prob_default = float(score_row[0])\n",
    "                    prob_no_default = 1 - prob_default\n",
    "                \n",
    "                prediction = 1 if prob_default >= 0.5 else 0\n",
    "                \n",
    "                results.append({\n",
    "                    'ID': int(row_id),\n",
    "                    'prediction': prediction,\n",
    "                    'probability_default': round(prob_default, 6),\n",
    "                    'probability_no_default': round(prob_no_default, 6)\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing response: {e}\")\n",
    "        print(f\"Response: {response_json}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"Helper functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Main Scoring Function\n",
    "\n",
    "This function scores the entire inference dataset in batches and collects performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_inference_dataset(df, endpoint_url, api_key=None, batch_size=100):\n",
    "    \"\"\"\n",
    "    Score the entire inference dataset using H2O MLOps API.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with inference data\n",
    "        endpoint_url: H2O MLOps scoring endpoint URL\n",
    "        api_key: Optional API key\n",
    "        batch_size: Number of samples per API request\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (scores_df, metrics_dict)\n",
    "    \"\"\"\n",
    "    headers = create_request_headers(api_key)\n",
    "    \n",
    "    all_results = []\n",
    "    latencies = []\n",
    "    failed_batches = 0\n",
    "    \n",
    "    n_samples = len(df)\n",
    "    n_batches = (n_samples + batch_size - 1) // batch_size\n",
    "    \n",
    "    print(f\"\\nStarting inference scoring...\")\n",
    "    print(f\"  Total samples: {n_samples:,}\")\n",
    "    print(f\"  Batch size: {batch_size}\")\n",
    "    print(f\"  Total batches: {n_batches}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, n_samples)\n",
    "        batch_df = df.iloc[start_idx:end_idx]\n",
    "        \n",
    "        # Create payload\n",
    "        payload = create_scoring_payload(batch_df)\n",
    "        \n",
    "        # Send request\n",
    "        response_json, latency_ms, success = send_scoring_request(\n",
    "            endpoint_url, payload, headers\n",
    "        )\n",
    "        \n",
    "        latencies.append(latency_ms)\n",
    "        \n",
    "        if success and response_json:\n",
    "            # Parse response\n",
    "            batch_results = parse_scoring_response(response_json, batch_df)\n",
    "            all_results.extend(batch_results)\n",
    "        else:\n",
    "            failed_batches += 1\n",
    "            # Add null results for failed batch\n",
    "            for idx in range(len(batch_df)):\n",
    "                all_results.append({\n",
    "                    'ID': int(batch_df.iloc[idx]['ID']),\n",
    "                    'prediction': None,\n",
    "                    'probability_default': None,\n",
    "                    'probability_no_default': None\n",
    "                })\n",
    "        \n",
    "        # Progress update\n",
    "        print(f\"\\rBatch {batch_idx + 1}/{n_batches} completed | \"\n",
    "              f\"Latency: {latency_ms:.2f}ms | \"\n",
    "              f\"Progress: {end_idx}/{n_samples} samples\", end=\"\", flush=True)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"\\n\\nScoring completed in {total_time:.2f} seconds\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    scores_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'endpoint': endpoint_url,\n",
    "        'total_samples': n_samples,\n",
    "        'batch_size': batch_size,\n",
    "        'total_batches': n_batches,\n",
    "        'failed_batches': failed_batches,\n",
    "        'success_rate': ((n_batches - failed_batches) / n_batches * 100) if n_batches > 0 else 0,\n",
    "        'total_time_seconds': total_time,\n",
    "        'throughput_samples_per_second': n_samples / total_time if total_time > 0 else 0,\n",
    "        'latency_stats': {\n",
    "            'avg_ms': np.mean(latencies),\n",
    "            'min_ms': np.min(latencies),\n",
    "            'max_ms': np.max(latencies),\n",
    "            'p50_ms': np.percentile(latencies, 50),\n",
    "            'p95_ms': np.percentile(latencies, 95),\n",
    "            'p99_ms': np.percentile(latencies, 99),\n",
    "            'total_ms': np.sum(latencies)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return scores_df, metrics\n",
    "\n",
    "\n",
    "print(\"Main scoring function loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Execute Scoring\n",
    "\n",
    "**IMPORTANT**: Make sure you have updated `ENDPOINT_URL` with your actual deployment endpoint before running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify endpoint is configured\n",
    "if \"<YOUR-DEPLOYMENT-ID>\" in ENDPOINT_URL:\n",
    "    print(\"ERROR: Please update ENDPOINT_URL with your actual deployment ID!\")\n",
    "    print(f\"Current value: {ENDPOINT_URL}\")\n",
    "else:\n",
    "    print(\"Endpoint configured. Ready to score.\")\n",
    "    print(f\"Endpoint: {ENDPOINT_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute scoring\n",
    "scores_df, metrics = score_inference_dataset(\n",
    "    df=inference_df,\n",
    "    endpoint_url=ENDPOINT_URL,\n",
    "    api_key=API_KEY,\n",
    "    batch_size=BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Review Scoring Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display scores summary\n",
    "print(\"Scoring Results Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total predictions: {len(scores_df):,}\")\n",
    "print(f\"Predicted defaults: {(scores_df['prediction'] == 1).sum():,}\")\n",
    "print(f\"Predicted non-defaults: {(scores_df['prediction'] == 0).sum():,}\")\n",
    "print(f\"Null predictions (failed): {scores_df['prediction'].isnull().sum():,}\")\n",
    "print(f\"\\nPredicted default rate: {(scores_df['prediction'] == 1).mean():.2%}\")\n",
    "print(f\"Average probability: {scores_df['probability_default'].mean():.4f}\")\n",
    "\n",
    "# Display first few predictions\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "scores_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display performance metrics\n",
    "print(\"\\nAPI Performance Metrics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total samples scored: {metrics['total_samples']:,}\")\n",
    "print(f\"Total batches: {metrics['total_batches']}\")\n",
    "print(f\"Failed batches: {metrics['failed_batches']}\")\n",
    "print(f\"Success rate: {metrics['success_rate']:.2f}%\")\n",
    "print(f\"\\nTotal time: {metrics['total_time_seconds']:.2f} seconds\")\n",
    "print(f\"Throughput: {metrics['throughput_samples_per_second']:.2f} samples/second\")\n",
    "print(f\"\\nLatency Statistics:\")\n",
    "print(f\"  Average: {metrics['latency_stats']['avg_ms']:.2f} ms\")\n",
    "print(f\"  Minimum: {metrics['latency_stats']['min_ms']:.2f} ms\")\n",
    "print(f\"  Maximum: {metrics['latency_stats']['max_ms']:.2f} ms\")\n",
    "print(f\"  P50 (Median): {metrics['latency_stats']['p50_ms']:.2f} ms\")\n",
    "print(f\"  P95: {metrics['latency_stats']['p95_ms']:.2f} ms\")\n",
    "print(f\"  P99: {metrics['latency_stats']['p99_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Save Results\n",
    "\n",
    "Save the scoring results and metrics for model monitoring and reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save scores to CSV\n",
    "scores_df.to_csv(SCORES_OUTPUT_PATH, index=False)\n",
    "print(f\"Scores saved to: {SCORES_OUTPUT_PATH}\")\n",
    "\n",
    "# Save metrics to JSON\n",
    "os.makedirs(os.path.dirname(METRICS_OUTPUT_PATH), exist_ok=True)\n",
    "with open(METRICS_OUTPUT_PATH, 'w') as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "print(f\"Metrics saved to: {METRICS_OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify saved files\n",
    "print(\"\\nVerification:\")\n",
    "print(f\"scores.csv size: {os.path.getsize(SCORES_OUTPUT_PATH):,} bytes\")\n",
    "print(f\"metrics.json size: {os.path.getsize(METRICS_OUTPUT_PATH):,} bytes\")\n",
    "\n",
    "# Quick check of saved scores\n",
    "saved_scores = pd.read_csv(SCORES_OUTPUT_PATH)\n",
    "print(f\"\\nSaved scores shape: {saved_scores.shape}\")\n",
    "print(f\"Columns: {list(saved_scores.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Next Steps\n",
    "\n",
    "You have successfully:\n",
    "1. Loaded the inference dataset\n",
    "2. Scored all samples using your H2O MLOps endpoint\n",
    "3. Saved predictions to `scores.csv`\n",
    "4. Recorded API performance metrics\n",
    "\n",
    "**Next**: Proceed to **Step 5: Model Monitoring** to evaluate model performance by comparing `scores.csv` with `ground_truth.csv`.\n",
    "\n",
    "---\n",
    "### Files Generated:\n",
    "- `data/scores.csv` - Model predictions for inference dataset\n",
    "- `reports/inference_metrics.json` - API latency and performance metrics"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}