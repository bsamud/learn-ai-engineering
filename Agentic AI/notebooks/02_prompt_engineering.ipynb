{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Prompt Engineering\n",
    "\n",
    "**Master the art of effective prompting** to get better, more reliable results from LLMs.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand zero-shot vs few-shot prompting\n",
    "- Write effective system prompts\n",
    "- Use chain-of-thought prompting for reasoning\n",
    "- Create reusable prompt templates\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Prompt Engineering Matters](#why)\n",
    "2. [Zero-Shot Prompting](#zero-shot)\n",
    "3. [Few-Shot Prompting](#few-shot)\n",
    "4. [System Prompts & Roles](#system-prompts)\n",
    "5. [Chain-of-Thought Prompting](#cot)\n",
    "6. [Prompt Templates](#templates)\n",
    "7. [Exercises](#exercises)\n",
    "8. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def ask(prompt, system=None, temperature=0.7):\n",
    "    \"\"\"Helper function to make API calls.\"\"\"\n",
    "    messages = []\n",
    "    if system:\n",
    "        messages.append({\"role\": \"system\", \"content\": system})\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages,\n",
    "        temperature=temperature\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why Prompt Engineering Matters <a id='why'></a>\n",
    "\n",
    "The same model can give vastly different results based on how you ask.\n",
    "\n",
    "### Bad Prompt vs Good Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Compare bad and good prompts\n",
    "\n",
    "# Bad prompt - vague\n",
    "bad_prompt = \"Tell me about Python\"\n",
    "print(\"Bad Prompt:\")\n",
    "print(f\"  '{bad_prompt}'\")\n",
    "print(f\"  Response: {ask(bad_prompt)[:200]}...\\n\")\n",
    "\n",
    "# Good prompt - specific\n",
    "good_prompt = \"\"\"Explain Python's list comprehension feature.\n",
    "\n",
    "Include:\n",
    "1. A simple example\n",
    "2. When to use it\n",
    "3. A comparison with regular for loops\n",
    "\n",
    "Keep the response under 150 words.\"\"\"\n",
    "\n",
    "print(\"Good Prompt:\")\n",
    "print(f\"  (structured with clear requirements)\")\n",
    "print(f\"  Response: {ask(good_prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Principles\n",
    "\n",
    "1. **Be Specific**: Tell the model exactly what you want\n",
    "2. **Provide Context**: Give background information\n",
    "3. **Set Constraints**: Specify format, length, style\n",
    "4. **Use Examples**: Show the model what you expect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Zero-Shot Prompting <a id='zero-shot'></a>\n",
    "\n",
    "**Zero-shot** means asking the model to perform a task without any examples.\n",
    "\n",
    "The model relies on its training to understand what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Zero-shot examples\n",
    "\n",
    "# Classification\n",
    "prompt1 = \"\"\"Classify the sentiment of this review as POSITIVE, NEGATIVE, or NEUTRAL:\n",
    "\n",
    "Review: \"The product arrived on time but the quality was disappointing.\"\n",
    "\n",
    "Sentiment:\"\"\"\n",
    "\n",
    "print(\"Classification (zero-shot):\")\n",
    "print(ask(prompt1, temperature=0))\n",
    "\n",
    "# Extraction\n",
    "prompt2 = \"\"\"Extract the following information from this text:\n",
    "- Name\n",
    "- Company\n",
    "- Role\n",
    "\n",
    "Text: \"Hi, I'm Sarah Chen and I work as a Senior Engineer at TechCorp.\"\n",
    "\n",
    "Output as JSON:\"\"\"\n",
    "\n",
    "print(\"\\nExtraction (zero-shot):\")\n",
    "print(ask(prompt2, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When Zero-Shot Works Well\n",
    "\n",
    "- Common tasks (summarization, translation, Q&A)\n",
    "- Clear instructions\n",
    "- Well-defined output format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Few-Shot Prompting <a id='few-shot'></a>\n",
    "\n",
    "**Few-shot** means providing examples of the task before asking the model to perform it.\n",
    "\n",
    "This helps the model understand the pattern you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Few-shot prompting\n",
    "\n",
    "few_shot_prompt = \"\"\"Convert natural language to SQL queries.\n",
    "\n",
    "Example 1:\n",
    "Input: Show all users\n",
    "Output: SELECT * FROM users;\n",
    "\n",
    "Example 2:\n",
    "Input: Find users named John\n",
    "Output: SELECT * FROM users WHERE name = 'John';\n",
    "\n",
    "Example 3:\n",
    "Input: Count total orders\n",
    "Output: SELECT COUNT(*) FROM orders;\n",
    "\n",
    "Now convert:\n",
    "Input: Find all products with price greater than 100\n",
    "Output:\"\"\"\n",
    "\n",
    "print(\"Few-Shot SQL Generation:\")\n",
    "print(ask(few_shot_prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Few-shot for custom formats\n",
    "\n",
    "custom_format_prompt = \"\"\"Generate product descriptions in our brand voice.\n",
    "\n",
    "Example 1:\n",
    "Product: Wireless headphones\n",
    "Description: Experience audio freedom with our sleek wireless headphones. \n",
    "Crystal-clear sound meets all-day comfort. Your soundtrack, untethered.\n",
    "\n",
    "Example 2:\n",
    "Product: Smart water bottle\n",
    "Description: Hydration meets innovation with our smart water bottle.\n",
    "Track your intake, sync with your fitness goals. Drink smarter, live better.\n",
    "\n",
    "Now write:\n",
    "Product: Portable laptop stand\n",
    "Description:\"\"\"\n",
    "\n",
    "print(\"Custom Format Generation:\")\n",
    "print(ask(custom_format_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few-Shot Best Practices\n",
    "\n",
    "1. **Use 2-5 examples** - enough to show the pattern\n",
    "2. **Diverse examples** - cover different cases\n",
    "3. **Consistent format** - same structure in all examples\n",
    "4. **Representative examples** - similar to actual use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. System Prompts & Roles <a id='system-prompts'></a>\n",
    "\n",
    "**System prompts** set the behavior and personality of the model.\n",
    "\n",
    "They're persistent instructions that apply to the entire conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Different system prompts = different responses\n",
    "\n",
    "question = \"What should I do if my code has a bug?\"\n",
    "\n",
    "# Casual assistant\n",
    "casual_system = \"You are a friendly, casual coding buddy. Use informal language and emojis.\"\n",
    "\n",
    "print(\"Casual Assistant:\")\n",
    "print(ask(question, system=casual_system))\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Professional consultant\n",
    "professional_system = \"\"\"You are a senior software engineering consultant.\n",
    "Provide structured, professional advice.\n",
    "Use bullet points and technical terminology.\"\"\"\n",
    "\n",
    "print(\"Professional Consultant:\")\n",
    "print(ask(question, system=professional_system))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Role-based system prompts\n",
    "\n",
    "expert_system = \"\"\"You are an expert Python developer with 15 years of experience.\n",
    "\n",
    "Your expertise includes:\n",
    "- Web development (Django, FastAPI)\n",
    "- Data science (pandas, scikit-learn)\n",
    "- DevOps (Docker, Kubernetes)\n",
    "\n",
    "When answering questions:\n",
    "1. Consider best practices and common pitfalls\n",
    "2. Provide code examples when helpful\n",
    "3. Explain trade-offs between different approaches\n",
    "4. Keep responses concise but complete\"\"\"\n",
    "\n",
    "question = \"What's the best way to handle configuration in a Python app?\"\n",
    "\n",
    "print(\"Expert Response:\")\n",
    "print(ask(question, system=expert_system))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### System Prompt Components\n",
    "\n",
    "A good system prompt includes:\n",
    "\n",
    "1. **Role/Identity**: Who is the assistant?\n",
    "2. **Expertise**: What does it know?\n",
    "3. **Behavior**: How should it respond?\n",
    "4. **Constraints**: What should it avoid?\n",
    "5. **Format**: How to structure responses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Chain-of-Thought Prompting <a id='cot'></a>\n",
    "\n",
    "**Chain-of-thought (CoT)** prompting encourages the model to think step by step.\n",
    "\n",
    "This dramatically improves reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Without chain-of-thought\n",
    "\n",
    "math_problem = \"\"\"If a store has 3 boxes of apples with 12 apples each, \n",
    "and they sell 15 apples, then receive a shipment of 2 boxes with 8 apples each,\n",
    "how many apples do they have?\"\"\"\n",
    "\n",
    "print(\"Without CoT:\")\n",
    "print(ask(math_problem, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: With chain-of-thought\n",
    "\n",
    "cot_prompt = f\"\"\"{math_problem}\n",
    "\n",
    "Think through this step by step:\n",
    "1. Calculate the starting number of apples\n",
    "2. Subtract the apples sold\n",
    "3. Add the new shipment\n",
    "4. Give the final answer\n",
    "\n",
    "Show your work:\"\"\"\n",
    "\n",
    "print(\"With CoT:\")\n",
    "print(ask(cot_prompt, temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Simple CoT trigger\n",
    "\n",
    "# Just adding \"Let's think step by step\" can help!\n",
    "simple_cot = f\"\"\"{math_problem}\n",
    "\n",
    "Let's think step by step.\"\"\"\n",
    "\n",
    "print(\"Simple CoT trigger:\")\n",
    "print(ask(simple_cot, temperature=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Chain-of-Thought\n",
    "\n",
    "- **Math problems**: Multi-step calculations\n",
    "- **Logic puzzles**: Reasoning through constraints\n",
    "- **Complex analysis**: Breaking down problems\n",
    "- **Decision making**: Weighing options\n",
    "- **Debugging**: Tracing through code logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Prompt Templates <a id='templates'></a>\n",
    "\n",
    "Create reusable prompt templates for consistent results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Create a prompt template class\n",
    "\n",
    "class PromptTemplate:\n",
    "    \"\"\"A reusable prompt template with variable substitution.\"\"\"\n",
    "    \n",
    "    def __init__(self, template: str):\n",
    "        self.template = template\n",
    "    \n",
    "    def format(self, **kwargs) -> str:\n",
    "        \"\"\"Fill in the template with provided values.\"\"\"\n",
    "        return self.template.format(**kwargs)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"PromptTemplate({self.template[:50]}...)\"\n",
    "\n",
    "\n",
    "# Create a code review template\n",
    "code_review_template = PromptTemplate(\"\"\"\n",
    "Review this {language} code and provide feedback:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Focus on:\n",
    "1. Code correctness\n",
    "2. Best practices\n",
    "3. Potential improvements\n",
    "\n",
    "Provide a rating from 1-10 and explain your reasoning.\n",
    "\"\"\")\n",
    "\n",
    "# Use the template\n",
    "prompt = code_review_template.format(\n",
    "    language=\"python\",\n",
    "    code=\"def add(a, b): return a + b\"\n",
    ")\n",
    "\n",
    "print(\"Generated Prompt:\")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Library of prompt templates\n",
    "\n",
    "PROMPT_LIBRARY = {\n",
    "    \"summarize\": PromptTemplate(\n",
    "        \"Summarize the following text in {num_sentences} sentences:\\n\\n{text}\"\n",
    "    ),\n",
    "    \n",
    "    \"translate\": PromptTemplate(\n",
    "        \"Translate the following {source_lang} text to {target_lang}:\\n\\n{text}\"\n",
    "    ),\n",
    "    \n",
    "    \"explain_code\": PromptTemplate(\n",
    "        \"\"\"Explain this {language} code to a {audience}:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Focus on what the code does, not how it works syntactically.\"\"\"\n",
    "    ),\n",
    "    \n",
    "    \"debug\": PromptTemplate(\n",
    "        \"\"\"Debug this {language} code:\n",
    "\n",
    "```{language}\n",
    "{code}\n",
    "```\n",
    "\n",
    "Error message: {error}\n",
    "\n",
    "Identify the bug and provide a fix.\"\"\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Use a template from the library\n",
    "prompt = PROMPT_LIBRARY[\"explain_code\"].format(\n",
    "    language=\"python\",\n",
    "    audience=\"beginner programmer\",\n",
    "    code=\"result = [x**2 for x in range(10) if x % 2 == 0]\"\n",
    ")\n",
    "\n",
    "print(\"Code Explanation:\")\n",
    "print(ask(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Exercises <a id='exercises'></a>\n",
    "\n",
    "Practice your prompt engineering skills!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Few-Shot Classifier\n",
    "\n",
    "Create a few-shot prompt that classifies customer support tickets into categories:\n",
    "- BILLING\n",
    "- TECHNICAL\n",
    "- GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a few-shot prompt for ticket classification\n",
    "# Include 2-3 examples, then classify: \"My payment didn't go through yesterday\"\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: System Prompt Design\n",
    "\n",
    "Design a system prompt for an AI coding tutor that:\n",
    "- Teaches beginners\n",
    "- Uses encouraging language\n",
    "- Gives hints instead of full solutions\n",
    "- Asks follow-up questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Design a system prompt for a coding tutor\n",
    "# Then test it with a beginner question like \"How do I create a list in Python?\"\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Chain-of-Thought Problem\n",
    "\n",
    "Use CoT prompting to solve:\n",
    "\"A train travels 120km in 2 hours. If it needs to travel 300km total and has already traveled for 2.5 hours at the same speed, how much longer will it take?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use chain-of-thought to solve the train problem\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You understand zero-shot vs few-shot prompting\n",
    "- [ ] You can write effective system prompts\n",
    "- [ ] You know when and how to use chain-of-thought\n",
    "- [ ] You created at least one prompt template\n",
    "- [ ] You completed at least 2 exercises\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll learn about **Tool Use Fundamentals** - giving LLMs the ability to use external tools and APIs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Key Techniques:**\n",
    "\n",
    "| Technique | Use Case |\n",
    "|-----------|----------|\n",
    "| Zero-shot | Common tasks with clear instructions |\n",
    "| Few-shot | Custom formats, specific patterns |\n",
    "| System prompts | Consistent behavior and personality |\n",
    "| Chain-of-thought | Reasoning and complex problems |\n",
    "| Templates | Reusable, consistent prompts |\n",
    "\n",
    "**Remember:**\n",
    "- Be specific and provide context\n",
    "- Use examples when the task is non-standard\n",
    "- For reasoning tasks, ask the model to think step by step\n",
    "- Create templates for repeated use cases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
