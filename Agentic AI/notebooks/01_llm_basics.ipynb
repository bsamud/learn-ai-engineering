{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - LLM Basics\n",
    "\n",
    "**Welcome to Agentic AI!** This is your first notebook in the journey to building AI agents.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand what Large Language Models (LLMs) are\n",
    "- Set up API access for OpenAI and Anthropic\n",
    "- Make your first API calls\n",
    "- Understand key concepts: tokens, context windows, temperature\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What are LLMs?](#what-are-llms)\n",
    "2. [Setup & Configuration](#setup)\n",
    "3. [Your First API Call](#first-call)\n",
    "4. [Understanding Tokens](#tokens)\n",
    "5. [Parameters: Temperature & More](#parameters)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What are LLMs? <a id='what-are-llms'></a>\n",
    "\n",
    "**Large Language Models (LLMs)** are AI systems trained on massive amounts of text data. They learn patterns in language and can:\n",
    "\n",
    "- Generate human-like text\n",
    "- Answer questions\n",
    "- Summarize documents\n",
    "- Write code\n",
    "- And much more!\n",
    "\n",
    "### How Do They Work? (Simplified)\n",
    "\n",
    "1. **Training**: The model reads billions of text examples (books, websites, code)\n",
    "2. **Pattern Learning**: It learns statistical patterns in language\n",
    "3. **Generation**: Given a prompt, it predicts the most likely next words\n",
    "\n",
    "### Popular LLMs\n",
    "\n",
    "| Provider | Models | Strengths |\n",
    "|----------|--------|----------|\n",
    "| OpenAI | GPT-4, GPT-4o, GPT-4o-mini | General purpose, code, reasoning |\n",
    "| Anthropic | Claude 3.5 Sonnet, Claude 3 Opus | Safety, long context, analysis |\n",
    "| Google | Gemini Pro, Gemini Ultra | Multimodal, integration |\n",
    "| Meta | Llama 3 | Open source, customizable |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Setup & Configuration <a id='setup'></a>\n",
    "\n",
    "First, let's set up our environment and API keys.\n",
    "\n",
    "### Install Dependencies\n",
    "\n",
    "Run this cell to ensure all packages are installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Install required packages\n",
    "# Uncomment and run if packages are not installed\n",
    "# !pip install openai anthropic python-dotenv tiktoken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Import required libraries\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path for our utilities\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# For direct API usage\n",
    "from openai import OpenAI\n",
    "from anthropic import Anthropic\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure API Keys\n",
    "\n",
    "You'll need API keys from at least one provider:\n",
    "- **OpenAI**: Get your key at [platform.openai.com](https://platform.openai.com/api-keys)\n",
    "- **Anthropic**: Get your key at [console.anthropic.com](https://console.anthropic.com/)\n",
    "\n",
    "Create a `.env` file in the `Agentic AI/` folder with:\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Load API keys from .env file\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "# Check which keys are available\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "print(\"API Key Status:\")\n",
    "print(f\"  OpenAI: {'✓ Found' if openai_key else '✗ Not found'}\")\n",
    "print(f\"  Anthropic: {'✓ Found' if anthropic_key else '✗ Not found'}\")\n",
    "\n",
    "if not openai_key and not anthropic_key:\n",
    "    print(\"\\n⚠️ No API keys found! Please add them to your .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Your First API Call <a id='first-call'></a>\n",
    "\n",
    "Let's make your first call to an LLM!\n",
    "\n",
    "### OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Make your first OpenAI API call\n",
    "if openai_key:\n",
    "    # Initialize the client\n",
    "    client = OpenAI()\n",
    "    \n",
    "    # Make a chat completion request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # Using the smaller, faster model\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Hello! What's your name?\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract the response\n",
    "    answer = response.choices[0].message.content\n",
    "    print(\"OpenAI Response:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"OpenAI API key not found. Skipping this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anthropic API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Make your first Anthropic API call\n",
    "if anthropic_key:\n",
    "    # Initialize the client\n",
    "    client = Anthropic()\n",
    "    \n",
    "    # Make a message request\n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-5-sonnet-20241022\",\n",
    "        max_tokens=1024,\n",
    "        system=\"You are a helpful assistant.\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Hello! What's your name?\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Extract the response\n",
    "    answer = response.content[0].text\n",
    "    print(\"Anthropic Response:\")\n",
    "    print(answer)\n",
    "else:\n",
    "    print(\"Anthropic API key not found. Skipping this example.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Response Structure\n",
    "\n",
    "Let's examine what we get back from the API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Examine the full response object\n",
    "if openai_key:\n",
    "    client = OpenAI()\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Say hello in 3 words.\"}]\n",
    "    )\n",
    "    \n",
    "    print(\"Full Response Object:\")\n",
    "    print(f\"  Model: {response.model}\")\n",
    "    print(f\"  Finish Reason: {response.choices[0].finish_reason}\")\n",
    "    print(f\"  Content: {response.choices[0].message.content}\")\n",
    "    print(f\"\\nUsage:\")\n",
    "    print(f\"  Prompt tokens: {response.usage.prompt_tokens}\")\n",
    "    print(f\"  Completion tokens: {response.usage.completion_tokens}\")\n",
    "    print(f\"  Total tokens: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Understanding Tokens <a id='tokens'></a>\n",
    "\n",
    "**Tokens** are the basic units that LLMs process. A token is roughly:\n",
    "- 4 characters in English\n",
    "- 3/4 of a word\n",
    "\n",
    "### Why Tokens Matter\n",
    "\n",
    "1. **Pricing**: You pay per token (input + output)\n",
    "2. **Context Limits**: Each model has a maximum context window\n",
    "3. **Speed**: More tokens = longer processing time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Count tokens using tiktoken\n",
    "import tiktoken\n",
    "\n",
    "# Get the encoder for GPT-4\n",
    "encoder = tiktoken.encoding_for_model(\"gpt-4\")\n",
    "\n",
    "# Example texts\n",
    "texts = [\n",
    "    \"Hello!\",\n",
    "    \"Hello, how are you today?\",\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"def fibonacci(n):\\n    if n <= 1: return n\\n    return fibonacci(n-1) + fibonacci(n-2)\"\n",
    "]\n",
    "\n",
    "print(\"Token Counting Examples:\")\n",
    "print(\"=\" * 50)\n",
    "for text in texts:\n",
    "    tokens = encoder.encode(text)\n",
    "    print(f\"\\nText: {text[:50]}{'...' if len(text) > 50 else ''}\")\n",
    "    print(f\"Characters: {len(text)}\")\n",
    "    print(f\"Tokens: {len(tokens)}\")\n",
    "    print(f\"Ratio: {len(text)/len(tokens):.1f} chars per token\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Windows\n",
    "\n",
    "Each model has a maximum context size:\n",
    "\n",
    "| Model | Context Window |\n",
    "|-------|---------------|\n",
    "| GPT-4o | 128K tokens |\n",
    "| GPT-4o-mini | 128K tokens |\n",
    "| Claude 3.5 Sonnet | 200K tokens |\n",
    "| Claude 3 Opus | 200K tokens |\n",
    "\n",
    "Context includes: system prompt + conversation history + user message + response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Parameters: Temperature & More <a id='parameters'></a>\n",
    "\n",
    "You can control LLM behavior with various parameters.\n",
    "\n",
    "### Temperature\n",
    "\n",
    "Controls randomness in responses:\n",
    "- **0.0**: Deterministic, most likely tokens\n",
    "- **0.7**: Balanced creativity (default)\n",
    "- **1.0+**: More random, creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Experiment with temperature\n",
    "if openai_key:\n",
    "    client = OpenAI()\n",
    "    \n",
    "    prompt = \"Write a one-sentence story about a robot.\"\n",
    "    \n",
    "    print(\"Temperature Comparison:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for temp in [0.0, 0.7, 1.2]:\n",
    "        print(f\"\\nTemperature: {temp}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Run 3 times to see variation\n",
    "        for i in range(3):\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temp,\n",
    "                max_tokens=50\n",
    "            )\n",
    "            print(f\"  {i+1}. {response.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Important Parameters\n",
    "\n",
    "| Parameter | Description | Typical Values |\n",
    "|-----------|-------------|----------------|\n",
    "| `max_tokens` | Maximum response length | 100-4096 |\n",
    "| `temperature` | Randomness | 0.0-2.0 |\n",
    "| `top_p` | Nucleus sampling | 0.0-1.0 |\n",
    "| `stop` | Stop sequences | List of strings |\n",
    "| `presence_penalty` | Discourage repetition | -2.0 to 2.0 |\n",
    "| `frequency_penalty` | Discourage repeated words | -2.0 to 2.0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Using max_tokens to control response length\n",
    "if openai_key:\n",
    "    client = OpenAI()\n",
    "    \n",
    "    print(\"Max Tokens Comparison:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for max_tokens in [20, 50, 100]:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Explain what Python is.\"}],\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        content = response.choices[0].message.content\n",
    "        finish = response.choices[0].finish_reason\n",
    "        \n",
    "        print(f\"\\nmax_tokens={max_tokens} (finish_reason: {finish}):\")\n",
    "        print(f\"  {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises <a id='exercises'></a>\n",
    "\n",
    "Now it's your turn! Complete these exercises to practice what you've learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Basic API Call\n",
    "\n",
    "Make an API call asking the model to explain what an AI agent is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Make an API call to explain what an AI agent is\n",
    "# Use either OpenAI or Anthropic based on which key you have\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Token Estimation\n",
    "\n",
    "Estimate the cost of processing a document. Given:\n",
    "- Document: 5000 words\n",
    "- Expected response: 500 words\n",
    "- Model: GPT-4o-mini ($0.15/1M input, $0.60/1M output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the estimated cost\n",
    "# Hint: ~0.75 tokens per word\n",
    "\n",
    "# Your code here:\n",
    "document_words = 5000\n",
    "response_words = 500\n",
    "\n",
    "# Calculate tokens\n",
    "\n",
    "# Calculate cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Temperature Experiment\n",
    "\n",
    "Ask the model to generate a creative name for an AI startup. Run it at temperature 0.0 and 1.0. What differences do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare responses at different temperatures\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving to the next notebook, verify:\n",
    "\n",
    "- [ ] You have at least one API key configured\n",
    "- [ ] You successfully made an API call\n",
    "- [ ] You understand what tokens are and why they matter\n",
    "- [ ] You experimented with temperature and max_tokens\n",
    "- [ ] You completed at least 2 exercises\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll dive into **Prompt Engineering** - the art of crafting effective prompts to get better results from LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **LLMs** are AI systems that generate text by predicting the most likely next tokens\n",
    "2. **API calls** follow a simple pattern: create a client, send messages, get response\n",
    "3. **Tokens** are the units of processing - they affect cost, speed, and limits\n",
    "4. **Temperature** controls creativity - lower = deterministic, higher = random\n",
    "5. **Context windows** limit how much text you can process at once\n",
    "\n",
    "**Resources:**\n",
    "- [OpenAI API Documentation](https://platform.openai.com/docs)\n",
    "- [Anthropic API Documentation](https://docs.anthropic.com)\n",
    "- [Tiktoken Tokenizer](https://github.com/openai/tiktoken)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
