{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06 - Fine-tuning Basics\n",
    "\n",
    "**When and how to fine-tune language models.**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Know when to fine-tune vs prompt vs RAG\n",
    "- Prepare training data properly\n",
    "- Fine-tune models using OpenAI API\n",
    "- Evaluate fine-tuned models\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [When to Fine-tune](#when)\n",
    "2. [Data Preparation](#data)\n",
    "3. [OpenAI Fine-tuning](#openai)\n",
    "4. [Evaluation](#evaluation)\n",
    "5. [Best Practices](#practices)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. When to Fine-tune <a id='when'></a>\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "```\n",
    "Need specific behavior?\n",
    "│\n",
    "├─ Few examples work? ──────────► Use Few-Shot Prompting\n",
    "│\n",
    "├─ Need external knowledge? ────► Use RAG\n",
    "│\n",
    "├─ Consistent style/format? ────► Consider Fine-tuning\n",
    "│\n",
    "└─ Domain-specific tasks? ──────► Fine-tuning + RAG\n",
    "```\n",
    "\n",
    "### When Fine-tuning Makes Sense:\n",
    "- Consistent output format (JSON structure, specific style)\n",
    "- Domain-specific terminology or behavior\n",
    "- Reducing prompt length (examples → learned behavior)\n",
    "- Cost optimization at scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Data Preparation <a id='data'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Create training examples\n",
    "from src.finetuning_utils import TrainingExample\n",
    "\n",
    "# Example: Training a sentiment classifier\n",
    "examples = [\n",
    "    TrainingExample(\n",
    "        instruction=\"Classify the sentiment of this review.\",\n",
    "        input=\"This product is amazing! Best purchase ever.\",\n",
    "        output=\"positive\",\n",
    "        system=\"You are a sentiment classifier. Respond with: positive, negative, or neutral.\"\n",
    "    ),\n",
    "    TrainingExample(\n",
    "        instruction=\"Classify the sentiment of this review.\",\n",
    "        input=\"Terrible quality. Broke after one day.\",\n",
    "        output=\"negative\",\n",
    "        system=\"You are a sentiment classifier. Respond with: positive, negative, or neutral.\"\n",
    "    ),\n",
    "    TrainingExample(\n",
    "        instruction=\"Classify the sentiment of this review.\",\n",
    "        input=\"It's okay. Nothing special but works fine.\",\n",
    "        output=\"neutral\",\n",
    "        system=\"You are a sentiment classifier. Respond with: positive, negative, or neutral.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(examples)} training examples\")\n",
    "for ex in examples:\n",
    "    print(f\"  Input: {ex.input[:40]}... -> {ex.output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Format data for OpenAI\n",
    "from src.finetuning_utils import format_for_openai\n",
    "from pathlib import Path\n",
    "\n",
    "# Create more examples for a realistic dataset\n",
    "sentiment_data = [\n",
    "    (\"Love this product! Exceeded expectations.\", \"positive\"),\n",
    "    (\"Works great, highly recommend!\", \"positive\"),\n",
    "    (\"Best purchase I've made this year.\", \"positive\"),\n",
    "    (\"Absolutely fantastic quality.\", \"positive\"),\n",
    "    (\"Don't waste your money on this.\", \"negative\"),\n",
    "    (\"Broke within a week. Disappointing.\", \"negative\"),\n",
    "    (\"Customer service was unhelpful.\", \"negative\"),\n",
    "    (\"Poor quality, not worth the price.\", \"negative\"),\n",
    "    (\"It's fine. Does what it's supposed to.\", \"neutral\"),\n",
    "    (\"Average product, nothing special.\", \"neutral\"),\n",
    "    (\"Meets basic expectations.\", \"neutral\"),\n",
    "    (\"Neither good nor bad.\", \"neutral\"),\n",
    "]\n",
    "\n",
    "examples = [\n",
    "    TrainingExample(\n",
    "        instruction=\"Classify the sentiment.\",\n",
    "        input=text,\n",
    "        output=label,\n",
    "        system=\"Classify sentiment as: positive, negative, or neutral.\"\n",
    "    )\n",
    "    for text, label in sentiment_data\n",
    "]\n",
    "\n",
    "# Save as JSONL\n",
    "output_dir = Path(\"../data/training_data\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "output_path = output_dir / \"sentiment_train.jsonl\"\n",
    "\n",
    "format_for_openai(examples, str(output_path))\n",
    "\n",
    "# Show what the file looks like\n",
    "print(\"\\nFile contents (first entry):\")\n",
    "with open(output_path) as f:\n",
    "    first_line = json.loads(f.readline())\n",
    "    print(json.dumps(first_line, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Validate training data\n",
    "from src.finetuning_utils import validate_training_data\n",
    "\n",
    "results = validate_training_data(examples)\n",
    "\n",
    "print(\"Validation Results:\")\n",
    "print(f\"  Total examples: {results['total']}\")\n",
    "print(f\"  Valid examples: {results['valid']}\")\n",
    "print(f\"  Issues found: {len(results['issues'])}\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Avg instruction length: {results['stats']['avg_instruction_length']:.0f} chars\")\n",
    "print(f\"  Avg output length: {results['stats']['avg_output_length']:.0f} chars\")\n",
    "print(f\"  Duplicates: {results['stats']['duplicates']}\")\n",
    "print(f\"  Empty outputs: {results['stats']['empty_outputs']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Split into train/val/test\n",
    "from src.finetuning_utils import split_data\n",
    "\n",
    "train, val, test = split_data(\n",
    "    examples,\n",
    "    train_ratio=0.7,\n",
    "    val_ratio=0.15,\n",
    "    test_ratio=0.15\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train)} examples\")\n",
    "print(f\"Val: {len(val)} examples\")\n",
    "print(f\"Test: {len(test)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. OpenAI Fine-tuning <a id='openai'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Fine-tuning with OpenAI (demonstration)\n",
    "from src.finetuning_utils import OpenAIFineTuner\n",
    "\n",
    "# Note: This will use your API credits\n",
    "# Uncomment to actually run\n",
    "\n",
    "# tuner = OpenAIFineTuner()\n",
    "\n",
    "# Create a job\n",
    "# job = tuner.create_job(\n",
    "#     training_file=\"../data/training_data/sentiment_train.jsonl\",\n",
    "#     model=\"gpt-4o-mini-2024-07-18\",\n",
    "#     epochs=3,\n",
    "#     suffix=\"sentiment-v1\"\n",
    "# )\n",
    "\n",
    "# print(f\"Job created: {job.id}\")\n",
    "# print(f\"Status: {job.status}\")\n",
    "\n",
    "print(\"Fine-tuning demonstration (commented out to avoid API costs)\")\n",
    "print(\"\"\"\\nTo actually fine-tune:\n",
    "1. Uncomment the code above\n",
    "2. Ensure you have 10+ training examples\n",
    "3. Wait for the job to complete (check status with tuner.get_status(job.id))\n",
    "4. Use the fine-tuned model name from job.fine_tuned_model\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Check job status\n",
    "# tuner = OpenAIFineTuner()\n",
    "\n",
    "# List recent jobs\n",
    "# jobs = tuner.list_jobs(limit=5)\n",
    "# for job in jobs:\n",
    "#     print(f\"{job['id']}: {job['status']} - {job['model']}\")\n",
    "\n",
    "# Get specific job status\n",
    "# status = tuner.get_status(\"ftjob-xxx\")\n",
    "# print(f\"Status: {status['status']}\")\n",
    "# print(f\"Model: {status['model']}\")\n",
    "\n",
    "print(\"Status checking demonstration (uncomment to run)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Use a fine-tuned model\n",
    "from openai import OpenAI\n",
    "\n",
    "# Replace with your fine-tuned model name\n",
    "# fine_tuned_model = \"ft:gpt-4o-mini-2024-07-18:org::xxxxx\"\n",
    "\n",
    "# client = OpenAI()\n",
    "# response = client.chat.completions.create(\n",
    "#     model=fine_tuned_model,\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"Classify sentiment as: positive, negative, or neutral.\"},\n",
    "#         {\"role\": \"user\", \"content\": \"This is the best thing I've ever bought!\"}\n",
    "#     ]\n",
    "# )\n",
    "# print(response.choices[0].message.content)\n",
    "\n",
    "print(\"Using fine-tuned model demonstration (uncomment with your model name)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Evaluation <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Evaluate model performance\n",
    "from src.evaluation import Evaluator\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "# Create test cases\n",
    "evaluator = Evaluator()\n",
    "\n",
    "test_cases = [\n",
    "    (\"Absolutely love it!\", \"positive\"),\n",
    "    (\"Worst purchase ever.\", \"negative\"),\n",
    "    (\"It works, I guess.\", \"neutral\"),\n",
    "    (\"Outstanding quality and fast shipping!\", \"positive\"),\n",
    "    (\"Disappointed with the product.\", \"negative\"),\n",
    "]\n",
    "\n",
    "for i, (text, expected) in enumerate(test_cases):\n",
    "    evaluator.add_test(\n",
    "        id=f\"test_{i}\",\n",
    "        input=text,\n",
    "        expected=expected\n",
    "    )\n",
    "\n",
    "# Define the system to test (using base model for demo)\n",
    "def sentiment_classifier(text: str) -> str:\n",
    "    client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "    response = client.chat(\n",
    "        message=f\"Classify this sentiment: {text}\",\n",
    "        system=\"Respond with exactly one word: positive, negative, or neutral.\"\n",
    "    )\n",
    "    return response.strip().lower()\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.run(sentiment_classifier)\n",
    "print(results.summary())\n",
    "\n",
    "# Show individual results\n",
    "print(\"\\nDetailed results:\")\n",
    "for r in results.results:\n",
    "    status = \"PASS\" if r.passed else \"FAIL\"\n",
    "    print(f\"  [{status}] {r.input[:30]}... -> {r.actual} (expected: {r.expected})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Best Practices <a id='practices'></a>\n",
    "\n",
    "### Data Quality\n",
    "- Minimum 10 examples (50-100+ recommended)\n",
    "- Diverse examples covering edge cases\n",
    "- Consistent format across all examples\n",
    "- Clean, accurate labels\n",
    "\n",
    "### Training\n",
    "- Start with fewer epochs (3) and increase if needed\n",
    "- Monitor validation loss\n",
    "- Don't overtrain on small datasets\n",
    "\n",
    "### Evaluation\n",
    "- Hold out test set for final evaluation\n",
    "- Compare against base model performance\n",
    "- Test on edge cases and adversarial examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Create Training Dataset\n",
    "\n",
    "Create a training dataset for a custom task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create 20+ training examples for a task of your choice\n",
    "# Ideas: code comments, email classification, product categorization\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compare Base vs Fine-tuned\n",
    "\n",
    "Design an experiment to compare performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluation comparing base model with few-shot vs fine-tuned\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You know when fine-tuning is appropriate\n",
    "- [ ] You can prepare training data\n",
    "- [ ] You understand the fine-tuning API\n",
    "- [ ] You can evaluate model performance\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **LoRA & PEFT** - efficient fine-tuning for open models!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
