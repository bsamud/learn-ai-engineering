{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08 - Evaluation & Testing\n",
    "\n",
    "**Build robust evaluation frameworks for AI systems.**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Design evaluation frameworks\n",
    "- Build test datasets\n",
    "- Use LLM-as-judge for evaluation\n",
    "- Implement regression testing\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why Evaluation Matters](#why)\n",
    "2. [Building Test Datasets](#datasets)\n",
    "3. [Automated Evaluation](#auto)\n",
    "4. [LLM-as-Judge](#llm-judge)\n",
    "5. [Regression Testing](#regression)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why Evaluation Matters <a id='why'></a>\n",
    "\n",
    "### Challenges with AI Systems:\n",
    "- **Non-deterministic**: Same input can give different outputs\n",
    "- **Subjective quality**: \"Good\" is hard to define\n",
    "- **Emergent behavior**: Changes can have unexpected effects\n",
    "\n",
    "### Evaluation Types:\n",
    "```\n",
    "Type              Use Case                   Example Metrics\n",
    "──────────────────────────────────────────────────────────────\n",
    "Exact Match       Classification, QA         Accuracy, F1\n",
    "Similarity        Text generation            BLEU, ROUGE\n",
    "Semantic          Open-ended generation      LLM-as-Judge\n",
    "Task-specific     RAG, Agents                Precision@K, Success rate\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Building Test Datasets <a id='datasets'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Create test cases\n",
    "from src.evaluation import TestCase, Evaluator\n",
    "\n",
    "# Define test cases\n",
    "test_cases = [\n",
    "    TestCase(\n",
    "        id=\"math_1\",\n",
    "        input=\"What is 2 + 2?\",\n",
    "        expected=\"4\",\n",
    "        metadata={\"category\": \"math\", \"difficulty\": \"easy\"}\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"math_2\",\n",
    "        input=\"What is the square root of 144?\",\n",
    "        expected=\"12\",\n",
    "        metadata={\"category\": \"math\", \"difficulty\": \"medium\"}\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"fact_1\",\n",
    "        input=\"What is the capital of France?\",\n",
    "        expected=\"Paris\",\n",
    "        metadata={\"category\": \"geography\", \"difficulty\": \"easy\"}\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"fact_2\",\n",
    "        input=\"Who wrote Romeo and Juliet?\",\n",
    "        expected=\"Shakespeare\",\n",
    "        metadata={\"category\": \"literature\", \"difficulty\": \"easy\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(test_cases)} test cases\")\n",
    "for tc in test_cases:\n",
    "    print(f\"  [{tc.id}] {tc.input[:40]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Save and load test cases\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Save test cases\n",
    "test_data = [\n",
    "    {\n",
    "        \"id\": tc.id,\n",
    "        \"input\": tc.input,\n",
    "        \"expected\": tc.expected,\n",
    "        \"metadata\": tc.metadata\n",
    "    }\n",
    "    for tc in test_cases\n",
    "]\n",
    "\n",
    "test_file = Path(\"../data/test_cases.json\")\n",
    "test_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(test_file, \"w\") as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "\n",
    "print(f\"Saved to {test_file}\")\n",
    "\n",
    "# Load test cases\n",
    "evaluator = Evaluator()\n",
    "evaluator.add_tests_from_json(str(test_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Automated Evaluation <a id='auto'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Run automated evaluation\n",
    "from src.evaluation import Evaluator\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "# Create evaluator and add tests\n",
    "evaluator = Evaluator()\n",
    "for tc in test_cases:\n",
    "    evaluator.add_test(tc.id, tc.input, tc.expected, **tc.metadata)\n",
    "\n",
    "# Define the system to test\n",
    "client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "def qa_system(question: str) -> str:\n",
    "    return client.chat(\n",
    "        message=question,\n",
    "        system=\"Answer the question concisely in one or two words when possible.\"\n",
    "    )\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluator.run(qa_system)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(results.summary())\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "for r in results.results:\n",
    "    status = \"PASS\" if r.passed else \"FAIL\"\n",
    "    print(f\"[{status}] {r.id}: '{r.actual}' (expected: '{r.expected}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Custom evaluation function\n",
    "def fuzzy_match_evaluator(\n",
    "    input: str,\n",
    "    actual: str,\n",
    "    expected: str\n",
    ") -> tuple[bool, float, str]:\n",
    "    \"\"\"Fuzzy matching that handles variations.\"\"\"\n",
    "    actual_clean = actual.strip().lower()\n",
    "    expected_clean = expected.strip().lower()\n",
    "    \n",
    "    # Exact match\n",
    "    if actual_clean == expected_clean:\n",
    "        return True, 1.0, \"Exact match\"\n",
    "    \n",
    "    # Expected in actual\n",
    "    if expected_clean in actual_clean:\n",
    "        return True, 0.9, \"Expected found in response\"\n",
    "    \n",
    "    # Partial match (for numbers, names, etc.)\n",
    "    actual_words = set(actual_clean.split())\n",
    "    expected_words = set(expected_clean.split())\n",
    "    \n",
    "    if expected_words & actual_words:\n",
    "        overlap = len(expected_words & actual_words) / len(expected_words)\n",
    "        return overlap >= 0.5, overlap, f\"Partial match ({overlap:.0%})\"\n",
    "    \n",
    "    return False, 0.0, f\"No match: expected '{expected}', got '{actual}'\"\n",
    "\n",
    "# Run with custom evaluator\n",
    "results = evaluator.run(qa_system, evaluator_fn=fuzzy_match_evaluator)\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. LLM-as-Judge <a id='llm-judge'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Use LLM to evaluate outputs\n",
    "from src.evaluation import LLMJudge\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "llm = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "judge = LLMJudge(llm)\n",
    "\n",
    "# Evaluate a response\n",
    "question = \"Explain what machine learning is in simple terms.\"\n",
    "answer = \"\"\"Machine learning is when computers learn from examples instead of \n",
    "being explicitly programmed. It's like how a child learns to recognize cats \n",
    "by seeing many pictures of cats.\"\"\"\n",
    "\n",
    "score, feedback = judge.evaluate(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    criteria=\"accuracy, clarity, simplicity, completeness\"\n",
    ")\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"\\nAnswer: {answer}\")\n",
    "print(f\"\\nScore: {score:.2f}\")\n",
    "print(f\"Feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Compare two responses\n",
    "question = \"What are the benefits of exercise?\"\n",
    "\n",
    "answer_a = \"Exercise is good for you. It helps with fitness.\"\n",
    "\n",
    "answer_b = \"\"\"Regular exercise offers numerous benefits including improved \n",
    "cardiovascular health, better mental well-being, weight management, \n",
    "increased energy levels, and reduced risk of chronic diseases.\"\"\"\n",
    "\n",
    "winner, explanation = judge.compare(question, answer_a, answer_b)\n",
    "\n",
    "print(f\"Question: {question}\\n\")\n",
    "print(f\"Answer A: {answer_a}\\n\")\n",
    "print(f\"Answer B: {answer_b}\\n\")\n",
    "print(f\"Winner: {winner}\")\n",
    "print(f\"Explanation: {explanation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Create LLM-based evaluator function\n",
    "def llm_evaluator(input: str, actual: str, expected: str) -> tuple[bool, float, str]:\n",
    "    \"\"\"Use LLM to evaluate responses.\"\"\"\n",
    "    score, feedback = judge.evaluate(\n",
    "        question=input,\n",
    "        answer=actual,\n",
    "        criteria=\"accuracy, relevance\",\n",
    "        reference=expected\n",
    "    )\n",
    "    return score >= 0.7, score, feedback\n",
    "\n",
    "# Test open-ended questions\n",
    "open_evaluator = Evaluator()\n",
    "open_evaluator.add_test(\n",
    "    id=\"explain_1\",\n",
    "    input=\"What is photosynthesis?\",\n",
    "    expected=\"The process by which plants convert sunlight into energy.\"\n",
    ")\n",
    "\n",
    "def explain_system(question: str) -> str:\n",
    "    return client.chat(\n",
    "        message=question,\n",
    "        system=\"Explain concepts clearly and concisely.\"\n",
    "    )\n",
    "\n",
    "results = open_evaluator.run(explain_system, evaluator_fn=llm_evaluator)\n",
    "print(results.summary())\n",
    "print(f\"\\nFeedback: {results.results[0].feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Regression Testing <a id='regression'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Create regression test suite\n",
    "from dataclasses import dataclass, field\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "@dataclass\n",
    "class RegressionSuite:\n",
    "    \"\"\"Manage regression tests across versions.\"\"\"\n",
    "    \n",
    "    name: str\n",
    "    tests: list[dict] = field(default_factory=list)\n",
    "    history: list[dict] = field(default_factory=list)\n",
    "    \n",
    "    def add_test(self, id: str, input: str, baseline_output: str):\n",
    "        \"\"\"Add a test with baseline output.\"\"\"\n",
    "        self.tests.append({\n",
    "            \"id\": id,\n",
    "            \"input\": input,\n",
    "            \"baseline\": baseline_output\n",
    "        })\n",
    "    \n",
    "    def run(self, system_fn, threshold: float = 0.8):\n",
    "        \"\"\"Run regression tests.\"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for test in self.tests:\n",
    "            output = system_fn(test[\"input\"])\n",
    "            \n",
    "            # Compare with baseline using similarity\n",
    "            score, feedback = judge.evaluate(\n",
    "                question=test[\"input\"],\n",
    "                answer=output,\n",
    "                criteria=\"semantic similarity\",\n",
    "                reference=test[\"baseline\"]\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                \"id\": test[\"id\"],\n",
    "                \"output\": output,\n",
    "                \"baseline\": test[\"baseline\"],\n",
    "                \"score\": score,\n",
    "                \"regressed\": score < threshold\n",
    "            })\n",
    "        \n",
    "        # Record run\n",
    "        run_result = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"results\": results,\n",
    "            \"regressions\": sum(1 for r in results if r[\"regressed\"])\n",
    "        }\n",
    "        self.history.append(run_result)\n",
    "        \n",
    "        return run_result\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        \"\"\"Save suite to file.\"\"\"\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"name\": self.name,\n",
    "                \"tests\": self.tests,\n",
    "                \"history\": self.history\n",
    "            }, f, indent=2)\n",
    "\n",
    "# Create regression suite\n",
    "suite = RegressionSuite(name=\"QA Regression\")\n",
    "suite.add_test(\n",
    "    id=\"qa_1\",\n",
    "    input=\"What is the capital of Japan?\",\n",
    "    baseline_output=\"Tokyo\"\n",
    ")\n",
    "suite.add_test(\n",
    "    id=\"qa_2\",\n",
    "    input=\"What is 10 * 5?\",\n",
    "    baseline_output=\"50\"\n",
    ")\n",
    "\n",
    "print(f\"Created regression suite with {len(suite.tests)} tests\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Run regression tests\n",
    "run_result = suite.run(qa_system)\n",
    "\n",
    "print(f\"Regression Run: {run_result['timestamp']}\")\n",
    "print(f\"Regressions found: {run_result['regressions']}\\n\")\n",
    "\n",
    "for r in run_result['results']:\n",
    "    status = \"REGRESSION\" if r['regressed'] else \"OK\"\n",
    "    print(f\"[{status}] {r['id']}: score={r['score']:.2f}\")\n",
    "    if r['regressed']:\n",
    "        print(f\"   Baseline: {r['baseline']}\")\n",
    "        print(f\"   Current:  {r['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build Test Suite\n",
    "\n",
    "Create a comprehensive test suite for a RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create test cases covering: factual accuracy, relevance, completeness\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Custom Metrics\n",
    "\n",
    "Implement domain-specific evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create metrics for code generation (syntax, correctness, style)\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You can build test datasets\n",
    "- [ ] You implemented automated evaluation\n",
    "- [ ] You used LLM-as-Judge\n",
    "- [ ] You understand regression testing\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll cover **Production Deployment** - taking AI apps to production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
