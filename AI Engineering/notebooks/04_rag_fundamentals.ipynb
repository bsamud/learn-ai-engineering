{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - RAG Fundamentals\n",
    "\n",
    "**Build your first Retrieval-Augmented Generation system!**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand the RAG architecture\n",
    "- Load and chunk documents\n",
    "- Create embeddings and store in vector DB\n",
    "- Implement retrieval and generation\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [What is RAG?](#what-is-rag)\n",
    "2. [Document Loading](#loading)\n",
    "3. [Chunking Strategies](#chunking)\n",
    "4. [Vector Storage](#vector)\n",
    "5. [Retrieval & Generation](#retrieval)\n",
    "6. [Complete Pipeline](#pipeline)\n",
    "7. [Exercises](#exercises)\n",
    "8. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. What is RAG? <a id='what-is-rag'></a>\n",
    "\n",
    "**RAG = Retrieval-Augmented Generation**\n",
    "\n",
    "```\n",
    "┌─────────────┐     ┌─────────────┐     ┌─────────────┐\n",
    "│   Query     │────▶│  Retrieve   │────▶│  Generate   │\n",
    "│             │     │  Relevant   │     │  Answer     │\n",
    "│             │     │  Context    │     │             │\n",
    "└─────────────┘     └─────────────┘     └─────────────┘\n",
    "                           │\n",
    "                    ┌──────┴──────┐\n",
    "                    │  Vector DB  │\n",
    "                    │  (indexed   │\n",
    "                    │  documents) │\n",
    "                    └─────────────┘\n",
    "```\n",
    "\n",
    "### Why RAG?\n",
    "\n",
    "- **Current Information**: Access up-to-date data\n",
    "- **Source Citation**: Know where answers come from\n",
    "- **Domain Knowledge**: Add your own documents\n",
    "- **Reduced Hallucination**: Ground answers in facts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Document Loading <a id='loading'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Load documents using our loader\n",
    "from src.rag_pipeline import DocumentLoader, Document\n",
    "\n",
    "loader = DocumentLoader()\n",
    "\n",
    "# Create sample document\n",
    "sample_text = \"\"\"\n",
    "# Introduction to RAG\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) is a technique that combines retrieval \n",
    "and generation for better AI responses.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. Document Store: Where your documents are indexed\n",
    "2. Embeddings: Vector representations of text\n",
    "3. Retrieval: Finding relevant documents\n",
    "4. Generation: Creating the final answer\n",
    "\n",
    "## Benefits\n",
    "\n",
    "- More accurate answers\n",
    "- Traceable sources\n",
    "- Up-to-date information\n",
    "- Domain-specific knowledge\n",
    "\"\"\"\n",
    "\n",
    "# Save sample document\n",
    "sample_path = Path(\"../data/documents/sample.txt\")\n",
    "sample_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "sample_path.write_text(sample_text)\n",
    "\n",
    "# Load it\n",
    "docs = loader.load_file(str(sample_path))\n",
    "print(f\"Loaded {len(docs)} document(s)\")\n",
    "print(f\"Content preview: {docs[0].content[:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Chunking Strategies <a id='chunking'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Chunk documents\n",
    "from src.rag_pipeline import Chunker\n",
    "\n",
    "# Create chunker\n",
    "chunker = Chunker(\n",
    "    chunk_size=200,  # Max characters per chunk\n",
    "    overlap=50       # Overlap between chunks\n",
    ")\n",
    "\n",
    "# Chunk the document\n",
    "chunks = chunker.chunk_all(docs)\n",
    "\n",
    "print(f\"Created {len(chunks)} chunks:\")\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk.content)} chars):\")\n",
    "    print(f\"  {chunk.content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Vector Storage <a id='vector'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Create embeddings and store\n",
    "from src.embedding_utils import EmbeddingModel, SimpleVectorStore\n",
    "\n",
    "# Create embedding model\n",
    "embedder = EmbeddingModel(provider=\"openai\", model=\"text-embedding-3-small\")\n",
    "\n",
    "# Create vector store\n",
    "store = SimpleVectorStore(embedding_model=embedder)\n",
    "\n",
    "# Add chunks\n",
    "store.add(\n",
    "    texts=[c.content for c in chunks],\n",
    "    metadata=[c.metadata for c in chunks]\n",
    ")\n",
    "\n",
    "print(f\"Vector store contains {len(store)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Retrieval & Generation <a id='retrieval'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Search and retrieve\n",
    "query = \"What are the benefits of RAG?\"\n",
    "\n",
    "results = store.search(query, k=3)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"\\nTop {len(results)} results:\")\n",
    "for i, r in enumerate(results):\n",
    "    print(f\"\\n{i+1}. Score: {r['score']:.3f}\")\n",
    "    print(f\"   {r['text'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Generate answer with context\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "llm = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "# Build context from retrieved documents\n",
    "context = \"\\n\\n\".join([r[\"text\"] for r in results])\n",
    "\n",
    "# Create prompt\n",
    "prompt = f\"\"\"Use the following context to answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "answer = llm.chat(prompt)\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Complete Pipeline <a id='pipeline'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Use our RAG pipeline class\n",
    "from src.rag_pipeline import RAGPipeline\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "# Create pipeline\n",
    "llm = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "rag = RAGPipeline(\n",
    "    llm_client=llm,\n",
    "    chunk_size=200,\n",
    "    chunk_overlap=50,\n",
    "    collection_name=\"demo_rag\"\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "rag.load_documents(\"../data/documents/\")\n",
    "\n",
    "# Query\n",
    "result = rag.query(\"What are the key components of RAG?\")\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(result[\"answer\"])\n",
    "print(f\"\\nSources: {len(result['sources'])} documents used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Add Your Documents\n",
    "\n",
    "Add your own documents to the RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add your own documents and query them\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Experiment with Chunk Size\n",
    "\n",
    "Try different chunk sizes and see how it affects retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare chunk sizes 100, 500, 1000\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You understand the RAG pipeline\n",
    "- [ ] You can load and chunk documents\n",
    "- [ ] You created a vector store\n",
    "- [ ] You can query and get answers\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **Advanced RAG** - hybrid search, re-ranking, and evaluation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
