{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 09 - Production Deployment\n",
    "\n",
    "**Deploy AI applications to production.**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Design production-ready APIs\n",
    "- Implement caching and optimization\n",
    "- Add monitoring and observability\n",
    "- Manage costs effectively\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [API Design](#api)\n",
    "2. [Caching](#caching)\n",
    "3. [Rate Limiting](#ratelimit)\n",
    "4. [Monitoring](#monitoring)\n",
    "5. [Cost Management](#costs)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. API Design <a id='api'></a>\n",
    "\n",
    "### Best Practices for AI APIs:\n",
    "- **Streaming**: Return partial results for better UX\n",
    "- **Async**: Don't block on long operations\n",
    "- **Versioning**: Support multiple API versions\n",
    "- **Error handling**: Clear, actionable error messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: FastAPI example for AI endpoint\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "\n",
    "# Request/Response models\n",
    "class ChatRequest(BaseModel):\n",
    "    message: str = Field(..., min_length=1, max_length=10000)\n",
    "    system_prompt: Optional[str] = None\n",
    "    temperature: float = Field(default=0.7, ge=0, le=2)\n",
    "    max_tokens: int = Field(default=1000, ge=1, le=4000)\n",
    "\n",
    "class ChatResponse(BaseModel):\n",
    "    message: str\n",
    "    tokens_used: int\n",
    "    processing_time_ms: float\n",
    "    request_id: str\n",
    "\n",
    "class ErrorResponse(BaseModel):\n",
    "    error: str\n",
    "    error_code: str\n",
    "    request_id: str\n",
    "\n",
    "# Example validation\n",
    "request = ChatRequest(\n",
    "    message=\"Hello, how are you?\",\n",
    "    temperature=0.8\n",
    ")\n",
    "print(f\"Valid request: {request.model_dump()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: FastAPI application structure\n",
    "api_code = '''\n",
    "from fastapi import FastAPI, HTTPException, Request\n",
    "from fastapi.responses import StreamingResponse\n",
    "import uuid\n",
    "import time\n",
    "\n",
    "app = FastAPI(title=\"AI Chat API\", version=\"1.0.0\")\n",
    "\n",
    "@app.post(\"/v1/chat\", response_model=ChatResponse)\n",
    "async def chat(request: ChatRequest):\n",
    "    \"\"\"Send a message and get a response.\"\"\"\n",
    "    request_id = str(uuid.uuid4())\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        # Call LLM\n",
    "        from src.llm_utils import LLMClient\n",
    "        client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "        \n",
    "        response = client.chat(\n",
    "            message=request.message,\n",
    "            system=request.system_prompt\n",
    "        )\n",
    "        \n",
    "        processing_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        return ChatResponse(\n",
    "            message=response,\n",
    "            tokens_used=client.get_stats().total_input_tokens + client.get_stats().total_output_tokens,\n",
    "            processing_time_ms=processing_time,\n",
    "            request_id=request_id\n",
    "        )\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise HTTPException(\n",
    "            status_code=500,\n",
    "            detail={\"error\": str(e), \"error_code\": \"LLM_ERROR\", \"request_id\": request_id}\n",
    "        )\n",
    "\n",
    "@app.post(\"/v1/chat/stream\")\n",
    "async def chat_stream(request: ChatRequest):\n",
    "    \"\"\"Stream a response token by token.\"\"\"\n",
    "    async def generate():\n",
    "        from src.llm_utils import LLMClient\n",
    "        client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "        \n",
    "        for chunk in client.stream(request.message, system=request.system_prompt):\n",
    "            yield f\"data: {chunk}\\\\n\\\\n\"\n",
    "        yield \"data: [DONE]\\\\n\\\\n\"\n",
    "    \n",
    "    return StreamingResponse(generate(), media_type=\"text/event-stream\")\n",
    "'''\n",
    "\n",
    "print(\"FastAPI application structure:\")\n",
    "print(api_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Caching <a id='caching'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Implement semantic caching\n",
    "import hashlib\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class CacheEntry:\n",
    "    key: str\n",
    "    value: str\n",
    "    created_at: float\n",
    "    ttl: float  # Time to live in seconds\n",
    "    \n",
    "    def is_expired(self) -> bool:\n",
    "        return time.time() > self.created_at + self.ttl\n",
    "\n",
    "class SimpleCache:\n",
    "    \"\"\"Simple in-memory cache with TTL.\"\"\"\n",
    "    \n",
    "    def __init__(self, default_ttl: float = 3600):\n",
    "        self.cache: dict[str, CacheEntry] = {}\n",
    "        self.default_ttl = default_ttl\n",
    "        self.hits = 0\n",
    "        self.misses = 0\n",
    "    \n",
    "    def _hash_key(self, text: str) -> str:\n",
    "        \"\"\"Create a hash key from text.\"\"\"\n",
    "        return hashlib.sha256(text.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def get(self, key: str) -> Optional[str]:\n",
    "        \"\"\"Get value from cache.\"\"\"\n",
    "        hash_key = self._hash_key(key)\n",
    "        \n",
    "        if hash_key in self.cache:\n",
    "            entry = self.cache[hash_key]\n",
    "            if not entry.is_expired():\n",
    "                self.hits += 1\n",
    "                return entry.value\n",
    "            else:\n",
    "                del self.cache[hash_key]\n",
    "        \n",
    "        self.misses += 1\n",
    "        return None\n",
    "    \n",
    "    def set(self, key: str, value: str, ttl: Optional[float] = None):\n",
    "        \"\"\"Set value in cache.\"\"\"\n",
    "        hash_key = self._hash_key(key)\n",
    "        self.cache[hash_key] = CacheEntry(\n",
    "            key=hash_key,\n",
    "            value=value,\n",
    "            created_at=time.time(),\n",
    "            ttl=ttl or self.default_ttl\n",
    "        )\n",
    "    \n",
    "    def stats(self) -> dict:\n",
    "        \"\"\"Get cache statistics.\"\"\"\n",
    "        total = self.hits + self.misses\n",
    "        hit_rate = self.hits / total if total > 0 else 0\n",
    "        return {\n",
    "            \"hits\": self.hits,\n",
    "            \"misses\": self.misses,\n",
    "            \"hit_rate\": hit_rate,\n",
    "            \"size\": len(self.cache)\n",
    "        }\n",
    "\n",
    "# Test the cache\n",
    "cache = SimpleCache(default_ttl=60)\n",
    "\n",
    "cache.set(\"What is Python?\", \"Python is a programming language.\")\n",
    "print(f\"Cached value: {cache.get('What is Python?')}\")\n",
    "print(f\"Cache miss: {cache.get('What is Java?')}\")\n",
    "print(f\"Stats: {cache.stats()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Cached LLM client\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "class CachedLLMClient:\n",
    "    \"\"\"LLM client with caching.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient, cache: SimpleCache):\n",
    "        self.client = client\n",
    "        self.cache = cache\n",
    "    \n",
    "    def chat(self, message: str, system: Optional[str] = None) -> str:\n",
    "        # Create cache key from message + system\n",
    "        cache_key = f\"{system or ''}::{message}\"\n",
    "        \n",
    "        # Check cache\n",
    "        cached = self.cache.get(cache_key)\n",
    "        if cached:\n",
    "            print(\"[CACHE HIT]\")\n",
    "            return cached\n",
    "        \n",
    "        # Call LLM\n",
    "        print(\"[CACHE MISS - calling LLM]\")\n",
    "        response = self.client.chat(message, system=system)\n",
    "        \n",
    "        # Cache result\n",
    "        self.cache.set(cache_key, response)\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Test cached client\n",
    "base_client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "cached_client = CachedLLMClient(base_client, cache)\n",
    "\n",
    "# First call - cache miss\n",
    "response1 = cached_client.chat(\"What is 2+2?\")\n",
    "print(f\"Response: {response1}\\n\")\n",
    "\n",
    "# Second call - cache hit\n",
    "response2 = cached_client.chat(\"What is 2+2?\")\n",
    "print(f\"Response: {response2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Rate Limiting <a id='ratelimit'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Token bucket rate limiter\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class RateLimiter:\n",
    "    \"\"\"Token bucket rate limiter.\"\"\"\n",
    "    \n",
    "    requests_per_minute: int\n",
    "    tokens: float = 0\n",
    "    last_update: float = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        self.tokens = float(self.requests_per_minute)\n",
    "        self.last_update = time.time()\n",
    "    \n",
    "    def _refill(self):\n",
    "        \"\"\"Refill tokens based on elapsed time.\"\"\"\n",
    "        now = time.time()\n",
    "        elapsed = now - self.last_update\n",
    "        self.tokens = min(\n",
    "            self.requests_per_minute,\n",
    "            self.tokens + (elapsed * self.requests_per_minute / 60)\n",
    "        )\n",
    "        self.last_update = now\n",
    "    \n",
    "    def acquire(self, tokens: int = 1) -> bool:\n",
    "        \"\"\"Try to acquire tokens. Returns True if allowed.\"\"\"\n",
    "        self._refill()\n",
    "        \n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def wait_and_acquire(self, tokens: int = 1, timeout: float = 30) -> bool:\n",
    "        \"\"\"Wait for tokens to become available.\"\"\"\n",
    "        start = time.time()\n",
    "        \n",
    "        while time.time() - start < timeout:\n",
    "            if self.acquire(tokens):\n",
    "                return True\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        return False\n",
    "\n",
    "# Test rate limiter\n",
    "limiter = RateLimiter(requests_per_minute=10)\n",
    "\n",
    "for i in range(12):\n",
    "    if limiter.acquire():\n",
    "        print(f\"Request {i+1}: Allowed\")\n",
    "    else:\n",
    "        print(f\"Request {i+1}: Rate limited!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Monitoring <a id='monitoring'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Simple metrics tracking\n",
    "from dataclasses import dataclass, field\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "import statistics\n",
    "\n",
    "@dataclass\n",
    "class MetricsCollector:\n",
    "    \"\"\"Collect and report metrics.\"\"\"\n",
    "    \n",
    "    counters: dict = field(default_factory=lambda: defaultdict(int))\n",
    "    gauges: dict = field(default_factory=dict)\n",
    "    histograms: dict = field(default_factory=lambda: defaultdict(list))\n",
    "    \n",
    "    def increment(self, name: str, value: int = 1):\n",
    "        \"\"\"Increment a counter.\"\"\"\n",
    "        self.counters[name] += value\n",
    "    \n",
    "    def gauge(self, name: str, value: float):\n",
    "        \"\"\"Set a gauge value.\"\"\"\n",
    "        self.gauges[name] = value\n",
    "    \n",
    "    def histogram(self, name: str, value: float):\n",
    "        \"\"\"Record a histogram value.\"\"\"\n",
    "        self.histograms[name].append(value)\n",
    "    \n",
    "    def report(self) -> dict:\n",
    "        \"\"\"Generate metrics report.\"\"\"\n",
    "        report = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"counters\": dict(self.counters),\n",
    "            \"gauges\": dict(self.gauges),\n",
    "            \"histograms\": {}\n",
    "        }\n",
    "        \n",
    "        for name, values in self.histograms.items():\n",
    "            if values:\n",
    "                report[\"histograms\"][name] = {\n",
    "                    \"count\": len(values),\n",
    "                    \"min\": min(values),\n",
    "                    \"max\": max(values),\n",
    "                    \"mean\": statistics.mean(values),\n",
    "                    \"p50\": statistics.median(values),\n",
    "                    \"p95\": statistics.quantiles(values, n=20)[18] if len(values) >= 20 else max(values),\n",
    "                }\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Test metrics\n",
    "metrics = MetricsCollector()\n",
    "\n",
    "# Simulate some requests\n",
    "import random\n",
    "for i in range(100):\n",
    "    metrics.increment(\"requests_total\")\n",
    "    metrics.histogram(\"response_time_ms\", random.gauss(200, 50))\n",
    "    if random.random() < 0.05:\n",
    "        metrics.increment(\"errors_total\")\n",
    "\n",
    "metrics.gauge(\"cache_size\", 150)\n",
    "metrics.gauge(\"active_connections\", 10)\n",
    "\n",
    "report = metrics.report()\n",
    "print(json.dumps(report, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Instrumented LLM client\n",
    "class InstrumentedLLMClient:\n",
    "    \"\"\"LLM client with metrics.\"\"\"\n",
    "    \n",
    "    def __init__(self, client: LLMClient, metrics: MetricsCollector):\n",
    "        self.client = client\n",
    "        self.metrics = metrics\n",
    "    \n",
    "    def chat(self, message: str, system: Optional[str] = None) -> str:\n",
    "        self.metrics.increment(\"llm_requests_total\")\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            response = self.client.chat(message, system=system)\n",
    "            \n",
    "            # Record metrics\n",
    "            latency = (time.time() - start) * 1000\n",
    "            self.metrics.histogram(\"llm_latency_ms\", latency)\n",
    "            \n",
    "            stats = self.client.get_stats()\n",
    "            self.metrics.gauge(\"llm_total_tokens\", stats.total_input_tokens + stats.total_output_tokens)\n",
    "            self.metrics.gauge(\"llm_total_cost\", stats.total_cost)\n",
    "            \n",
    "            return response\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.metrics.increment(\"llm_errors_total\")\n",
    "            raise\n",
    "\n",
    "# Test instrumented client\n",
    "metrics = MetricsCollector()\n",
    "base_client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "instrumented = InstrumentedLLMClient(base_client, metrics)\n",
    "\n",
    "# Make some calls\n",
    "for q in [\"What is 1+1?\", \"What is 2+2?\", \"What is 3+3?\"]:\n",
    "    instrumented.chat(q)\n",
    "\n",
    "print(\"Metrics after 3 calls:\")\n",
    "print(json.dumps(metrics.report(), indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cost Management <a id='costs'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Cost tracking and budgets\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "@dataclass\n",
    "class Budget:\n",
    "    \"\"\"Track spending against a budget.\"\"\"\n",
    "    \n",
    "    daily_limit: float\n",
    "    monthly_limit: float\n",
    "    daily_spent: float = 0\n",
    "    monthly_spent: float = 0\n",
    "    last_daily_reset: datetime = field(default_factory=datetime.now)\n",
    "    last_monthly_reset: datetime = field(default_factory=datetime.now)\n",
    "    \n",
    "    def _maybe_reset(self):\n",
    "        \"\"\"Reset counters if needed.\"\"\"\n",
    "        now = datetime.now()\n",
    "        \n",
    "        # Reset daily\n",
    "        if now.date() > self.last_daily_reset.date():\n",
    "            self.daily_spent = 0\n",
    "            self.last_daily_reset = now\n",
    "        \n",
    "        # Reset monthly\n",
    "        if now.month != self.last_monthly_reset.month:\n",
    "            self.monthly_spent = 0\n",
    "            self.last_monthly_reset = now\n",
    "    \n",
    "    def can_spend(self, amount: float) -> bool:\n",
    "        \"\"\"Check if spending is within budget.\"\"\"\n",
    "        self._maybe_reset()\n",
    "        \n",
    "        if self.daily_spent + amount > self.daily_limit:\n",
    "            return False\n",
    "        if self.monthly_spent + amount > self.monthly_limit:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def record_spend(self, amount: float):\n",
    "        \"\"\"Record spending.\"\"\"\n",
    "        self._maybe_reset()\n",
    "        self.daily_spent += amount\n",
    "        self.monthly_spent += amount\n",
    "    \n",
    "    def status(self) -> dict:\n",
    "        \"\"\"Get budget status.\"\"\"\n",
    "        self._maybe_reset()\n",
    "        return {\n",
    "            \"daily\": {\n",
    "                \"spent\": self.daily_spent,\n",
    "                \"limit\": self.daily_limit,\n",
    "                \"remaining\": self.daily_limit - self.daily_spent,\n",
    "                \"percent_used\": (self.daily_spent / self.daily_limit) * 100\n",
    "            },\n",
    "            \"monthly\": {\n",
    "                \"spent\": self.monthly_spent,\n",
    "                \"limit\": self.monthly_limit,\n",
    "                \"remaining\": self.monthly_limit - self.monthly_spent,\n",
    "                \"percent_used\": (self.monthly_spent / self.monthly_limit) * 100\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Test budget\n",
    "budget = Budget(daily_limit=10.0, monthly_limit=100.0)\n",
    "\n",
    "# Simulate spending\n",
    "costs = [0.01, 0.02, 0.015, 0.03]\n",
    "for cost in costs:\n",
    "    if budget.can_spend(cost):\n",
    "        budget.record_spend(cost)\n",
    "        print(f\"Spent ${cost:.3f}\")\n",
    "    else:\n",
    "        print(f\"Budget exceeded for ${cost:.3f}\")\n",
    "\n",
    "print(f\"\\nBudget status: {json.dumps(budget.status(), indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Cost-aware routing\n",
    "from src.llm_utils import estimate_cost\n",
    "\n",
    "class CostAwareRouter:\n",
    "    \"\"\"Route requests to different models based on complexity and budget.\"\"\"\n",
    "    \n",
    "    def __init__(self, budget: Budget):\n",
    "        self.budget = budget\n",
    "        self.models = {\n",
    "            \"cheap\": \"gpt-4o-mini\",\n",
    "            \"expensive\": \"gpt-4o\"\n",
    "        }\n",
    "    \n",
    "    def route(self, message: str, prefer_quality: bool = False) -> str:\n",
    "        \"\"\"Select appropriate model.\"\"\"\n",
    "        # Estimate tokens (rough)\n",
    "        estimated_tokens = len(message.split()) * 1.5\n",
    "        \n",
    "        # Check budget\n",
    "        status = self.budget.status()\n",
    "        budget_remaining_pct = 100 - status[\"daily\"][\"percent_used\"]\n",
    "        \n",
    "        # Decision logic\n",
    "        if budget_remaining_pct < 20:\n",
    "            return self.models[\"cheap\"]\n",
    "        \n",
    "        if prefer_quality and budget_remaining_pct > 50:\n",
    "            return self.models[\"expensive\"]\n",
    "        \n",
    "        return self.models[\"cheap\"]\n",
    "\n",
    "# Test router\n",
    "router = CostAwareRouter(budget)\n",
    "\n",
    "print(f\"Normal request: {router.route('Simple question')}\")\n",
    "print(f\"Quality request: {router.route('Complex question', prefer_quality=True)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Build Complete API\n",
    "\n",
    "Create a production-ready API with all components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Combine caching, rate limiting, metrics, and budget into one API\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Alerting System\n",
    "\n",
    "Build an alerting system for errors and budget thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create alerts for high error rates, budget warnings, latency spikes\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You can design production APIs\n",
    "- [ ] You implemented caching\n",
    "- [ ] You understand rate limiting\n",
    "- [ ] You can track metrics and costs\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the final notebook, we'll build a **Complete AI Application** as a capstone project!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
