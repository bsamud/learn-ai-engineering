{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Working with LLM APIs\n",
    "\n",
    "**Build production-ready LLM integrations** with multiple providers.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Set up and use multiple LLM providers\n",
    "- Handle streaming responses\n",
    "- Implement rate limiting and error handling\n",
    "- Track costs and usage\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Setup & Configuration](#setup)\n",
    "2. [OpenAI API](#openai)\n",
    "3. [Anthropic API](#anthropic)\n",
    "4. [Streaming Responses](#streaming)\n",
    "5. [Cost Tracking](#cost)\n",
    "6. [Error Handling](#errors)\n",
    "7. [Exercises](#exercises)\n",
    "8. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"OpenAI key: {'found' if os.getenv('OPENAI_API_KEY') else 'not found'}\")\n",
    "print(f\"Anthropic key: {'found' if os.getenv('ANTHROPIC_API_KEY') else 'not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup & Configuration <a id='setup'></a>\n",
    "\n",
    "Create a `.env` file in the `AI Engineering/` folder:\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=sk-your-key-here\n",
    "ANTHROPIC_API_KEY=sk-ant-your-key-here\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. OpenAI API <a id='openai'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Basic OpenAI usage\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    temperature=0.7,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(f\"\\nTokens used: {response.usage.total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Anthropic API <a id='anthropic'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Basic Anthropic usage\n",
    "from anthropic import Anthropic\n",
    "\n",
    "client = Anthropic()\n",
    "\n",
    "response = client.messages.create(\n",
    "    model=\"claude-3-5-sonnet-20241022\",\n",
    "    max_tokens=100,\n",
    "    system=\"You are a helpful assistant.\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Response:\", response.content[0].text)\n",
    "print(f\"\\nTokens: {response.usage.input_tokens} in, {response.usage.output_tokens} out\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Streaming Responses <a id='streaming'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Streaming with OpenAI\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "print(\"Streaming response:\")\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Write a haiku about coding.\"}],\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\\nDone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Cost Tracking <a id='cost'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Use our LLM client with cost tracking\n",
    "from src.llm_utils import LLMClient, estimate_cost\n",
    "\n",
    "client = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "# Make some requests\n",
    "for i in range(3):\n",
    "    response = client.chat(f\"Tell me fact #{i+1} about AI.\")\n",
    "    print(f\"Fact {i+1}: {response[:100]}...\")\n",
    "\n",
    "# Check usage\n",
    "stats = client.get_stats()\n",
    "print(f\"\\nUsage: {stats.summary()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Error Handling <a id='errors'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Robust error handling\n",
    "from openai import OpenAI, RateLimitError, APIError\n",
    "import time\n",
    "\n",
    "def robust_completion(messages, max_retries=3):\n",
    "    \"\"\"Make a completion with retry logic.\"\"\"\n",
    "    client = OpenAI()\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=messages\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "            \n",
    "        except RateLimitError:\n",
    "            wait = 2 ** attempt  # Exponential backoff\n",
    "            print(f\"Rate limited. Waiting {wait}s...\")\n",
    "            time.sleep(wait)\n",
    "            \n",
    "        except APIError as e:\n",
    "            print(f\"API error: {e}\")\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "    \n",
    "    raise Exception(\"Max retries exceeded\")\n",
    "\n",
    "# Test it\n",
    "result = robust_completion([{\"role\": \"user\", \"content\": \"Hello!\"}])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Multi-Provider Client\n",
    "\n",
    "Create a function that tries OpenAI first, falls back to Anthropic if it fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement fallback logic\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Cost Calculator\n",
    "\n",
    "Create a function that estimates the cost before making a request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Estimate cost before request\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You can use both OpenAI and Anthropic APIs\n",
    "- [ ] You understand streaming responses\n",
    "- [ ] You can track costs and usage\n",
    "- [ ] You implemented error handling\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll learn about **Structured Outputs** - getting reliable JSON and data from LLMs!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
