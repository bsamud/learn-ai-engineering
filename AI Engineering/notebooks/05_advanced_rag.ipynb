{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05 - Advanced RAG Techniques\n",
    "\n",
    "**Production-grade retrieval and generation.**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Implement advanced chunking strategies\n",
    "- Use hybrid search (BM25 + vectors)\n",
    "- Apply re-ranking for better results\n",
    "- Evaluate RAG performance\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Chunking Strategies](#chunking)\n",
    "2. [Hybrid Search](#hybrid)\n",
    "3. [Re-ranking](#reranking)\n",
    "4. [Query Transformation](#query)\n",
    "5. [Evaluation Metrics](#evaluation)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Chunking Strategies <a id='chunking'></a>\n",
    "\n",
    "How you split documents affects retrieval quality:\n",
    "\n",
    "```\n",
    "Strategy          Best For                    Trade-offs\n",
    "────────────────────────────────────────────────────────\n",
    "Fixed size        Simple, predictable         May break context\n",
    "Recursive         Preserves structure         More complex\n",
    "Semantic          Meaning-aware               Slower, needs model\n",
    "Sentence          Natural boundaries          Variable sizes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Compare chunking strategies\n",
    "sample_text = \"\"\"\n",
    "# Introduction to Machine Learning\n",
    "\n",
    "Machine learning is a subset of artificial intelligence that enables computers to learn from data.\n",
    "\n",
    "## Types of Machine Learning\n",
    "\n",
    "### Supervised Learning\n",
    "In supervised learning, the algorithm learns from labeled training data. Examples include:\n",
    "- Classification: Predicting categories (spam/not spam)\n",
    "- Regression: Predicting continuous values (house prices)\n",
    "\n",
    "### Unsupervised Learning\n",
    "Unsupervised learning finds patterns in unlabeled data. Common techniques:\n",
    "- Clustering: Grouping similar data points\n",
    "- Dimensionality reduction: Simplifying data\n",
    "\n",
    "### Reinforcement Learning\n",
    "The agent learns by interacting with an environment and receiving rewards or penalties.\n",
    "\n",
    "## Applications\n",
    "\n",
    "Machine learning powers many modern applications:\n",
    "- Image recognition\n",
    "- Natural language processing\n",
    "- Recommendation systems\n",
    "- Autonomous vehicles\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Total length: {len(sample_text)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Fixed-size chunking\n",
    "def chunk_fixed(text: str, chunk_size: int = 200, overlap: int = 50) -> list[str]:\n",
    "    \"\"\"Simple fixed-size chunking with overlap.\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "        if chunk.strip():\n",
    "            chunks.append(chunk.strip())\n",
    "        start = end - overlap\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "fixed_chunks = chunk_fixed(sample_text, chunk_size=300, overlap=50)\n",
    "print(f\"Fixed chunking: {len(fixed_chunks)} chunks\")\n",
    "for i, chunk in enumerate(fixed_chunks[:2]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Recursive chunking (respects structure)\n",
    "def chunk_recursive(\n",
    "    text: str,\n",
    "    chunk_size: int = 300,\n",
    "    separators: list[str] = None\n",
    ") -> list[str]:\n",
    "    \"\"\"Recursively split text, trying to preserve structure.\"\"\"\n",
    "    separators = separators or [\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    \n",
    "    if len(text) <= chunk_size:\n",
    "        return [text.strip()] if text.strip() else []\n",
    "    \n",
    "    # Try each separator\n",
    "    for sep in separators:\n",
    "        if sep in text:\n",
    "            parts = text.split(sep)\n",
    "            chunks = []\n",
    "            current = \"\"\n",
    "            \n",
    "            for part in parts:\n",
    "                if len(current) + len(part) + len(sep) <= chunk_size:\n",
    "                    current += (sep if current else \"\") + part\n",
    "                else:\n",
    "                    if current:\n",
    "                        chunks.append(current.strip())\n",
    "                    current = part\n",
    "            \n",
    "            if current:\n",
    "                chunks.append(current.strip())\n",
    "            \n",
    "            # Recursively split any chunks that are still too large\n",
    "            final_chunks = []\n",
    "            for chunk in chunks:\n",
    "                if len(chunk) > chunk_size:\n",
    "                    final_chunks.extend(chunk_recursive(chunk, chunk_size, separators[1:]))\n",
    "                else:\n",
    "                    final_chunks.append(chunk)\n",
    "            \n",
    "            return [c for c in final_chunks if c]\n",
    "    \n",
    "    # Fallback: split at chunk_size\n",
    "    return chunk_fixed(text, chunk_size, overlap=0)\n",
    "\n",
    "recursive_chunks = chunk_recursive(sample_text, chunk_size=300)\n",
    "print(f\"Recursive chunking: {len(recursive_chunks)} chunks\")\n",
    "for i, chunk in enumerate(recursive_chunks[:3]):\n",
    "    print(f\"\\nChunk {i+1} ({len(chunk)} chars):\")\n",
    "    print(f\"  {chunk[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Use our Chunker class\n",
    "from src.rag_pipeline import Chunker, Document\n",
    "\n",
    "doc = Document(content=sample_text, source=\"sample.md\")\n",
    "\n",
    "# Compare different settings\n",
    "for size, overlap in [(200, 50), (500, 100), (1000, 200)]:\n",
    "    chunker = Chunker(chunk_size=size, overlap=overlap)\n",
    "    chunks = chunker.chunk(doc)\n",
    "    print(f\"Size {size}, Overlap {overlap}: {len(chunks)} chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Hybrid Search <a id='hybrid'></a>\n",
    "\n",
    "Combine keyword (BM25) and semantic (vector) search for better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Simple BM25 implementation\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "class SimpleBM25:\n",
    "    \"\"\"Basic BM25 implementation.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: list[str], k1: float = 1.5, b: float = 0.75):\n",
    "        self.documents = documents\n",
    "        self.k1 = k1\n",
    "        self.b = b\n",
    "        \n",
    "        # Tokenize\n",
    "        self.doc_tokens = [doc.lower().split() for doc in documents]\n",
    "        self.doc_lengths = [len(tokens) for tokens in self.doc_tokens]\n",
    "        self.avg_length = sum(self.doc_lengths) / len(self.doc_lengths)\n",
    "        \n",
    "        # Calculate IDF\n",
    "        self.idf = {}\n",
    "        n_docs = len(documents)\n",
    "        all_terms = set()\n",
    "        for tokens in self.doc_tokens:\n",
    "            all_terms.update(tokens)\n",
    "        \n",
    "        for term in all_terms:\n",
    "            doc_freq = sum(1 for tokens in self.doc_tokens if term in tokens)\n",
    "            self.idf[term] = math.log((n_docs - doc_freq + 0.5) / (doc_freq + 0.5) + 1)\n",
    "    \n",
    "    def score(self, query: str) -> list[tuple[int, float]]:\n",
    "        \"\"\"Score all documents for a query.\"\"\"\n",
    "        query_tokens = query.lower().split()\n",
    "        scores = []\n",
    "        \n",
    "        for i, (tokens, length) in enumerate(zip(self.doc_tokens, self.doc_lengths)):\n",
    "            score = 0\n",
    "            token_counts = Counter(tokens)\n",
    "            \n",
    "            for term in query_tokens:\n",
    "                if term in token_counts:\n",
    "                    tf = token_counts[term]\n",
    "                    idf = self.idf.get(term, 0)\n",
    "                    \n",
    "                    # BM25 formula\n",
    "                    numerator = tf * (self.k1 + 1)\n",
    "                    denominator = tf + self.k1 * (1 - self.b + self.b * length / self.avg_length)\n",
    "                    score += idf * numerator / denominator\n",
    "            \n",
    "            scores.append((i, score))\n",
    "        \n",
    "        return sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Test BM25\n",
    "docs = [\n",
    "    \"Python is a programming language for data science\",\n",
    "    \"Machine learning algorithms learn from data\",\n",
    "    \"Data visualization helps understand patterns\",\n",
    "    \"Python and JavaScript are popular languages\"\n",
    "]\n",
    "\n",
    "bm25 = SimpleBM25(docs)\n",
    "results = bm25.score(\"Python programming\")\n",
    "\n",
    "print(\"BM25 Results for 'Python programming':\")\n",
    "for idx, score in results:\n",
    "    print(f\"  {score:.3f}: {docs[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Hybrid search combining BM25 and vectors\n",
    "from src.embedding_utils import EmbeddingModel, cosine_similarity\n",
    "\n",
    "class HybridSearch:\n",
    "    \"\"\"Combine BM25 and vector search.\"\"\"\n",
    "    \n",
    "    def __init__(self, documents: list[str], embedder: EmbeddingModel):\n",
    "        self.documents = documents\n",
    "        self.embedder = embedder\n",
    "        \n",
    "        # Initialize BM25\n",
    "        self.bm25 = SimpleBM25(documents)\n",
    "        \n",
    "        # Create embeddings\n",
    "        self.embeddings = embedder.embed_batch(documents)\n",
    "    \n",
    "    def search(\n",
    "        self, \n",
    "        query: str, \n",
    "        k: int = 5,\n",
    "        alpha: float = 0.5  # Weight for vector search (0=BM25 only, 1=vector only)\n",
    "    ) -> list[tuple[str, float]]:\n",
    "        \"\"\"Hybrid search with weighted combination.\"\"\"\n",
    "        # BM25 scores\n",
    "        bm25_results = self.bm25.score(query)\n",
    "        bm25_scores = {idx: score for idx, score in bm25_results}\n",
    "        \n",
    "        # Normalize BM25 scores\n",
    "        max_bm25 = max(bm25_scores.values()) if bm25_scores.values() else 1\n",
    "        bm25_norm = {idx: score / max_bm25 for idx, score in bm25_scores.items()}\n",
    "        \n",
    "        # Vector scores\n",
    "        query_embedding = self.embedder.embed(query)\n",
    "        vector_scores = {}\n",
    "        for i, emb in enumerate(self.embeddings):\n",
    "            vector_scores[i] = cosine_similarity(query_embedding, emb)\n",
    "        \n",
    "        # Combine scores\n",
    "        combined = []\n",
    "        for i in range(len(self.documents)):\n",
    "            bm25_s = bm25_norm.get(i, 0)\n",
    "            vec_s = vector_scores.get(i, 0)\n",
    "            final_score = alpha * vec_s + (1 - alpha) * bm25_s\n",
    "            combined.append((self.documents[i], final_score))\n",
    "        \n",
    "        return sorted(combined, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# Test hybrid search\n",
    "embedder = EmbeddingModel(provider=\"openai\", model=\"text-embedding-3-small\")\n",
    "hybrid = HybridSearch(docs, embedder)\n",
    "\n",
    "print(\"Hybrid Search Results (alpha=0.5):\")\n",
    "for doc, score in hybrid.search(\"Python programming\", alpha=0.5):\n",
    "    print(f\"  {score:.3f}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Re-ranking <a id='reranking'></a>\n",
    "\n",
    "Use a more powerful model to re-order initial results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: LLM-based re-ranking\n",
    "from src.llm_utils import LLMClient\n",
    "import json\n",
    "\n",
    "def rerank_with_llm(\n",
    "    query: str,\n",
    "    documents: list[str],\n",
    "    llm: LLMClient,\n",
    "    top_k: int = 3\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"Re-rank documents using an LLM.\"\"\"\n",
    "    # Format documents for the prompt\n",
    "    doc_list = \"\\n\".join([f\"{i+1}. {doc}\" for i, doc in enumerate(documents)])\n",
    "    \n",
    "    prompt = f\"\"\"Given the query and documents, rank the documents by relevance.\n",
    "Return JSON with 'rankings' array of objects with 'index' (1-based) and 'score' (0-1).\n",
    "\n",
    "Query: {query}\n",
    "\n",
    "Documents:\n",
    "{doc_list}\n",
    "\n",
    "Return only the top {top_k} most relevant. JSON:\"\"\"\n",
    "    \n",
    "    response = llm.chat(prompt)\n",
    "    \n",
    "    try:\n",
    "        # Parse JSON\n",
    "        start = response.find(\"{\")\n",
    "        end = response.rfind(\"}\") + 1\n",
    "        data = json.loads(response[start:end])\n",
    "        \n",
    "        results = []\n",
    "        for item in data.get(\"rankings\", []):\n",
    "            idx = item[\"index\"] - 1\n",
    "            if 0 <= idx < len(documents):\n",
    "                results.append((documents[idx], item[\"score\"]))\n",
    "        \n",
    "        return results\n",
    "    except:\n",
    "        # Fallback: return original order\n",
    "        return [(doc, 0.5) for doc in documents[:top_k]]\n",
    "\n",
    "# Test re-ranking\n",
    "llm = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "\n",
    "initial_docs = [\n",
    "    \"Python is used for web development with Django and Flask.\",\n",
    "    \"Machine learning in Python uses libraries like scikit-learn.\",\n",
    "    \"JavaScript runs in web browsers for frontend development.\",\n",
    "    \"Python's data science stack includes pandas and numpy.\"\n",
    "]\n",
    "\n",
    "reranked = rerank_with_llm(\n",
    "    \"Python for machine learning\",\n",
    "    initial_docs,\n",
    "    llm,\n",
    "    top_k=3\n",
    ")\n",
    "\n",
    "print(\"Re-ranked results:\")\n",
    "for doc, score in reranked:\n",
    "    print(f\"  {score:.2f}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Query Transformation <a id='query'></a>\n",
    "\n",
    "Improve retrieval by transforming the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Query expansion\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "def expand_query(query: str, llm: LLMClient) -> list[str]:\n",
    "    \"\"\"Generate multiple search queries from a single question.\"\"\"\n",
    "    prompt = f\"\"\"Generate 3 different search queries that could help answer this question.\n",
    "Make them diverse but relevant.\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Return as a JSON array of strings. JSON:\"\"\"\n",
    "    \n",
    "    response = llm.chat(prompt)\n",
    "    \n",
    "    try:\n",
    "        start = response.find(\"[\")\n",
    "        end = response.rfind(\"]\") + 1\n",
    "        queries = json.loads(response[start:end])\n",
    "        return [query] + queries  # Include original\n",
    "    except:\n",
    "        return [query]\n",
    "\n",
    "# Test\n",
    "llm = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "expanded = expand_query(\"How do neural networks learn?\", llm)\n",
    "\n",
    "print(\"Expanded queries:\")\n",
    "for i, q in enumerate(expanded):\n",
    "    print(f\"  {i+1}. {q}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Hypothetical Document Embedding (HyDE)\n",
    "def hyde_search(\n",
    "    query: str,\n",
    "    documents: list[str],\n",
    "    llm: LLMClient,\n",
    "    embedder: EmbeddingModel,\n",
    "    k: int = 3\n",
    ") -> list[tuple[str, float]]:\n",
    "    \"\"\"Generate hypothetical answer, then search for similar docs.\"\"\"\n",
    "    \n",
    "    # Generate hypothetical answer\n",
    "    prompt = f\"\"\"Write a short, factual answer to this question as if you were a document:\n",
    "\n",
    "Question: {query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    hypothetical = llm.chat(prompt)\n",
    "    print(f\"Hypothetical answer: {hypothetical[:100]}...\")\n",
    "    \n",
    "    # Embed the hypothetical answer\n",
    "    query_embedding = embedder.embed(hypothetical)\n",
    "    \n",
    "    # Search with hypothetical embedding\n",
    "    doc_embeddings = embedder.embed_batch(documents)\n",
    "    \n",
    "    results = []\n",
    "    for doc, emb in zip(documents, doc_embeddings):\n",
    "        score = cosine_similarity(query_embedding, emb)\n",
    "        results.append((doc, score))\n",
    "    \n",
    "    return sorted(results, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "# Test HyDE\n",
    "test_docs = [\n",
    "    \"Neural networks learn through backpropagation, adjusting weights based on errors.\",\n",
    "    \"Gradient descent optimizes the loss function by iteratively updating parameters.\",\n",
    "    \"Deep learning uses multiple layers to learn hierarchical representations.\",\n",
    "    \"Python is a popular programming language for machine learning.\"\n",
    "]\n",
    "\n",
    "hyde_results = hyde_search(\n",
    "    \"How do neural networks learn?\",\n",
    "    test_docs,\n",
    "    llm,\n",
    "    embedder\n",
    ")\n",
    "\n",
    "print(\"\\nHyDE Results:\")\n",
    "for doc, score in hyde_results:\n",
    "    print(f\"  {score:.3f}: {doc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Evaluation Metrics <a id='evaluation'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: RAG evaluation metrics\n",
    "from src.evaluation import calculate_retrieval_metrics, calculate_mrr\n",
    "\n",
    "# Example: Evaluating retrieval quality\n",
    "retrieved = [\"doc1\", \"doc2\", \"doc3\", \"doc4\", \"doc5\"]\n",
    "relevant = [\"doc2\", \"doc4\", \"doc6\"]\n",
    "\n",
    "metrics = calculate_retrieval_metrics(retrieved, relevant)\n",
    "print(\"Retrieval Metrics:\")\n",
    "print(f\"  Precision: {metrics['precision']:.3f}\")\n",
    "print(f\"  Recall: {metrics['recall']:.3f}\")\n",
    "print(f\"  F1 Score: {metrics['f1']:.3f}\")\n",
    "\n",
    "# MRR\n",
    "mrr = calculate_mrr(retrieved, \"doc2\")\n",
    "print(f\"  MRR: {mrr:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: End-to-end RAG evaluation\n",
    "from src.evaluation import LLMJudge\n",
    "from src.llm_utils import LLMClient\n",
    "\n",
    "llm = LLMClient(provider=\"openai\", model=\"gpt-4o-mini\")\n",
    "judge = LLMJudge(llm)\n",
    "\n",
    "# Evaluate a RAG answer\n",
    "question = \"What are the benefits of RAG?\"\n",
    "context = \"RAG reduces hallucination, provides citations, and enables domain-specific knowledge.\"\n",
    "answer = \"The benefits of RAG include reduced hallucinations, ability to cite sources, and adding custom knowledge.\"\n",
    "\n",
    "score, feedback = judge.evaluate(\n",
    "    question=question,\n",
    "    answer=answer,\n",
    "    criteria=\"accuracy, completeness, relevance to context\",\n",
    "    reference=context\n",
    ")\n",
    "\n",
    "print(f\"Score: {score:.2f}\")\n",
    "print(f\"Feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Optimize Chunking\n",
    "\n",
    "Find the optimal chunk size for your documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compare retrieval quality with different chunk sizes\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Build Advanced RAG Pipeline\n",
    "\n",
    "Combine multiple techniques into one pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create pipeline with: hybrid search + re-ranking + query expansion\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You understand different chunking strategies\n",
    "- [ ] You implemented hybrid search\n",
    "- [ ] You used re-ranking techniques\n",
    "- [ ] You can evaluate RAG systems\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll explore **Fine-tuning** - training models on your own data!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
