{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - LoRA & Parameter-Efficient Fine-tuning\n",
    "\n",
    "**Fine-tune large models on consumer hardware.**\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand parameter-efficient methods\n",
    "- Implement LoRA fine-tuning\n",
    "- Use Hugging Face PEFT library\n",
    "- Train models on limited hardware\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Why PEFT?](#why)\n",
    "2. [LoRA Explained](#lora)\n",
    "3. [Setting Up PEFT](#setup)\n",
    "4. [Training with LoRA](#training)\n",
    "5. [Inference](#inference)\n",
    "6. [Exercises](#exercises)\n",
    "7. [Checkpoint](#checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Setup\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "# Check for GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"Using Apple MPS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Why PEFT? <a id='why'></a>\n",
    "\n",
    "### Full Fine-tuning Problems:\n",
    "- **Memory**: Full model weights in GPU memory\n",
    "- **Storage**: Save entire model per task\n",
    "- **Time**: Update all parameters\n",
    "\n",
    "```\n",
    "Method              Parameters   Memory      Storage\n",
    "─────────────────────────────────────────────────────\n",
    "Full fine-tuning    100%        ~48GB       ~14GB\n",
    "LoRA (r=8)          0.1%        ~8GB        ~20MB\n",
    "QLoRA               0.1%        ~4GB        ~20MB\n",
    "```\n",
    "\n",
    "### PEFT Methods:\n",
    "- **LoRA**: Low-Rank Adaptation\n",
    "- **QLoRA**: Quantized LoRA\n",
    "- **Prefix Tuning**: Trainable prefix tokens\n",
    "- **Adapters**: Small bottleneck layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. LoRA Explained <a id='lora'></a>\n",
    "\n",
    "Instead of updating all weights, LoRA adds small trainable matrices:\n",
    "\n",
    "```\n",
    "Original:  W (frozen)\n",
    "LoRA:      W + BA  (where B and A are small matrices)\n",
    "\n",
    "If W is [4096 x 4096] = 16M parameters\n",
    "With rank r=8:\n",
    "  B: [4096 x 8] = 32K parameters\n",
    "  A: [8 x 4096] = 32K parameters\n",
    "  Total: 64K (0.4% of original)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Visualize LoRA concept\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRALayer(nn.Module):\n",
    "    \"\"\"Simple LoRA implementation for demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self, original_layer: nn.Linear, rank: int = 8, alpha: float = 16):\n",
    "        super().__init__()\n",
    "        self.original = original_layer\n",
    "        self.original.requires_grad_(False)  # Freeze original\n",
    "        \n",
    "        in_features = original_layer.in_features\n",
    "        out_features = original_layer.out_features\n",
    "        \n",
    "        # LoRA matrices\n",
    "        self.lora_A = nn.Parameter(torch.randn(in_features, rank) * 0.01)\n",
    "        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))\n",
    "        \n",
    "        self.scaling = alpha / rank\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Original output + LoRA adjustment\n",
    "        original_output = self.original(x)\n",
    "        lora_output = (x @ self.lora_A @ self.lora_B) * self.scaling\n",
    "        return original_output + lora_output\n",
    "\n",
    "# Example\n",
    "original = nn.Linear(1024, 1024)\n",
    "lora = LoRALayer(original, rank=8)\n",
    "\n",
    "original_params = sum(p.numel() for p in original.parameters())\n",
    "lora_params = sum(p.numel() for p in lora.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Original layer parameters: {original_params:,}\")\n",
    "print(f\"LoRA trainable parameters: {lora_params:,}\")\n",
    "print(f\"Reduction: {100 * (1 - lora_params/original_params):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Setting Up PEFT <a id='setup'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Load a base model with PEFT\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load a small model for demonstration\n",
    "model_name = \"facebook/opt-125m\"  # Small model for demo\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"Loaded {model_name}\")\n",
    "print(f\"Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    r=8,                     # Rank\n",
    "    lora_alpha=16,           # Scaling factor\n",
    "    lora_dropout=0.1,        # Dropout for regularization\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Which layers to adapt\n",
    ")\n",
    "\n",
    "# Apply LoRA to model\n",
    "peft_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Check trainable parameters\n",
    "trainable = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
    "total = sum(p.numel() for p in peft_model.parameters())\n",
    "\n",
    "print(f\"Trainable parameters: {trainable:,} ({100*trainable/total:.2f}%)\")\n",
    "peft_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Training with LoRA <a id='training'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Prepare training data\n",
    "from datasets import Dataset\n",
    "\n",
    "# Simple training examples\n",
    "training_data = [\n",
    "    {\"text\": \"Question: What is Python? Answer: Python is a programming language.\"},\n",
    "    {\"text\": \"Question: What is AI? Answer: AI stands for Artificial Intelligence.\"},\n",
    "    {\"text\": \"Question: What is ML? Answer: ML stands for Machine Learning.\"},\n",
    "    {\"text\": \"Question: What is NLP? Answer: NLP stands for Natural Language Processing.\"},\n",
    "]\n",
    "\n",
    "dataset = Dataset.from_list(training_data)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"text\"])\n",
    "print(f\"Dataset size: {len(tokenized_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Set up training\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora_output\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    learning_rate=3e-4,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    warmup_steps=10,\n",
    "    report_to=\"none\",  # Disable wandb\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not masked LM, we're doing causal LM\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer configured!\")\n",
    "print(\"Uncomment the next cell to train (takes a few minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Train the model (uncomment to run)\n",
    "# trainer.train()\n",
    "\n",
    "# Save the LoRA weights\n",
    "# peft_model.save_pretrained(\"./lora_output/final\")\n",
    "\n",
    "print(\"Training demonstration (uncomment to actually train)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Inference <a id='inference'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Load and use LoRA model\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "# base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Load LoRA weights\n",
    "# model_with_lora = PeftModel.from_pretrained(base_model, \"./lora_output/final\")\n",
    "\n",
    "# Generate\n",
    "# inputs = tokenizer(\"Question: What is Python?\", return_tensors=\"pt\")\n",
    "# outputs = model_with_lora.generate(**inputs, max_new_tokens=50)\n",
    "# print(tokenizer.decode(outputs[0]))\n",
    "\n",
    "print(\"Inference demonstration (uncomment after training)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUIDED: Merge LoRA weights (for deployment)\n",
    "# merged_model = model_with_lora.merge_and_unload()\n",
    "# merged_model.save_pretrained(\"./merged_model\")\n",
    "\n",
    "print(\"\"\"Merging LoRA weights:\n",
    "- Creates a single model with LoRA weights merged\n",
    "- No additional latency at inference\n",
    "- Larger file size (full model)\n",
    "- Good for production deployment\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Exercises <a id='exercises'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Experiment with Rank\n",
    "\n",
    "Compare different LoRA ranks (4, 8, 16, 32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create models with different ranks and compare parameter counts\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Train for Classification\n",
    "\n",
    "Adapt the code for a text classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use LoRA for sentiment classification\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Checkpoint <a id='checkpoint'></a>\n",
    "\n",
    "Before moving on, verify:\n",
    "\n",
    "- [ ] You understand why PEFT is useful\n",
    "- [ ] You know how LoRA works\n",
    "- [ ] You can configure and train with PEFT\n",
    "- [ ] You can load and use LoRA models\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "In the next notebook, we'll cover **Evaluation & Testing** - building robust test suites for AI systems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
