{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 - Capstone: Build Your AI Application\n",
    "\n",
    "**Put everything together in a complete project.**\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "Build a **Document Q&A Assistant** that:\n",
    "- Loads and indexes your documents\n",
    "- Answers questions using RAG\n",
    "- Provides source citations\n",
    "- Includes production-ready features\n",
    "\n",
    "## What You'll Build\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────┐\n",
    "│              Document Q&A Assistant              │\n",
    "├─────────────────────────────────────────────────┤\n",
    "│  ┌─────────┐    ┌─────────┐    ┌─────────────┐ │\n",
    "│  │Documents│───▶│ Indexer │───▶│Vector Store │ │\n",
    "│  └─────────┘    └─────────┘    └─────────────┘ │\n",
    "│                                       │        │\n",
    "│  ┌─────────┐    ┌─────────┐    ┌─────┴───────┐ │\n",
    "│  │ Answer  │◀───│   LLM   │◀───│  Retriever  │ │\n",
    "│  └─────────┘    └─────────┘    └─────────────┘ │\n",
    "│        │                                       │\n",
    "│        ▼                                       │\n",
    "│  ┌─────────────────────────────────────────┐  │\n",
    "│  │  + Caching + Metrics + Cost Tracking    │  │\n",
    "│  └─────────────────────────────────────────┘  │\n",
    "└─────────────────────────────────────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "sys.path.append(str(Path.cwd().parent))\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Path.cwd().parent / \".env\")\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 1: Create the Document Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample documents for the Q&A system\n",
    "from pathlib import Path\n",
    "\n",
    "docs_dir = Path(\"../data/documents\")\n",
    "docs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sample documents\n",
    "documents = {\n",
    "    \"company_policies.txt\": \"\"\"\n",
    "# Company Policies\n",
    "\n",
    "## Work Hours\n",
    "Standard work hours are 9 AM to 5 PM, Monday through Friday.\n",
    "Flexible hours are available with manager approval.\n",
    "Remote work is permitted up to 3 days per week.\n",
    "\n",
    "## Time Off\n",
    "Employees receive 20 days of paid time off per year.\n",
    "Sick leave is separate and unlimited with documentation.\n",
    "Holidays follow the company calendar (12 days per year).\n",
    "\n",
    "## Benefits\n",
    "Health insurance is provided for all full-time employees.\n",
    "401(k) matching up to 4% of salary.\n",
    "Professional development budget of $2,000 per year.\n",
    "\"\"\",\n",
    "    \"product_guide.txt\": \"\"\"\n",
    "# Product Guide\n",
    "\n",
    "## Getting Started\n",
    "1. Create an account at our website\n",
    "2. Download the desktop or mobile app\n",
    "3. Log in with your credentials\n",
    "4. Complete the onboarding tutorial\n",
    "\n",
    "## Features\n",
    "- Document Management: Store and organize files\n",
    "- Collaboration: Share with team members\n",
    "- Search: Find anything instantly\n",
    "- Analytics: Track usage and trends\n",
    "\n",
    "## Pricing\n",
    "- Free: Up to 5 GB storage, 3 users\n",
    "- Pro: $10/month, 100 GB storage, 10 users\n",
    "- Enterprise: Custom pricing, unlimited storage and users\n",
    "\"\"\",\n",
    "    \"faq.txt\": \"\"\"\n",
    "# Frequently Asked Questions\n",
    "\n",
    "## Account\n",
    "Q: How do I reset my password?\n",
    "A: Click \"Forgot Password\" on the login page and follow the instructions.\n",
    "\n",
    "Q: Can I change my email address?\n",
    "A: Yes, go to Settings > Account > Email to update it.\n",
    "\n",
    "## Billing\n",
    "Q: What payment methods are accepted?\n",
    "A: We accept all major credit cards and PayPal.\n",
    "\n",
    "Q: Can I get a refund?\n",
    "A: Yes, within 30 days of purchase with no questions asked.\n",
    "\n",
    "## Technical\n",
    "Q: What browsers are supported?\n",
    "A: Chrome, Firefox, Safari, and Edge (latest versions).\n",
    "\n",
    "Q: Is my data secure?\n",
    "A: Yes, we use industry-standard encryption and security practices.\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Write documents\n",
    "for filename, content in documents.items():\n",
    "    (docs_dir / filename).write_text(content)\n",
    "    print(f\"Created: {filename}\")\n",
    "\n",
    "print(f\"\\nDocuments saved to {docs_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 2: Build the RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the complete Q&A system\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "from src.rag_pipeline import DocumentLoader, Chunker, VectorStore, Document\n",
    "from src.llm_utils import LLMClient\n",
    "from src.embedding_utils import EmbeddingModel\n",
    "\n",
    "@dataclass\n",
    "class QAResult:\n",
    "    \"\"\"Result from Q&A query.\"\"\"\n",
    "    answer: str\n",
    "    sources: list[dict]\n",
    "    cached: bool\n",
    "    latency_ms: float\n",
    "    tokens_used: int\n",
    "    cost: float\n",
    "\n",
    "class DocumentQA:\n",
    "    \"\"\"Production-ready Document Q&A System.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        llm_provider: str = \"openai\",\n",
    "        llm_model: str = \"gpt-4o-mini\",\n",
    "        embedding_model: str = \"text-embedding-3-small\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 50,\n",
    "        cache_ttl: float = 3600\n",
    "    ):\n",
    "        # Core components\n",
    "        self.llm = LLMClient(provider=llm_provider, model=llm_model)\n",
    "        self.loader = DocumentLoader()\n",
    "        self.chunker = Chunker(chunk_size=chunk_size, overlap=chunk_overlap)\n",
    "        self.vector_store = VectorStore(collection_name=\"qa_docs\")\n",
    "        \n",
    "        # Production features\n",
    "        self.cache: dict = {}\n",
    "        self.cache_ttl = cache_ttl\n",
    "        self.metrics = {\n",
    "            \"queries\": 0,\n",
    "            \"cache_hits\": 0,\n",
    "            \"total_tokens\": 0,\n",
    "            \"total_cost\": 0,\n",
    "            \"latencies\": []\n",
    "        }\n",
    "        \n",
    "        self.documents_loaded = 0\n",
    "    \n",
    "    def load_documents(self, path: str) -> int:\n",
    "        \"\"\"Load and index documents.\"\"\"\n",
    "        path = Path(path)\n",
    "        \n",
    "        if path.is_file():\n",
    "            docs = self.loader.load_file(str(path))\n",
    "        else:\n",
    "            docs = self.loader.load_directory(str(path))\n",
    "        \n",
    "        # Chunk documents\n",
    "        chunks = self.chunker.chunk_all(docs)\n",
    "        \n",
    "        # Add to vector store\n",
    "        self.vector_store.add_documents(chunks)\n",
    "        self.documents_loaded = len(chunks)\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def _cache_key(self, query: str) -> str:\n",
    "        \"\"\"Generate cache key.\"\"\"\n",
    "        return hashlib.sha256(query.lower().strip().encode()).hexdigest()[:16]\n",
    "    \n",
    "    def _get_cached(self, query: str) -> Optional[QAResult]:\n",
    "        \"\"\"Check cache for result.\"\"\"\n",
    "        key = self._cache_key(query)\n",
    "        if key in self.cache:\n",
    "            entry = self.cache[key]\n",
    "            if time.time() - entry[\"timestamp\"] < self.cache_ttl:\n",
    "                return entry[\"result\"]\n",
    "        return None\n",
    "    \n",
    "    def _set_cached(self, query: str, result: QAResult):\n",
    "        \"\"\"Store result in cache.\"\"\"\n",
    "        key = self._cache_key(query)\n",
    "        self.cache[key] = {\n",
    "            \"result\": result,\n",
    "            \"timestamp\": time.time()\n",
    "        }\n",
    "    \n",
    "    def query(self, question: str, k: int = 3) -> QAResult:\n",
    "        \"\"\"Answer a question using the indexed documents.\"\"\"\n",
    "        self.metrics[\"queries\"] += 1\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Check cache\n",
    "        cached = self._get_cached(question)\n",
    "        if cached:\n",
    "            self.metrics[\"cache_hits\"] += 1\n",
    "            cached.cached = True\n",
    "            cached.latency_ms = (time.time() - start_time) * 1000\n",
    "            return cached\n",
    "        \n",
    "        # Retrieve relevant documents\n",
    "        results = self.vector_store.search(question, k=k)\n",
    "        \n",
    "        # Build context\n",
    "        context_parts = []\n",
    "        sources = []\n",
    "        for i, r in enumerate(results):\n",
    "            context_parts.append(f\"[{i+1}] {r['content']}\")\n",
    "            sources.append({\n",
    "                \"id\": i + 1,\n",
    "                \"content\": r[\"content\"][:200] + \"...\",\n",
    "                \"metadata\": r.get(\"metadata\", {})\n",
    "            })\n",
    "        \n",
    "        context = \"\\n\\n\".join(context_parts)\n",
    "        \n",
    "        # Generate answer\n",
    "        prompt = f\"\"\"Use the following context to answer the question. \n",
    "Include source references like [1], [2] when citing information.\n",
    "If you cannot answer from the context, say so.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer = self.llm.chat(prompt)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        latency = (time.time() - start_time) * 1000\n",
    "        stats = self.llm.get_stats()\n",
    "        tokens = stats.total_input_tokens + stats.total_output_tokens\n",
    "        cost = stats.total_cost\n",
    "        \n",
    "        # Update metrics\n",
    "        self.metrics[\"total_tokens\"] += tokens\n",
    "        self.metrics[\"total_cost\"] += cost\n",
    "        self.metrics[\"latencies\"].append(latency)\n",
    "        \n",
    "        result = QAResult(\n",
    "            answer=answer,\n",
    "            sources=sources,\n",
    "            cached=False,\n",
    "            latency_ms=latency,\n",
    "            tokens_used=tokens,\n",
    "            cost=cost\n",
    "        )\n",
    "        \n",
    "        # Cache result\n",
    "        self._set_cached(question, result)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_metrics(self) -> dict:\n",
    "        \"\"\"Get system metrics.\"\"\"\n",
    "        latencies = self.metrics[\"latencies\"]\n",
    "        return {\n",
    "            \"documents_indexed\": self.documents_loaded,\n",
    "            \"total_queries\": self.metrics[\"queries\"],\n",
    "            \"cache_hit_rate\": self.metrics[\"cache_hits\"] / max(1, self.metrics[\"queries\"]),\n",
    "            \"total_tokens\": self.metrics[\"total_tokens\"],\n",
    "            \"total_cost\": f\"${self.metrics['total_cost']:.4f}\",\n",
    "            \"avg_latency_ms\": sum(latencies) / max(1, len(latencies)),\n",
    "            \"cache_size\": len(self.cache)\n",
    "        }\n",
    "\n",
    "print(\"DocumentQA class defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 3: Initialize and Load Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialize the Q&A system\n",
    "qa = DocumentQA(\n",
    "    llm_provider=\"openai\",\n",
    "    llm_model=\"gpt-4o-mini\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "# Load documents\n",
    "num_chunks = qa.load_documents(\"../data/documents\")\n",
    "print(f\"\\nIndexed {num_chunks} document chunks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 4: Test the System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with various questions\n",
    "questions = [\n",
    "    \"How many days of PTO do employees get?\",\n",
    "    \"What is the pricing for the Pro plan?\",\n",
    "    \"How do I reset my password?\",\n",
    "    \"What payment methods are accepted?\",\n",
    "    \"Is remote work allowed?\"\n",
    "]\n",
    "\n",
    "print(\"Testing Document Q&A System\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"\\nQ: {q}\")\n",
    "    result = qa.query(q)\n",
    "    print(f\"A: {result.answer}\")\n",
    "    print(f\"   [Latency: {result.latency_ms:.0f}ms, Cached: {result.cached}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test caching - repeat a question\n",
    "print(\"Testing cache (repeating first question)...\")\n",
    "result = qa.query(questions[0])\n",
    "print(f\"Q: {questions[0]}\")\n",
    "print(f\"A: {result.answer}\")\n",
    "print(f\"   [Cached: {result.cached}, Latency: {result.latency_ms:.2f}ms]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 5: View Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display system metrics\n",
    "metrics = qa.get_metrics()\n",
    "\n",
    "print(\"System Metrics\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in metrics.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Step 6: Interactive Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Q&A (run this cell and enter questions)\n",
    "def interactive_qa():\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Document Q&A Assistant\")\n",
    "    print(\"Type 'quit' to exit, 'metrics' to see stats\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    while True:\n",
    "        question = input(\"You: \").strip()\n",
    "        \n",
    "        if not question:\n",
    "            continue\n",
    "        \n",
    "        if question.lower() == 'quit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if question.lower() == 'metrics':\n",
    "            print(json.dumps(qa.get_metrics(), indent=2))\n",
    "            continue\n",
    "        \n",
    "        result = qa.query(question)\n",
    "        print(f\"\\nAssistant: {result.answer}\")\n",
    "        \n",
    "        if result.sources:\n",
    "            print(f\"\\n[Sources: {len(result.sources)} documents | \"\n",
    "                  f\"Latency: {result.latency_ms:.0f}ms | \"\n",
    "                  f\"Cached: {result.cached}]\")\n",
    "        print()\n",
    "\n",
    "# Uncomment to run interactive mode\n",
    "# interactive_qa()\n",
    "\n",
    "print(\"Uncomment the last line to run interactive Q&A!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Capstone Challenges\n",
    "\n",
    "Extend your Document Q&A system with these features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1: Add Conversation History\n",
    "\n",
    "Implement multi-turn conversations that remember context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add conversation history to enable follow-up questions\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Add Source Highlighting\n",
    "\n",
    "Show which parts of the source documents were used for the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Highlight relevant passages in source documents\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Add Feedback Loop\n",
    "\n",
    "Implement user feedback to improve answers over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add thumbs up/down feedback and store for analysis\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Create API Endpoint\n",
    "\n",
    "Wrap the system in a FastAPI endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create FastAPI app with /query endpoint\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Congratulations!\n",
    "\n",
    "You've completed the AI Engineering track! You now know how to:\n",
    "\n",
    "- Work with LLM APIs and structured outputs\n",
    "- Build RAG systems with vector databases\n",
    "- Fine-tune models for specific tasks\n",
    "- Evaluate and test AI systems\n",
    "- Deploy production-ready AI applications\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "1. **Expand this project** with the challenges above\n",
    "2. **Try the Agentic AI track** to build autonomous agents\n",
    "3. **Build your own project** applying these concepts\n",
    "4. **Contribute** to open-source AI tools\n",
    "\n",
    "Happy building!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
